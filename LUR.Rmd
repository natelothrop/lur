---
title: "Lothrop PhD - Aim 1"
author: "Nathan Lothrop"
date: "January 20, 2019"
output: html_document
---


## General LUR Workflow

In order to create the best externally validated LUR model for each pollutant, there are several steps: 

1. wrangling the source data for TAPS and PCWS, as well as PDEQ background monitor data; 
2. developing the LUR model using the ESCAPE method; and 
3. externally validating the model with 4 different ways of adjusting for changes over time.

### Wrangling Study Data

Basic steps:

1. Pull in study data
2. pull in background PDEQ monitor
3. create annual avg of each pollutant via ESCAPE method

However PCWS didn't follow ESCAPE so this is slightly more involved and we will have to make assumptions compared to TAPS.

```{r echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Clear environment of temporary files
rm(list=ls())

# Read in packages
packages <- c('Hmisc', 'corrplot', 'tidyverse', 'ggplot2', 'lubridate', 'broom', 'rlist')


package.check <- lapply(packages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})

#read in all p5 data
p5all<- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/P5/Data/LUR/Results/P5_Master.csv")

#subset out the monitor data that has hhid >5000
monitor<-subset(p5all,hhid>5001)
data<-subset(p5all,hhid<5001)

str(data)

keeps<-c("hhid","stday","stmon","styear",
         "endday","endmon","endyear"
         ,"no2ppb", "pm25ugm3","pm10ugm3")
data<-data[keeps]

# Average duplicates (same run period)
data <- data %>%
  group_by(hhid,stday,stmon,styear,endday,endmon,endyear) %>%
  summarise(no2ppb = mean(no2ppb),
         pm25ugm3 = mean(pm25ugm3),
         pm10ugm3 = mean(pm10ugm3))

# Assign 1 of 3 season values (Winter, Intermed, Summer)
data$stssn3 <- ifelse((data$stmon==3 | data$stmon==4 | data$stmon==9 | data$stmon==10), 2,
                     ifelse((data$stmon==1 | data$stmon==2 | data$stmon==11 | data$stmon==12), 1, 3))
                               
data$stssn3 <- factor(data$stssn3,
                    levels = c(1,2,3),
                    labels = c("Winter", "Intermed.", "Summer"))

# Assign 1 of 4 season values (Winter, Spring, Summer, Fall)
data$stssn4 <- ifelse((data$stmon==1 | data$stmon==2 | data$stmon==11 | data$stmon==12), 1,
                      ifelse((data$stmon==3 | data$stmon==4), 2,
                              ifelse((data$stmon==5 | data$stmon==6 | data$stmon==7 | data$stmon==8), 3, 4)))
                               
data$stssn4 <- factor(data$stssn4,
                    levels = c(1,2,3,4),
                    labels = c("Winter", "Spring", "Summer", "Fall"))

datano2 <- subset(data,!is.na(no2ppb))
datapm25 <- subset(data,!is.na(pm25ugm3))
datapm10 <- subset(data,!is.na(pm10ugm3))
```

######PCWS NO2
```{r, echo=FALSE, warning=FALSE}

datano2 %>%
  group_by(styear) %>%
  summarise(n.obs=length(no2ppb),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(no2ppb))),
            mean=mean(no2ppb),
            geosd=exp(sd(log(no2ppb))),
            min=min(no2ppb),
            max=max(no2ppb))


data %>%
  filter(!is.na(no2ppb)) %>%
  group_by(hhid) %>%
  summarise(n.obs=length(no2ppb),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(no2ppb))),
            mean=mean(no2ppb),
            geosd=exp(sd(log(no2ppb))),
            min=min(no2ppb),
            max=max(no2ppb))
```

######PCWS PM2.5
```{r, echo=FALSE, warning=FALSE}

datapm25 %>%
  group_by(styear) %>%
  summarise(n.obs=length(pm25ugm3),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(pm25ugm3))),
            mean=mean(pm25ugm3),
            geosd=exp(sd(log(pm25ugm3))),
            min=min(pm25ugm3),
            max=max(pm25ugm3))
```

######PCWS PM10
```{r, echo=FALSE, warning=FALSE}

datapm10 %>%
  group_by(styear) %>%
  summarise(n.obs=length(pm10ugm3),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(pm10ugm3))),
            mean=mean(pm10ugm3),
            geosd=exp(sd(log(pm10ugm3))),
            min=min(pm10ugm3),
            max=max(pm10ugm3))

data %>%
  filter(!is.na(pm10ugm3)) %>%
  group_by(hhid) %>%
  summarise(n.obs=length(pm10ugm3),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(pm10ugm3))),
            mean=mean(pm10ugm3),
            geosd=exp(sd(log(pm10ugm3))),
            min=min(pm10ugm3),
            max=max(pm10ugm3))

```
Many PCWS homes are measured more than once in a year, but often are measured back to back (one week, then the next). We will use all measures (and correct for changes in air pollution levels over time), as ESCAPE allows for 1-3 or more measures throughout the year (see page 9 and 10 here: http://escapeproject.eu/manuals/ESCAPE_Exposure-manualv9.pdf).

However, before committing to this, we will investigate how many homes have measures over multiple years, because if they do, then it may not make sense in LUR development if we have predictors that change over the time span to put all homes in the same model.


######Homes with more than 2 measures in different MONTHS in the same year for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

# data %>%
#   filter(!is.na(no2ppb)) %>%
#   dplyr::group_by(hhid,styear, stmon) %>%
#   summarise(n_homes = n_distinct(hhid),
#             n_obs_no2 = n_distinct(no2ppb)) %>%
#   arrange(desc(n_obs_no2, hhid))
data %>%
  filter(!is.na(no2ppb)) %>%
  dplyr::group_by(hhid,styear, stmon) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(no2ppb))

datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_bymonth = n_distinct(stmon))

datano2 %>%
  filter(no2_bymonth>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(no2ppb))
```
So for NO2, there are multiple homes in 87-89 with 2 or more measures in the same month and different months in the same year.


Let's check this same thing for PM2.5.

######Homes with 2 or more measures in different MONTHS in the same year for PM2.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm25 <- datapm25 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm25_bymonth = n_distinct(stmon))

datapm25 %>%
  filter(pm25_bymonth>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm25ugm3))
```
For PM2.5, only 2 homes have more than 1 measure in different months. 

######Homes with 2 or more measures in different MONTHS in the same year for PM10:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm10 <- datapm10 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm10_bymonth = n_distinct(stmon))

datapm10 %>%
  filter(pm10_bymonth>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm10ugm3))
```
For PM10, only 2 homes have more than 1 measure in different months. 


######Homes with 2 or more measures in different SEASONS in the same year (ESCAPE 3-season approach) for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_byseason3 = n_distinct(stssn3))

datano2 %>%
  filter(no2_byseason3>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(no2ppb))
``` 

######Homes with 2 or more measures in different SEASONS in the same year (ESCAPE 3-season approach) for PM2.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# 
# data %>%
#   filter(!is.na(no2ppb)) %>%
#   dplyr::group_by(hhid,styear, stssn3) %>%
#   summarise(n_homes = n_distinct(hhid),
#             n_obs_no2 = n_distinct(no2ppb)) %>%
#   arrange(desc(n_obs_no2, hhid))

datapm25 <- datapm25 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm25_byseason3 = n_distinct(stssn3))

datapm25 %>%
  filter(pm25_byseason3>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm25ugm3))
``` 
There are NO homes with 2 or more PM2.5 measures in different ESCAPE seasons in same year.

######Homes with 2 or more measures in different SEASONS in the same year (ESCAPE 3-season approach) for PM10:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm10 <- datapm10 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm10_byseason3 = n_distinct(stssn3))

datapm10 %>%
  filter(pm10_byseason3>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm10ugm3))
``` 
There are NO homes with 2 or more PM10 measures in different ESCAPE seasons in same year.

######Homes with 2 or more measures in different SEASONS in the same year (non-ESCAPE 4-season approach) for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_byseason4 = n_distinct(stssn4))

datano2 %>%
  filter(no2_byseason4>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(no2ppb))
``` 
######Homes with 2 or more measures in different SEASONS in the same year (non-ESCAPE 4-season approach) for PM2.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm25 <- datapm25 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm25_byseason4 = n_distinct(stssn4))

datapm25 %>%
  filter(pm25_byseason4>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm25ugm3))
```
No homes again for PM2.5, using 4 season approach.

######Homes with 2 or more measures in different SEASONS in the same year (non-ESCAPE 4-season approach) for PM10:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm10 <- datapm10 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm10_byseason4 = n_distinct(stssn4))

datapm10 %>%
  filter(pm10_byseason4>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm10ugm3))
```
No homes again for PM10, using 4 season approach.

Since only NO2 has >1 measure/season or month in more than a handful of homes like PM2.5 and PM10, we'll base our season definition off this. 

**We'll go with the 3 season as this the ESCAPE approach** which we want to follow anyways and there are no difference b/w 3 and 4 month season definitions.

Now let's look at whether there are homes that have >1 NO2 measure in distinct months/year IN multiple years and the same, but for seasons in multiple years.

######Homes with 2 or more measures in different MONTHS in the same year over MULTIPLE years for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

#create frequency of measurement by year and season
#no2
datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_bymonth = n_distinct(stmon))

datano2 %>%
  group_by(hhid,styear) %>%
  summarise(n_obs_no2 = n_distinct(no2ppb)) %>%
  filter(n_obs_no2>1) %>%
  arrange(hhid,desc(n_obs_no2))
```

There are many homes that have obs in more than 1 year (>1 distinct measure/month).


######Homes with 2 or more measures in different SEASONS in the same year over MULTIPLE years for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

#create frequency of measurement by year and season
#no2
datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_byseason3 = n_distinct(stssn3))

datano2 %>%
  filter(no2_byseason3>1) %>%
  group_by(hhid) %>%
  summarise(n_years = n_distinct(styear),
            n_obs = n_distinct(no2ppb)) %>%
  arrange(desc(n_years,hhid))

datano2 %>%
  group_by(hhid) %>%
  summarise(n_unique_years = n_distinct(styear),
            n_obs = n_distinct(no2ppb)) %>%
  filter(n_unique_years>2) %>%
  arrange(desc(n_unique_years,n_obs))
```
**NO2**
Of 334 homes, 155 have measures in 2 different years, amd 15 have measures in 3 different years.

######PM2.5 test for how many homes have measures in multiple seasons and multiple years:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm25 <- datapm25 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm25_byseason3 = n_distinct(stssn3))

datapm25 %>%
  filter(pm25_byseason3>1) %>%
  group_by(hhid) %>%
  summarise(n_years = n_distinct(styear),
            n_obs = n_distinct(pm25ugm3)) %>%
  arrange(desc(n_years,hhid))

datapm25 %>%
  group_by(hhid) %>%
  summarise(n_unique_years = n_distinct(styear),
            n_obs = n_distinct(pm25ugm3)) %>%
  # filter(n_unique_years>1) %>%
  arrange(desc(n_unique_years,n_obs))
```

**PM2.5**
Of 60 homes, 7 have measures in different years. The rest have only a single measure (in a single year).

**After speaking with Dr. Beamer on 1/30/19 in office hours, the approach will be this:**

1. for a single pollutant (NO2?) do annual average for all homes, regardless of how many times they're measured
2. complete LUR model for this "all homes" approach (all homes get year-specific predictor, eg a home with 1987 avg will have the traffic volume predictors be named same as home from 1989, but have year-specific value)
3. then, as a sensitivity analysis, do it so that each home is included annual average just once in model

#### PCWS Annual Averages

##### Create Daily Avg for PDEQ Background Monitors
```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Read in NO2, PM2.5, and PM10 background values from the longest, constantly running PDEQ monitors for NAAQS compliance
# Downloaded in 20 year increments for all available data from 1980 - 2017 from https://aqs.epa.gov/api on 2018/12/28 with user: lothrop@email.arizona.edu and password 'rubycat56'
# Parameter codes are NO2: 42602 at 1 hour sample duration; PM2.5: 88502 (Acceptable PM2.5 AQI & Speciation Mass); PM10: 81102 (PM10 Total 0-10um STP), both at 24 hour sample duration
# NO2, PM2.5, and PM10 monitor sites that running longest aren't same (NO2: Alvernon/Craycroft; PM2.5: Saguaro Park; PM10: Orange Grove)
# Note that PM2.5 monitoring is taken offline temporarily at Saguaro Park 1994-1999

wd <- "/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/Data/PDEQHistoric"

no2_ref1 <- read.csv(paste0(wd,"/","NO2_1980_2000.txt"), header = T)
no2_ref2 <- read.csv(paste0(wd,"/","NO2_2000_2017.txt"), header = T)
no2_ref <- bind_rows(no2_ref1, no2_ref2)

pm25_ref1 <- read.csv(paste0(wd,"/","PM25_1980_2000.txt"), header = T)
pm25_ref1$Qualifier.Description <- as.factor(pm25_ref1$Qualifier.Description) #need to update field type because as an artifact of import
pm25_ref2 <- read.csv(paste0(wd,"/","PM25_2000_2017.txt"), header = T)
pm25_ref <- bind_rows(pm25_ref1, pm25_ref2)

pm10_ref1 <- read.csv(paste0(wd,"/","PM10_1980_2000.txt"), header = T)
pm10_ref2 <- read.csv(paste0(wd,"/","PM10_2000_2017.txt"), header = T)
pm10_ref <- bind_rows(pm10_ref1, pm10_ref2)

# Create annual averages for each pollutant type
no2_ref <- no2_ref %>%
  dplyr::group_by(Date.Local) %>%
  dplyr::summarise(no2_ref = mean(Sample.Measurement)) %>%
  dplyr::filter(!is.na(Date.Local) | !is.na(no2_ref)) %>%
  dplyr::select(Date.Local, no2_ref)

pm25_ref <- pm25_ref %>%
  dplyr::group_by(Date.Local) %>%
  dplyr::summarise(pm25_ref = mean(Sample.Measurement)) %>%
  dplyr::filter(!is.na(Date.Local) | !is.na(pm25_ref)) %>%
  dplyr::select(Date.Local, pm25_ref)
  
pm10_ref <- pm10_ref %>%
  dplyr::group_by(Date.Local) %>%
  dplyr::summarise(pm10_ref = mean(Sample.Measurement)) %>%
  dplyr::filter(!is.na(Date.Local) | !is.na(pm10_ref)) %>%
  dplyr::select(Date.Local, pm10_ref)

no2_ref <- filter(no2_ref, !is.na(no2_ref))
pm25_ref <- filter(pm25_ref, !is.na(pm25_ref))
pm10_ref <- filter(pm10_ref, !is.na(pm10_ref))

summary(no2_ref$no2_ref)

```
Since there are values below 0 that have no qualifier or description of an error or issue, we will truncate these values to 0 ppb.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

no2_ref$no2_ref <- ifelse(no2_ref$no2_ref < 0, 0, no2_ref$no2_ref)
summary(no2_ref$no2_ref)
summary(pm25_ref$pm25_ref)
summary(pm10_ref$pm10_ref)

```

Summary values all make sense now. The very high PM10 values all have description qualifiers on them in the raw data noting high wind events (eg dust storms), which are a known issue in this area that will push Pima County to violate PM10 NAAQS standards. Next step is to merge together by start date to relevant pollutant.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

read.data <- function(){
#read in all p5 data
p5all<- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/P5/Data/LUR/Results/P5_Master.csv")

#subset out the monitor data that has hhid >5000
monitor<-subset(p5all,hhid>5001)
data<-subset(p5all,hhid<5001)

str(data)

keeps<-c("hhid","stday","stmon","styear",
         "endday","endmon","endyear"
         ,"no2ppb", "pm25ugm3","pm10ugm3")
data<-data[keeps]

# Average duplicates (same run period)
data <- data %>%
  group_by(hhid,stday,stmon,styear,endday,endmon,endyear) %>%
  summarise(no2ppb = mean(no2ppb),
         pm25ugm3 = mean(pm25ugm3),
         pm10ugm3 = mean(pm10ugm3))


# Assign 1 of 3 season values (Winter, Intermed, Summer)
data$stssn3 <- ifelse((data$stmon==3 | data$stmon==4 | data$stmon==9 | data$stmon==10), 2,
                     ifelse((data$stmon==1 | data$stmon==2 | data$stmon==11 | data$stmon==12), 1, 3))
                               
data$stssn3 <- factor(data$stssn3,
                    levels = c(1,2,3),
                    labels = c("Winter", "Intermed.", "Summer"))

# Assign 1 of 4 season values (Winter, Spring, Summer, Fall)
data$stssn4 <- ifelse((data$stmon==1 | data$stmon==2 | data$stmon==11 | data$stmon==12), 1,
                      ifelse((data$stmon==3 | data$stmon==4), 2,
                              ifelse((data$stmon==5 | data$stmon==6 | data$stmon==7 | data$stmon==8), 3, 4)))
                               
data$stssn4 <- factor(data$stssn4,
                    levels = c(1,2,3,4),
                    labels = c("Winter", "Spring", "Summer", "Fall"))

datano2 <- subset(data,!is.na(no2ppb))
datapm25 <- subset(data,!is.na(pm25ugm3))
datapm10 <- subset(data,!is.na(pm10ugm3))
}

```

##### Create annual averages corrected for temporal changes
```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Approach:
# 1. figure out what ref monitor dates are after the start date and before the end date of the home measurement (through decimalized dating)
# 2. for this ref date period, take the average of the pollutant measurement, then divide that by the annual average to get the ratio temporal correction
# 3. multiple the ratio temporal correction by the home measure for that period
# 4. average the product(s) from step 3, resulting in the home's annual average

# Create decimalized dates for ref data
no2_ref$date_dec <- decimal_date(as.Date(no2_ref$Date.Local,format="%Y-%m-%d"))
pm25_ref$date_dec <- decimal_date(as.Date(pm25_ref$Date.Local,format="%Y-%m-%d"))
pm10_ref$date_dec <- decimal_date(as.Date(pm10_ref$Date.Local,format="%Y-%m-%d"))

# Create monthly and annual averages for ref data-NO2
no2_ref$year <- as.integer(year(no2_ref$Date.Local))
no2_ref$mon <- as.integer(month(no2_ref$Date.Local))

no2_ref_yearavg <- no2_ref %>%
  group_by(year) %>%
  summarise(no2_year_avg=mean(no2_ref))

no2_ref_monavg <- no2_ref %>%
  group_by(mon, year) %>%
  summarise(no2_mon_avg=mean(no2_ref))

# Create monthly and annual averages for ref data-PM2.5
pm25_ref$year <- as.integer(year(pm25_ref$Date.Local))
pm25_ref$mon <- as.integer(month(pm25_ref$Date.Local))

pm25_ref_yearavg <- pm25_ref %>%
  group_by(year) %>%
  summarise(pm25_year_avg=mean(pm25_ref))

pm25_ref_monavg <- pm25_ref %>%
  group_by(mon, year) %>%
  summarise(pm25_mon_avg=mean(pm25_ref))

# Create monthly and annual averages for ref data-PM10
pm10_ref$year <- as.integer(year(pm10_ref$Date.Local))
pm10_ref$mon <- as.integer(month(pm10_ref$Date.Local))

pm10_ref_yearavg <- pm10_ref %>%
  group_by(year) %>%
  summarise(pm10_year_avg=mean(pm10_ref))

pm10_ref_monavg <- pm10_ref %>%
  group_by(mon, year) %>%
  summarise(pm10_mon_avg=mean(pm10_ref))

# Create decimalized dates (start and end) for measurement data
data$styear <- data$styear + 1900
data$endyear <- data$endyear + 1900

data$st_date <- as.Date(with(data, paste(styear, stmon, stday,sep="-")), "%Y-%m-%d")
data$end_date <- as.Date(with(data, paste(endyear, endmon, endday,sep="-")), "%Y-%m-%d")

data$stdate_dec <- decimal_date(as.Date(data$st_date,format="%Y-%m-%d"))
data$enddate_dec <- decimal_date(as.Date(data$end_date,format="%Y-%m-%d"))

# Join monthly and yearly averages of reference monitors to measurement data
data <- full_join(data,no2_ref_monavg,by=c("styear"="year","stmon"="mon"))
data <- full_join(data,no2_ref_yearavg,by=c("styear"="year"))
data <- full_join(data,pm25_ref_monavg,by=c("styear"="year","stmon"="mon"))
data <- full_join(data,pm25_ref_yearavg,by=c("styear"="year"))
data <- full_join(data,pm10_ref_monavg,by=c("styear"="year","stmon"="mon"))
data <- full_join(data,pm10_ref_yearavg,by=c("styear"="year"))

# Filter out those that have no hhid
data <- filter(data, !is.na(hhid))

# Create the ratio-adjusted NO2 measurement (no2_adj)
data$no2_adj <- 0
for(i in 1:nrow(data)){
  stdate <- c(data[i,15])
  enddate <- c(data[i,16])
  no2_period_avg <- mean(as.numeric(unlist(c(no2_ref[no2_ref$date_dec >= stdate & no2_ref$date_dec <= enddate,2])))) # calculate the average ref for that period of measurment
  if(!is.na(no2_period_avg)){ # Note!-not all measurement periods have a ref monitor running, so use the monthly average (of that starting measurement month) to annual ratio instead
    no2_period_ratio <- no2_period_avg/data[i,18] # calculate the ratio of ref period to annual avg
    data[i,23] <- no2_period_ratio * data[i,8] # adjust measurement by period to year ratio
  }else{
    no2_mon_ratio <- data[i,17]/data[i,18] # calculate the ratio of ref mon to annual avg
    data[i,23] <- no2_mon_ratio * data[i,8] # adjust measurement by month to year ratio
    }
}

# Look for NO2 measures that have a raw measurement but no temporally-adjusted concentration
sum(!is.na(data$no2ppb) & is.na(data$no2_adj))

# Create the ratio-adjusted PM2.5 measurement (pm25_adj)
data$pm25_adj <- 0
for(i in 1:nrow(data)){
  stdate <- c(data[i,15])
  enddate <- c(data[i,16])
  pm25_period_avg <- mean(as.numeric(unlist(c(pm25_ref[pm25_ref$date_dec >= stdate & pm25_ref$date_dec <= enddate,2])))) # calculate the average ref for that period of measurment
  if(!is.na(pm25_period_avg)){ # Note!-not all measurement periods have a ref monitor running, so use the monthly average (of that starting measurement month) to annual ratio instead
    pm25_period_ratio <- pm25_period_avg/data[i,20] # calculate the ratio of ref period to annual avg
    data[i,24] <- pm25_period_ratio * data[i,9] # adjust measurement by period to year ratio
  }else{
    pm25_mon_ratio <- data[i,19]/data[i,20] # calculate the ratio of ref mon to annual avg
    data[i,24] <- pm25_mon_ratio * data[i,9] # adjust measurement by month to year ratio
    }
}

# Look for PM2.5 measures that have a raw measurement but no temporally-adjusted concentration
sum(!is.na(data$pm25ugm3) & is.na(data$pm25_adj))

# Create the ratio-adjusted PM10 measurement (pm10_adj)
data$pm10_adj <- 0
for(i in 1:nrow(data)){
  stdate <- c(data[i,15])
  enddate <- c(data[i,16])
  pm10_period_avg <- mean(as.numeric(unlist(c(pm10_ref[pm10_ref$date_dec >= stdate & pm10_ref$date_dec <= enddate,2])))) # calculate the average ref for that period of measurment
  if(!is.na(pm10_period_avg)){ # Note!-not all measurement periods have a ref monitor running, so use the monthly average (of that starting measurement month) to annual ratio instead
    pm10_period_ratio <- pm10_period_avg/data[i,22] # calculate the ratio of ref period to annual avg
    data[i,25] <- pm10_period_ratio * data[i,10] # adjust measurement by period to year ratio
  }else{
    pm10_mon_ratio <- data[i,21]/data[i,22] # calculate the ratio of ref mon to annual avg
    data[i,25] <- pm10_mon_ratio * data[i,10] # adjust measurement by month to year ratio
    }
}

# Look for PM10 measures that have a raw measurement but no temporally-adjusted concentration
sum(!is.na(data$pm10ugm3) & is.na(data$pm10_adj))

```

##### Missing PM2.5 Ref Monitor Data
PM2.5 measures in PCWS started in March 1987, but reference background monitoring didn't start until June 1988. This leaves `r sum(!is.na(data$pm25ugm3) & is.na(data$pm25_adj))` homes without reference values to temporally correct. To correct, we will test to see if we can use the monthly ratio of the closest reference dates from a full monitoring year to fill in March 1987-May 1988 (eg using all of 1989).

```{r, echo=FALSE, message=FALSE, warning=FALSE}

pm25_ref_replaceavg <- full_join(pm25_ref_monavg,pm25_ref_yearavg,by=c("year"))
pm25_ref_replaceavg$ratio <- 0
pm25_ref_replaceavg$ratio <- pm25_ref_replaceavg$pm25_mon_avg/pm25_ref_replaceavg$pm25_year_avg

pm25_ref_replaceavg_88 <- pm25_ref_replaceavg %>%
  filter(year==1988)
pm25_ref_replaceavg_89 <- pm25_ref_replaceavg %>%
  filter(year==1989)
pm25_ref_replaceavg_90 <- pm25_ref_replaceavg %>%
  filter(year==1990)
pm25_ref_replaceavg_91 <- pm25_ref_replaceavg %>%
  filter(year==1991)
pm25_ref_replaceavg_92 <- pm25_ref_replaceavg %>%
  filter(year==1992)

pm25_ref_replaceavg_ratios <- full_join(pm25_ref_replaceavg_89,pm25_ref_replaceavg_90,by=c("mon"))
pm25_ref_replaceavg_ratios <- full_join(pm25_ref_replaceavg_ratios,pm25_ref_replaceavg_91,by=c("mon"))
pm25_ref_replaceavg_ratios <- full_join(pm25_ref_replaceavg_ratios,pm25_ref_replaceavg_92,by=c("mon"))

```

The pearson correlations of month to month ratios for years of 89 to 90, 90 to 91, and 91 to 92 are `r cor(pm25_ref_replaceavg_ratios$ratio.x, pm25_ref_replaceavg_ratios$ratio.y, method="pearson")`, `r cor(pm25_ref_replaceavg_ratios$ratio.y, pm25_ref_replaceavg_ratios$ratio.x.x, method="pearson")`
, and `r cor(pm25_ref_replaceavg_ratios$ratio.x.x, pm25_ref_replaceavg_ratios$ratio.y.y, method="pearson")`, respectively.

This shows that month to year variation between years is wide!

Let's see if we can impute PM2.5 based on the PM10 for that year by looking at this relationship in other years.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Join monthly and yearly averages of reference monitors to measurement data
ref_data <- full_join(no2_ref_monavg,no2_ref_yearavg,by=c("year"="year"))
ref_data <- full_join(ref_data,pm25_ref_monavg,by=c("year"="year","mon"="mon"))
ref_data <- full_join(ref_data,pm25_ref_yearavg,by=c("year"="year"))
ref_data <- full_join(ref_data,pm10_ref_monavg,by=c("year"="year","mon"="mon"))
ref_data <- full_join(ref_data,pm10_ref_yearavg,by=c("year"="year"))

ref_data <- ref_data %>%
  mutate(no2_mon_ratio = no2_mon_avg/no2_year_avg,
         pm25_mon_ratio = pm25_mon_avg/pm25_year_avg,
         pm10_mon_ratio = pm10_mon_avg/pm10_year_avg,
         no2_mon_ratio_sqd = no2_mon_ratio*no2_mon_ratio,
         pm10_mon_ratio_sqd = pm10_mon_ratio*pm10_mon_ratio)

summary(lm(pm25_mon_ratio~
             no2_mon_ratio + 
             pm10_mon_ratio,
           data=filter(ref_data,year<=1992)))

```

There are simply not enough data points to impute or rather extrapolate on the early end of PM2.5 data. As per discussion with Paloma on 2/13/19 in office hours, we'll simply exclude these PCWS data that do not have a PM2.5 reference background monitor (ie anything before June 1988).

We will continue creating the annual average for each household to be used as the outcome in the linear regressions.

#### PCWS LUR Creation
PCWS data is collected over 5 years, so much messier.

For homes measured in more than 1 year, only keep year with the most measurements that year. For homes measured the same # of times in different years, keep the older as it will be closer to more life years of CRS early life stage (Birth + ID1, 1980-1992).

```{r PCWS LUR Outcome Creation, echo=FALSE, message=FALSE, warning=FALSE}

# Create the annual average pollutant values


# NO2
pcws_ap_no2 <- data %>%
  filter(!is.na(no2_adj)) %>%
  group_by(hhid,styear) %>%
  summarise(no2_adj=mean(no2_adj, na.rm=T),
            no2_adj_n=n_distinct(no2ppb)) %>%
  mutate(hhid_x=paste0(hhid,"_A")) %>%
  dplyr::ungroup() %>%
  dplyr::select(-hhid) %>%
  group_by(hhid_x) %>%
  filter(no2_adj_n==max(no2_adj_n)) %>%
  filter(styear==min(styear)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-no2_adj_n)

# PM2.5
pcws_ap_pm25 <- data %>%
  filter(!is.na(pm25_adj)) %>%
  group_by(hhid,styear) %>%
  summarise(pm25_adj=mean(pm25_adj, na.rm=T),
            pm25_adj_n=n_distinct(pm25ugm3)) %>%
  mutate(hhid_x=paste0(hhid,"_A")) %>%
  dplyr::ungroup() %>%
  dplyr::select(-hhid) %>%
  group_by(hhid_x) %>%
  filter(pm25_adj_n==max(pm25_adj_n)) %>%
  filter(styear==min(styear)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-pm25_adj_n)

# PM10
pcws_ap_pm10 <- data %>%
  filter(!is.na(pm10_adj)) %>%
  group_by(hhid,styear) %>%
  summarise(pm10_adj=mean(pm10_adj, na.rm=T),
            pm10_adj_n=n_distinct(pm10ugm3)) %>%
  mutate(hhid_x=paste0(hhid,"_A")) %>%
  dplyr::ungroup() %>%
  dplyr::select(-hhid) %>%
  group_by(hhid_x) %>%
  filter(pm10_adj_n==max(pm10_adj_n)) %>%
  filter(styear==min(styear)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-pm10_adj_n)

# Combine all single measures back into full dataset
pcws_ap <- full_join(pcws_ap_no2, pcws_ap_pm25, by=c("hhid_x","styear"))
pcws_ap <- full_join(pcws_ap, pcws_ap_pm10, by=c("hhid_x","styear"))
pcws_ap <- pcws_ap %>%
  dplyr::select(hhid_x, styear, everything())

# Check to make sure each home only in dataset once
pcws_ap %>%
  group_by(hhid_x) %>%
  summarise(n_homes = n_distinct(hhid_x)) %>%
  arrange(desc(n_homes))

# All homes are only in this dataset once!
```


```{r PCWS LUR Predictor Assembly, echo=FALSE, message=FALSE, warning=FALSE}

pcws_lur_preds_pull <- function(folder_year){
  # Pull out predictors from each year folder, assign a 'styear' to them for merging to pollutant measurement dataset
  filenames=list.files(path=paste0("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/P5/Data/LUR/Predictors/",folder_year), full.names=TRUE)

  datalist = lapply(filenames, function(x){read.csv(file=x,header=T)})

  # Combine all predictor files and creates a styear field and deletes all "....x....y" styear artifacts from merging
  pcws_lur_preds <- datalist %>%
    Reduce(function(x,y) left_join(x,y,by="hhid_x"), .) %>% # merge all predictor csvs together
    mutate(year = styear.x) %>%
    dplyr::select(-starts_with("styear")) %>%
    mutate(styear = year) %>%
    dplyr::select(-year)
  
  pcws_lur_preds

}

# Save the LUR predictor file for each folder year
pcws_lur_preds_1987 <- pcws_lur_preds_pull('1987')
pcws_lur_preds_1988 <- pcws_lur_preds_pull('1988')
pcws_lur_preds_1989 <- pcws_lur_preds_pull('1989')
pcws_lur_preds_1990 <- pcws_lur_preds_pull('1990')
pcws_lur_preds_1991 <- pcws_lur_preds_pull('1991')



# Combine all the predictor sets
pcws_lur_preds <- bind_rows(pcws_lur_preds_1987,
                             pcws_lur_preds_1988,
                             pcws_lur_preds_1989,
                             pcws_lur_preds_1990,
                             pcws_lur_preds_1991)

# Combine air pollution values and predictors
pcws_lur_data <- inner_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))

# Check for homes iwth no addresses and make sure they really don't have anything
pcws_lur_data_nonjoin <- anti_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))
# 21 homes with no address data, many unique to 1992, which we know have no address data
       

# Correct any predictor values (starting with the 5th column) that are NA to 0
pcws_lur_data[, 6:ncol(pcws_lur_data)][is.na(pcws_lur_data[, 6:ncol(pcws_lur_data)])] <- 0

# Remove predictor columns with all 0s, as this will cause the regression to fail
pcws_lur_data <- pcws_lur_data[, apply(pcws_lur_data, 2, function(x) !all(x==0))] 

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
pcws_lur_data <- pcws_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

# NOTE - we are dropping observations that start in 1992 - there are results data but no address info

pcws_lur_data <- filter(pcws_lur_data, styear!=1992)

```


```{r PCWS LUR, echo=FALSE, message=FALSE, warning=FALSE}

escape_lur_mod <- function(pollutant) {
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  pcws_lur_data_TEMP <- filter(pcws_lur_data, !is.na(eval(parse(text=pollutant_outcome))))
  
  # Remove predictor columns with all 0s, as this will cause the regression to fail
  pcws_lur_data_TEMP <- pcws_lur_data_TEMP[, apply(pcws_lur_data_TEMP, 2, function(x) !all(x==0))]

  # Create an expected coefficient direction field variable (eg "expected_coef_no2")
  expected_coef <- paste0("expected_coef_",pollutant)
  # And then create a specific file and field designation for the pollutant model df of coefficients (eg"pred_coef_test$expected_coef_no2")
  pred_coef_test.expected_coef <- as.character(paste0('pred_coef_test$', expected_coef))

  # Read in the expected coefficients pre-determined from previous research/literature
  preds_expected_coefs <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/lur/preds_expected_coefs.csv", header = T)

  preds_expected_coefs$pred <- as.character(preds_expected_coefs$pred)
  
  # Univariate regressions to find starting variable for pollutant
  
  # Pull out predictor names
  pred <- colnames(pcws_lur_data_TEMP[6:ncol(pcws_lur_data_TEMP)])
  
  # Run simple regressions with all predictors
  # Pull resulting adjusted R2s
  adjr2<-apply(pcws_lur_data_TEMP[6:ncol(pcws_lur_data_TEMP)], 2, function(x) summary(lm(eval(parse(text=pollutant_outcome)) ~ x,data=pcws_lur_data_TEMP))$adj.r.squared)
  
  # Pull resulting coefficients
  # coef<-apply(pcws_lur_data_TEMP[6:ncol(pcws_lur_data_TEMP)], 2, function(x) coef(summary(lm(eval(parse(text=pollutant_outcome)) ~ x,data=pcws_lur_data_TEMP)))[2,1])
  
    coef<-apply(pcws_lur_data_TEMP[6:ncol(pcws_lur_data_TEMP)], 2, function(x) tidy(lm(eval(parse(text=pollutant_outcome)) ~ x,data=pcws_lur_data_TEMP))[2,2])
  
  pcws_lur_simple_out <- data.frame(pred, coef, adjr2) # put the estimated adj r2 and coefs in data frame
  
  rownames(pcws_lur_simple_out) <- NULL
  
  # Create a predictor direction for the coefficients in the regression
  pcws_lur_simple_out <- pcws_lur_simple_out %>%
    mutate(coef_direction = ifelse(coef>0,"positive","negative")) 
  
  # Merge by predictor name to get expected coef direction data 
  pcws_lur_simple_out <- left_join(pcws_lur_simple_out, preds_expected_coefs, by=c("pred")) 
  
  # Filter out predictors that do not meet expected coefficient direction (those without a determined direction stay regardless) and choose the highest adjusted R2 value of those that remain for the seed predictor for this model
  pcws_lur_simple_out <- pcws_lur_simple_out %>%
    filter(eval(parse(text=expected_coef)) == "undetermined" | coef_direction == eval(parse(text=expected_coef))) %>%
    arrange(desc(adjr2))
  
  # Define the seed predictor
  seed_pred <- pcws_lur_simple_out$pred[1]
  
  # Reload all the predictors into a list, but cut out the seed predictor so it can be moved to the first position
  predictors <- pred[!pred %in% eval(seed_pred)]
  predictors <- c(seed_pred, predictors, "styear")
  
  # Seed model
  mod_form <- formula(paste0(pollutant_outcome,"~",seed_pred))
  mod <- lm(mod_form, pcws_lur_data_TEMP)
  
  # Seed adj R2
  adjr2 <- rep(0,length(predictors))
  adjr2[1] <- summary(mod)$adj.r.squared
  
  # For all preds in the list
    # Add in new pred, test model and assumptions
      # If assumptions are met, then keep this updated model with new pred, update it as the working model and make it next starting model
  
      # If assumptions are NOT met, then go back to previously saved working by removing the most recently added predictor
  
  for (i in 2:length(predictors)){
    
    mod_form <- update(mod_form,  as.formula(paste('~ . +', predictors[i])))
    mod <- lm(mod_form,pcws_lur_data_TEMP)
  
    adjr2[i] <- glance(summary(mod))[1,2]
    
    if (adjr2[i] > (adjr2[i-1] + 0.01)) {  
      print(paste0("Hooray! This predictor increased model adj R2 test by >0.01"))
      
      pred_coef <- tidy(mod)
    
      # Create a predictor direction for the coefficients in the regression
      pred_coef$coef_direction <- ifelse(pred_coef$estimate>0,"positive","negative")
      pred_coef$pred <- pred_coef$term
      
      # Merge by predictor name to get expected coef direction data
      pred_coef_test <- inner_join(pred_coef, preds_expected_coefs, by=c("pred"))
    
      if(all((as.character(eval(parse(text=pred_coef_test.expected_coef))) == "undetermined") |
         (as.character(eval(parse(text=pred_coef_test.expected_coef))) == pred_coef_test$coef_direction))) {     
          print(paste0("Hooray! Coefficients for predictor and previous predictors all meet predetermined directions. This predictor will be added to the model."))
      } else {            
          # code if 1 passes, 2 fails
        print(paste0("Boo! This predictor increased the adj R2 >0.01 compared to the previous model, but the coefficient for this predictor or another didn't match the predetermined direction."))
        
          # Turn formular in text, then find the first (+) sign searching from the right (eg. the most recently added predictor that's no good), and cut this section of "+ predictorname" and change it back to formula and model
          mod_form_text <- as.character(mod_form[3])
          pred_to_cut <- paste0(" + ",predictors[i])
          mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
          mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
          mod <- lm(mod_form,pcws_lur_data_TEMP)

      }
  } else {            
      # code if 1 fails
          print(paste0("Boo! This predictor didn't increase the adj R2 >0.01 compared to the previous model."))
    
          # Turn formular in text, then find the first (+) sign searching from the right (eg. the most recently added predictor that's no good), and cut this section of "+ predictorname" and change it back to formula and model
          mod_form_text <- as.character(mod_form[3])
          pred_to_cut <- paste0(" + ",predictors[i])
          mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
          mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
          mod <- lm(mod_form,pcws_lur_data_TEMP)
  
  }
  
  }
  
  pvaltest_preds <- tidy(mod)

  while(any(is.na(pvaltest_preds$p.value))){
  var_with_NA_pvalue <- pvaltest_preds[(which(pvaltest_preds$p.value == max(pvaltest_preds$p.value))),1]  # get the var with max pvalue
  mod_form_text <- as.character(mod_form[3])
  pred_to_cut <- paste0(" + ",var_with_NA_pvalue)
  mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
  mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
  mod <- lm(mod_form,pcws_lur_data_TEMP)
  pvaltest_preds <- tidy(mod)

  }
  
  # Pulling predictors one by one with pvalues>0.1
  pvaltest_preds <- tidy(mod)
  
  # Filter out the intercept as a candidate for this approach
  pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")

  while(any(pvaltest_preds$p.value > 0.1)){
    var_with_max_pvalue <- pvaltest_preds[(which(pvaltest_preds$p.value == max(pvaltest_preds$p.value))),1]  # get the var with max pvalue
    
    # Cut out the predictor with pvalue > 0.1
    mod_form_text <- as.character(mod_form[3])
    pred_to_cut <- paste0(" + ",var_with_max_pvalue)
    mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
    mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
    mod <- lm(mod_form,pcws_lur_data_TEMP)
    pvaltest_preds <- tidy(mod) 
      
    # Filter out the intercept as a candidate for this approach for the next 'pass'
    pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")
  }
  
  all_vifs <- vif(mod)
signif_all <- names(all_vifs)

  
  while(any(all_vifs > 3)){ #NOTE if vif>3, then exclude from model, starting with largest VIF first as needed
  var_with_max_vif <- names(which(all_vifs == max(all_vifs)))  # get the var with max vif

  # Cut out the predictor with pvalue > 0.1
  mod_form_text <- as.character(mod_form[3])
  pred_to_cut <- paste0(" + ",var_with_max_pvalue)
  mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
  mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
  mod <- lm(mod_form,pcws_lur_data_TEMP)
  
  all_vifs <- car::vif(selectedMod)
  }

# We assessed a high Cook’s D value (i.e. >1), indicating an influential observation, because it could be caused by an (extreme) high or low concentration of one of the site(s) or by an included predictor variable with extreme values or many zero values. In such case, the developed model was applied to all sites minus the site with the high Cook’s D value and the changes in model structure (parameter estimates and p-values of included variables, and model R2)were evaluated. If the high Cook’s D value was caused by one of the included predictor variables (indicated by a large change in parameter estimate for that variable without the site), a new LUR model was developed using all sites but without offering that specific predictor variable to the model. 

# Cook's D plot
# identify D values 4/(n-k-1) 
cutoff <- 4/((nrow(pcws_lur_data)-length(mod$coefficients)-2)) 
plot(mod, which=4, cook.levels=cutoff, labels.id = pcws_lur_data$hhid_x)

cooksd <- cooks.distance(mod)

if (any(cooksd>1)) {
    print("An observation(s) has a Cook's Distances >1! Review the plot to investigate.  Here are next steps from Beelen et al 2013 (the ESCAPE NO2/x LUR paper) : We assessed a high Cook’s D value (i.e. >1), indicating an influential observation, because it could be caused by an (extreme) high or low concentration of one of the site(s) or by an included predictor variable with extreme values or many zero values. In such case, the developed model was applied to all sites minus the site with the high Cook’s D value and the changes in model structure (parameter estimates and p-values of included variables, and model R2)were evaluated. If the high Cook’s D value was caused by one of the included predictor variables (indicated by a large change in parameter estimate for that variable without the site), a new LUR model was developed using all sites but without offering that specific predictor variable to the model.")
  
 
  cutoff <- 4/((nrow(pcws_lur_data)-length(mod$coefficients)-2)) 
  plot(mod, which=4, cook.levels=cutoff, labels.id = pcws_lur_data$hhid_x)
} else {
  print("All Cook's Distances are <1; no need to investigate for outsize influence.")
}



  return(mod)
}

mod <- escape_lur_mod('no2')
summary(mod)

mod <- escape_lur_mod('pm10') 
summary(mod)

# # # #  Error in eval(predvars, data, env) : object 'roads_rl_10000' not found

mod <- escape_lur_mod('pm25')
summary(mod)

```


```{r PCWS LUR Validation, echo=FALSE, message=FALSE, warning=FALSE}

#Leave one out Cross Validation
# define training control
train_control <- trainControl(method="LOOCV")
# train the model
model_loocv <- train(no2_adj~busstops_5000 +
                       caline_pm25 +
                       elev
                     ,data=filter(pcws_lur_data, !is.na(no2_adj)), trControl=train_control, method="lm")
# summarize results
print(model_loocv)

```

#### TAPS LUR Creation
TAPS data is relatively easy to work with and this has already been pulled in with the script TAPS_Data_Cleaning. It does QC and does the annual averages just as ESCAPE did.





