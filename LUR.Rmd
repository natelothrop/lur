---
title: "Lothrop PhD - Aim 1"
author: "Nathan Lothrop"
date: "June 18, 2019"
output: html_document
---


## General LUR Workflow

In order to create the best externally validated LUR model for each pollutant, there are several steps: 

1. wrangling the source data for TAPS and PCWS, as well as PDEQ background monitor data; 
2. developing the LUR model using the ESCAPE method; and 
3. externally validating the model with 4 different ways of adjusting for changes over time.

### Wrangling Study Data

Basic steps:

1. Pull in study data
2. pull in background PDEQ monitor
3. create annual avg of each pollutant via ESCAPE method

However PCWS didn't follow ESCAPE so this is slightly more involved and we will have to make assumptions compared to TAPS.

```{r Load Packages, echo=FALSE, results='hide', message=FALSE, warning=FALSE}

# Read in packages
packages <- c('Hmisc', 'corrplot', 'tidyverse', 'lubridate', 'broom', 'rlist', 'olsrr', 'car', 'AICcmodavg', 'lctools', 'sf', 'sp', 'leaflet', 'caret',  'lmerTest','formattable', 'corrr',  'PerformanceAnalytics', 'xts', 'factoextra', 'rms', 'nnet', 'tmap', 'raster', 'cartography',
              'data.table',
              'knitr', 'ggpubr','rms','modelr')


package.check <- lapply(packages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})
```


```{r PCWS vs TAPS Data Load and NO2 Palmes to Ogawa Calibration, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

# Clear environment of temporary files
rm(list=ls())


#read in all p5 data
p5all<- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/PCWS/Data/LUR/Results/P5_Master.csv")

#subset out the monitor data that has hhid >5000
monitor<-subset(p5all,hhid>5001)
data<-subset(p5all,hhid<5001)

str(data)

keeps<-c("hhid","stday","stmon","styear",
         "endday","endmon","endyear"
         ,"no2ppb", "pm25ugm3","pm10ugm3")
data<-data[keeps]

# Average duplicates (same run period) and remove anything from 1992 (no GIS coords for these homes)
data <- data %>%
  group_by(hhid,stday,stmon,styear,endday,endmon,endyear) %>%
  summarise(no2ppb = mean(no2ppb),
         pm25ugm3 = mean(pm25ugm3),
         pm10ugm3 = mean(pm10ugm3)) %>%
  filter(styear != 92)

# Assign 1 of 3 season values (Winter, Intermed, Summer)
data$stssn3 <- ifelse((data$stmon==3 | data$stmon==4 | data$stmon==9 | data$stmon==10), 2,
                     ifelse((data$stmon==1 | data$stmon==2 | data$stmon==11 | data$stmon==12), 1, 3))
                               
data$stssn3 <- factor(data$stssn3,
                    levels = c(1,2,3),
                    labels = c("Winter", "Intermed.", "Summer"))

# Assign 1 of 4 season values (Winter, Spring, Summer, Fall)
data$stssn4 <- ifelse((data$stmon==1 | data$stmon==2 | data$stmon==11 | data$stmon==12), 1,
                      ifelse((data$stmon==3 | data$stmon==4), 2,
                              ifelse((data$stmon==5 | data$stmon==6 | data$stmon==7 | data$stmon==8), 3, 4)))
                               
data$stssn4 <- factor(data$stssn4,
                    levels = c(1,2,3,4),
                    labels = c("Winter", "Spring", "Summer", "Fall"))

# Calibrate the Palmes NO2 measures to Ogawa measures using Caesaroni et al. 2012 (0.68 * Palmes) + 13.53 but this is all ug/m3, but must change units from ppb to ug/m3 first, then change back using conversion factor of 1ppb = 1.88 ug/m3 from here https://uk-air.defra.gov.uk/assets/documents/reports/cat06/0502160851_Conversion_Factors_Between_ppb_and.pdf:

hist(data$no2ppb)
summary(data$no2ppb)

data$no2ppb <- ((0.68 * (data$no2ppb * 1.88)) + 13.53)/1.88

hist(data$no2ppb)
summary(data$no2ppb)

# Note that the range of values is crunched down, making the model generally less predictive...this ultimately makes the model less predictive!!!


# Create pollutant subsets
datano2 <- subset(data,!is.na(no2ppb))
datapm25 <- subset(data,!is.na(pm25ugm3))
datapm10 <- subset(data,!is.na(pm10ugm3))
```
######Monitor sites with 2 or more measures in different SEASONS in the same year (non-ESCAPE 4-season approach) for PM2.5:

**As per Beamer lab meeting 6/18/19, discussion came up about how all the most temporally representative data (multiple measures at many homes over year) were done for PM2.5 in 1987 and 1988, but there's no regulatory monitor running till 1989, meaning we have no way to do temporal correction, resulting in us dropping these observations till a reg monitor comes online. Paloma suggested we look to make sure that Mary Kay didn't do any PCWS background monitoring that we could use in place of a regulatory monitor. **
```{r, echo=FALSE, message=FALSE, warning=FALSE}

monitor %>%
  dplyr::group_by(hhid,styear,stssn) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm25ugm3)) %>%
  arrange(styear)
```
**This shows how sparse the PCWS background monitoring was for PM2.5, as there are none that run the full year for 1987 or 1988, so we will NOT use any of the PM2.5 till a regulatory monitor comes online for the linear model (ie ESCAPE), but we WILL include all PM2.5 data (1987 and 1988) in the mixed effects model!**

######PCWS NO2
```{r, echo=FALSE, warning=FALSE}

datano2 %>%
  group_by(styear) %>%
  summarise(n.obs=length(no2ppb),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(no2ppb))),
            mean=mean(no2ppb),
            geosd=exp(sd(log(no2ppb))),
            min=min(no2ppb),
            max=max(no2ppb))


data %>%
  filter(!is.na(no2ppb)) %>%
  group_by(hhid) %>%
  summarise(n.obs=length(no2ppb),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(no2ppb))),
            mean=mean(no2ppb),
            geosd=exp(sd(log(no2ppb))),
            min=min(no2ppb),
            max=max(no2ppb))
```

######PCWS PM2.5
```{r, echo=FALSE, warning=FALSE}

datapm25 %>%
  group_by(styear) %>%
  summarise(n.obs=length(pm25ugm3),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(pm25ugm3))),
            mean=mean(pm25ugm3),
            geosd=exp(sd(log(pm25ugm3))),
            min=min(pm25ugm3),
            max=max(pm25ugm3))
```

######PCWS PM10
```{r, echo=FALSE, warning=FALSE}

datapm10 %>%
  group_by(styear) %>%
  summarise(n.obs=length(pm10ugm3),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(pm10ugm3))),
            mean=mean(pm10ugm3),
            geosd=exp(sd(log(pm10ugm3))),
            min=min(pm10ugm3),
            max=max(pm10ugm3))

data %>%
  filter(!is.na(pm10ugm3)) %>%
  group_by(hhid) %>%
  summarise(n.obs=length(pm10ugm3),
            n.homes=n_distinct(hhid),
            geomean=exp(mean(log(pm10ugm3))),
            mean=mean(pm10ugm3),
            geosd=exp(sd(log(pm10ugm3))),
            min=min(pm10ugm3),
            max=max(pm10ugm3))

```
Many PCWS homes are measured more than once in a year, but often are measured back to back (one week, then the next). We will use all measures (and correct for changes in air pollution levels over time), as ESCAPE allows for 1-3 or more measures throughout the year (see page 9 and 10 here: http://escapeproject.eu/manuals/ESCAPE_Exposure-manualv9.pdf).

However, before committing to this, we will investigate how many homes have measures over multiple years, because if they do, then it may not make sense in LUR development if we have predictors that change over the time span to put all homes in the same model.


######Homes with more than 2 measures in different MONTHS in the same year for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

# data %>%
#   filter(!is.na(no2ppb)) %>%
#   dplyr::group_by(hhid,styear, stmon) %>%
#   summarise(n_homes = n_distinct(hhid),
#             n_obs_no2 = n_distinct(no2ppb)) %>%
#   arrange(desc(n_obs_no2, hhid))
data %>%
  filter(!is.na(no2ppb)) %>%
  dplyr::group_by(hhid,styear, stmon) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(no2ppb))

datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_bymonth = n_distinct(stmon))

datano2 %>%
  filter(no2_bymonth>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(no2ppb))
```
So for NO2, there are multiple homes in 87-89 with 2 or more measures in the same month and different months in the same year.


Let's check this same thing for PM2.5.

######Homes with 2 or more measures in different MONTHS in the same year for PM2.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm25 <- datapm25 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm25_bymonth = n_distinct(stmon))

datapm25 %>%
  filter(pm25_bymonth>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm25ugm3))
```
For PM2.5, only 2 homes have more than 1 measure in different months. 

######Homes with 2 or more measures in different MONTHS in the same year for PM10:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm10 <- datapm10 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm10_bymonth = n_distinct(stmon))

datapm10 %>%
  filter(pm10_bymonth>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm10ugm3))
```
For PM10, only 2 homes have more than 1 measure in different months. 


######Homes with 2 or more measures in different SEASONS in the same year (ESCAPE 3-season approach) for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_byseason3 = n_distinct(stssn3))

datano2 %>%
  filter(no2_byseason3>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(no2ppb))
``` 

######Homes with 2 or more measures in different SEASONS in the same year (ESCAPE 3-season approach) for PM2.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# 
# data %>%
#   filter(!is.na(no2ppb)) %>%
#   dplyr::group_by(hhid,styear, stssn3) %>%
#   summarise(n_homes = n_distinct(hhid),
#             n_obs_no2 = n_distinct(no2ppb)) %>%
#   arrange(desc(n_obs_no2, hhid))

datapm25 <- datapm25 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm25_byseason3 = n_distinct(stssn3))

datapm25 %>%
  filter(pm25_byseason3>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm25ugm3))
``` 
There are NO homes with 2 or more PM2.5 measures in different ESCAPE seasons in same year.

######Homes with 2 or more measures in different SEASONS in the same year (ESCAPE 3-season approach) for PM10:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm10 <- datapm10 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm10_byseason3 = n_distinct(stssn3))

datapm10 %>%
  filter(pm10_byseason3>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm10ugm3))
``` 
There are NO homes with 2 or more PM10 measures in different ESCAPE seasons in same year.

######Homes with 2 or more measures in different SEASONS in the same year (non-ESCAPE 4-season approach) for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_byseason4 = n_distinct(stssn4))

datano2 %>%
  filter(no2_byseason4>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(no2ppb))
``` 
######Homes with 2 or more measures in different SEASONS in the same year (non-ESCAPE 4-season approach) for PM2.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm25 <- datapm25 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm25_byseason4 = n_distinct(stssn4))

datapm25 %>%
  filter(pm25_byseason4>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm25ugm3))
```
No homes again for PM2.5, using 4 season approach.

######Homes with 2 or more measures in different SEASONS in the same year (non-ESCAPE 4-season approach) for PM10:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm10 <- datapm10 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm10_byseason4 = n_distinct(stssn4))

datapm10 %>%
  filter(pm10_byseason4>1) %>%
  group_by(styear) %>%
  summarise(n_homes = n_distinct(hhid),
            n_obs = n_distinct(pm10ugm3))
```
No homes again for PM10, using 4 season approach.

Since only NO2 has >1 measure/season or month in more than a handful of homes like PM2.5 and PM10, we'll base our season definition off this. 

**We'll go with the 3 season as this the ESCAPE approach** which we want to follow anyways and there are no difference b/w 3 and 4 month season definitions.

Now let's look at whether there are homes that have >1 NO2 measure in distinct months/year IN multiple years and the same, but for seasons in multiple years.

######Homes with 2 or more measures in different MONTHS in the same year over MULTIPLE years for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

#create frequency of measurement by year and season
#no2
datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_bymonth = n_distinct(stmon))

datano2 %>%
  group_by(hhid,styear) %>%
  summarise(n_obs_no2 = n_distinct(no2ppb)) %>%
  filter(n_obs_no2>1) %>%
  arrange(hhid,desc(n_obs_no2))
```

There are many homes that have obs in more than 1 year (>1 distinct measure/month).


######Homes with 2 or more measures in different SEASONS in the same year over MULTIPLE years for NO2:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

#create frequency of measurement by year and season
#no2
datano2 <- datano2 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_byseason3 = n_distinct(stssn3))

datano2 %>%
  filter(no2_byseason3>1) %>%
  group_by(hhid) %>%
  summarise(n_years = n_distinct(styear),
            n_obs = n_distinct(no2ppb)) %>%
  arrange(desc(n_years,hhid))

datano2 %>%
  group_by(hhid) %>%
  summarise(n_unique_years = n_distinct(styear),
            n_obs = n_distinct(no2ppb)) %>%
  filter(n_unique_years>2) %>%
  arrange(desc(n_unique_years,n_obs))
```
**NO2**
Of 334 homes, 155 have measures in 2 different years, amd 15 have measures in 3 different years.

######PM2.5 test for how many homes have measures in multiple seasons and multiple years:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm25 <- datapm25 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm25_byseason3 = n_distinct(stssn3))

datapm25 %>%
  filter(pm25_byseason3>1) %>%
  group_by(hhid) %>%
  summarise(n_years = n_distinct(styear),
            n_obs = n_distinct(pm25ugm3)) %>%
  arrange(desc(n_years,hhid))

datapm25 %>%
  group_by(hhid) %>%
  summarise(n_unique_years = n_distinct(styear),
            n_obs = n_distinct(pm25ugm3)) %>%
  # filter(n_unique_years>1) %>%
  arrange(desc(n_unique_years,n_obs))
```
**PM2.5**
Of 60 homes, 7 have measures in different years. The rest have only a single measure (in a single year).

######PM10 test for how many homes have measures in multiple seasons and multiple years:
```{r, echo=FALSE, message=FALSE, warning=FALSE}

datapm10 <- datapm10 %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(pm10_byseason3 = n_distinct(stssn3))

datapm10 %>%
  filter(pm10_byseason3>1) %>%
  group_by(hhid) %>%
  summarise(n_years = n_distinct(styear),
            n_obs = n_distinct(pm10ugm3)) %>%
  arrange(desc(n_years,hhid))

datapm10 %>%
  group_by(hhid) %>%
  summarise(n_unique_years = n_distinct(styear),
            n_obs = n_distinct(pm10ugm3)) %>%
  # filter(n_unique_years>1) %>%
  arrange(desc(n_unique_years,n_obs))
```

**Of 64 homes, 7 have measures in different years. The rest have only a single measure (in a single year).**



**After speaking with Dr. Beamer on 1/30/19 in office hours, the approach will be this:**

1. for a single pollutant (NO2?) do annual average for all homes, regardless of how many times they're measured
2. complete LUR model for this "all homes" approach (all homes get year-specific predictor, eg a home with 1987 avg will have the traffic volume predictors be named same as home from 1989, but have year-specific value)
3. then, as a sensitivity analysis, do it so that each home is included annual average just once in model




#### PCWS Annual Averages

##### Create Daily Avg for PDEQ Background Monitors
```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Read in NO2, PM2.5, and PM10 background values from the longest, constantly running PDEQ monitors for NAAQS compliance
# Downloaded in 20 year increments for all available data from 1980 - 2017 from https://aqs.epa.gov/api on 2018/12/28 with user: lothrop@email.arizona.edu and password 'rubycat56'
# Parameter codes are NO2: 42602 at 1 hour sample duration; PM2.5: 88502 (Acceptable PM2.5 AQI & Speciation Mass); PM10: 81102 (PM10 Total 0-10um STP), both at 24 hour sample duration
# NO2, PM2.5, and PM10 monitor sites that running longest aren't same (NO2: Alvernon/Craycroft; PM2.5: Saguaro Park; PM10: Orange Grove)
# Note that PM2.5 monitoring is taken offline temporarily at Saguaro Park 1994-1999

wd <- "/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/Data/PDEQHistoric"

no2_ref1 <- read.csv(paste0(wd,"/","NO2_1980_2000.txt"), header = T)
no2_ref2 <- read.csv(paste0(wd,"/","NO2_2000_2017.txt"), header = T)
no2_ref <- bind_rows(no2_ref1, no2_ref2)

pm25_ref1 <- read.csv(paste0(wd,"/","PM25_1980_2000.txt"), header = T)
pm25_ref1$Qualifier.Description <- as.factor(pm25_ref1$Qualifier.Description) #need to update field type because as an artifact of import
pm25_ref2 <- read.csv(paste0(wd,"/","PM25_2000_2017.txt"), header = T)
pm25_ref <- bind_rows(pm25_ref1, pm25_ref2)

pm10_ref1 <- read.csv(paste0(wd,"/","PM10_1980_2000.txt"), header = T)
pm10_ref2 <- read.csv(paste0(wd,"/","PM10_2000_2017.txt"), header = T)
pm10_ref <- bind_rows(pm10_ref1, pm10_ref2)

# Create annual averages for each pollutant type
no2_ref <- no2_ref %>%
  dplyr::group_by(Date.Local) %>%
  dplyr::summarise(no2_ref = mean(Sample.Measurement)) %>%
  dplyr::filter(!is.na(Date.Local) | !is.na(no2_ref)) %>%
  dplyr::select(Date.Local, no2_ref)

pm25_ref <- pm25_ref %>%
  dplyr::group_by(Date.Local) %>%
  dplyr::summarise(pm25_ref = mean(Sample.Measurement)) %>%
  dplyr::filter(!is.na(Date.Local) | !is.na(pm25_ref)) %>%
  dplyr::select(Date.Local, pm25_ref)
  
pm10_ref <- pm10_ref %>%
  dplyr::group_by(Date.Local) %>%
  dplyr::summarise(pm10_ref = mean(Sample.Measurement)) %>%
  dplyr::filter(!is.na(Date.Local) | !is.na(pm10_ref)) %>%
  dplyr::select(Date.Local, pm10_ref)

no2_ref <- filter(no2_ref, !is.na(no2_ref))
pm25_ref <- filter(pm25_ref, !is.na(pm25_ref))
pm10_ref <- filter(pm10_ref, !is.na(pm10_ref))

summary(no2_ref$no2_ref)

```
Since there are values below 0 that have no qualifier or description of an error or issue, we will truncate these values to 0 ppb.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

no2_ref$no2_ref <- ifelse(no2_ref$no2_ref < 0, 0, no2_ref$no2_ref)
summary(no2_ref$no2_ref)
summary(pm25_ref$pm25_ref)
summary(pm10_ref$pm10_ref)

```

Summary values all make sense now. The very high PM10 values all have description qualifiers on them in the raw data noting high wind events (eg dust storms), which are a known issue in this area that will push Pima County to violate PM10 NAAQS standards. Next step is to merge together by start date to relevant pollutant.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

read.data <- function(){
#read in all p5 data
p5all<- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/PCWS/Data/LUR/Results/P5_Master.csv")

#subset out the monitor data that has hhid >5000
monitor<-subset(p5all,hhid>5001)
data<-subset(p5all,hhid<5001)

str(data)

keeps<-c("hhid","stday","stmon","styear",
         "endday","endmon","endyear"
         ,"no2ppb", "pm25ugm3","pm10ugm3")
data<-data[keeps]

# Average duplicates (same run period)
data <- data %>%
  group_by(hhid,stday,stmon,styear,endday,endmon,endyear) %>%
  summarise(no2ppb = mean(no2ppb),
         pm25ugm3 = mean(pm25ugm3),
         pm10ugm3 = mean(pm10ugm3))


# Assign 1 of 3 season values (Winter, Intermed, Summer)
data$stssn3 <- ifelse((data$stmon==3 | data$stmon==4 | data$stmon==9 | data$stmon==10), 2,
                     ifelse((data$stmon==1 | data$stmon==2 | data$stmon==11 | data$stmon==12), 1, 3))
                               
data$stssn3 <- factor(data$stssn3,
                    levels = c(1,2,3),
                    labels = c("Winter", "Intermed.", "Summer"))

# Assign 1 of 4 season values (Winter, Spring, Summer, Fall)
data$stssn4 <- ifelse((data$stmon==1 | data$stmon==2 | data$stmon==11 | data$stmon==12), 1,
                      ifelse((data$stmon==3 | data$stmon==4), 2,
                              ifelse((data$stmon==5 | data$stmon==6 | data$stmon==7 | data$stmon==8), 3, 4)))
                               
data$stssn4 <- factor(data$stssn4,
                    levels = c(1,2,3,4),
                    labels = c("Winter", "Spring", "Summer", "Fall"))

datano2 <- subset(data,!is.na(no2ppb))
datapm25 <- subset(data,!is.na(pm25ugm3))
datapm10 <- subset(data,!is.na(pm10ugm3))
}


```

##### Create annual averages corrected for temporal changes
```{r Background Monitor Averaging, echo=FALSE, message=FALSE, warning=FALSE}

# Approach:
# 1. figure out what ref monitor dates are after the start date and before the end date of the home measurement (through decimalized dating)
# 2. for this ref date period, take the average of the pollutant measurement, then divide that by the annual average to get the ratio temporal correction
# 3. multiple the ratio temporal correction by the home measure for that period
# 4. average the product(s) from step 3, resulting in the home's annual average

# Create decimalized dates for ref data
no2_ref$date_dec <- decimal_date(as.Date(no2_ref$Date.Local,format="%Y-%m-%d"))
pm25_ref$date_dec <- decimal_date(as.Date(pm25_ref$Date.Local,format="%Y-%m-%d"))
pm10_ref$date_dec <- decimal_date(as.Date(pm10_ref$Date.Local,format="%Y-%m-%d"))

# Create monthly and annual averages for ref data-NO2
no2_ref$year <- as.integer(year(no2_ref$Date.Local))
no2_ref$mon <- as.integer(month(no2_ref$Date.Local))

no2_ref_yearavg <- no2_ref %>%
  group_by(year) %>%
  summarise(no2_year_avg=mean(no2_ref))

no2_ref_monavg <- no2_ref %>%
  group_by(mon, year) %>%
  summarise(no2_mon_avg=mean(no2_ref))

# Create monthly and annual averages for ref data-PM2.5
pm25_ref$year <- as.integer(year(pm25_ref$Date.Local))
pm25_ref$mon <- as.integer(month(pm25_ref$Date.Local))

pm25_ref_yearavg <- pm25_ref %>%
  group_by(year) %>%
  summarise(pm25_year_avg=mean(pm25_ref))

pm25_ref_monavg <- pm25_ref %>%
  group_by(mon, year) %>%
  summarise(pm25_mon_avg=mean(pm25_ref))

# Create monthly and annual averages for ref data-PM10
pm10_ref$year <- as.integer(year(pm10_ref$Date.Local))
pm10_ref$mon <- as.integer(month(pm10_ref$Date.Local))

pm10_ref_yearavg <- pm10_ref %>%
  group_by(year) %>%
  summarise(pm10_year_avg=mean(pm10_ref))

pm10_ref_monavg <- pm10_ref %>%
  group_by(mon, year) %>%
  summarise(pm10_mon_avg=mean(pm10_ref))

# Create decimalized dates (start and end) for measurement data
data$styear <- data$styear + 1900
data$endyear <- data$endyear + 1900

data$st_date <- as.Date(with(data, paste(styear, stmon, stday,sep="-")), "%Y-%m-%d")
data$end_date <- as.Date(with(data, paste(endyear, endmon, endday,sep="-")), "%Y-%m-%d")

data$stdate_dec <- decimal_date(as.Date(data$st_date,format="%Y-%m-%d"))
data$enddate_dec <- decimal_date(as.Date(data$end_date,format="%Y-%m-%d"))

# Join monthly and yearly averages of reference monitors to measurement data
data <- full_join(data,no2_ref_monavg,by=c("styear"="year","stmon"="mon"))
data <- full_join(data,no2_ref_yearavg,by=c("styear"="year"))
data <- full_join(data,pm25_ref_monavg,by=c("styear"="year","stmon"="mon"))
data <- full_join(data,pm25_ref_yearavg,by=c("styear"="year"))
data <- full_join(data,pm10_ref_monavg,by=c("styear"="year","stmon"="mon"))
data <- full_join(data,pm10_ref_yearavg,by=c("styear"="year"))

# Filter out those that have no hhid
data <- filter(data, !is.na(hhid))

# Create the ratio-adjusted NO2 measurement (no2_adj)
data$no2_adj <- 0
for(i in 1:nrow(data)){
  stdate <- c(data[i,15])
  enddate <- c(data[i,16])
  no2_period_avg <- mean(as.numeric(unlist(c(no2_ref[no2_ref$date_dec >= stdate & no2_ref$date_dec <= enddate,2])))) # calculate the average ref for that period of measurment
  if(!is.na(no2_period_avg)){ # Note!-not all measurement periods have a ref monitor running, so use the monthly average (of that starting measurement month) to annual ratio instead
    no2_period_ratio <- no2_period_avg/data[i,18] # calculate the ratio of ref period to annual avg
    data[i,23] <- no2_period_ratio * data[i,8] # adjust measurement by period to year ratio
  }else{
    no2_mon_ratio <- data[i,17]/data[i,18] # calculate the ratio of ref mon to annual avg
    data[i,23] <- no2_mon_ratio * data[i,8] # adjust measurement by month to year ratio
    }
}

# Look for NO2 measures that have a raw measurement but no temporally-adjusted concentration
sum(!is.na(data$no2ppb) & is.na(data$no2_adj))

# Create the ratio-adjusted PM2.5 measurement (pm25_adj)
data$pm25_adj <- 0
for(i in 1:nrow(data)){
  stdate <- c(data[i,15])
  enddate <- c(data[i,16])
  pm25_period_avg <- mean(as.numeric(unlist(c(pm25_ref[pm25_ref$date_dec >= stdate & pm25_ref$date_dec <= enddate,2])))) # calculate the average ref for that period of measurment
  if(!is.na(pm25_period_avg)){ # Note!-not all measurement periods have a ref monitor running, so use the monthly average (of that starting measurement month) to annual ratio instead
    pm25_period_ratio <- pm25_period_avg/data[i,20] # calculate the ratio of ref period to annual avg
    data[i,24] <- pm25_period_ratio * data[i,9] # adjust measurement by period to year ratio
  }else{
    pm25_mon_ratio <- data[i,19]/data[i,20] # calculate the ratio of ref mon to annual avg
    data[i,24] <- pm25_mon_ratio * data[i,9] # adjust measurement by month to year ratio
    }
}

# Look for PM2.5 measures that have a raw measurement but no temporally-adjusted concentration
sum(!is.na(data$pm25ugm3) & is.na(data$pm25_adj))

# Create the ratio-adjusted PM10 measurement (pm10_adj)
data$pm10_adj <- 0
for(i in 1:nrow(data)){
  stdate <- c(data[i,15])
  enddate <- c(data[i,16])
  pm10_period_avg <- mean(as.numeric(unlist(c(pm10_ref[pm10_ref$date_dec >= stdate & pm10_ref$date_dec <= enddate,2])))) # calculate the average ref for that period of measurment
  if(!is.na(pm10_period_avg)){ # Note!-not all measurement periods have a ref monitor running, so use the monthly average (of that starting measurement month) to annual ratio instead
    pm10_period_ratio <- pm10_period_avg/data[i,22] # calculate the ratio of ref period to annual avg
    data[i,25] <- pm10_period_ratio * data[i,10] # adjust measurement by period to year ratio
  }else{
    pm10_mon_ratio <- data[i,21]/data[i,22] # calculate the ratio of ref mon to annual avg
    data[i,25] <- pm10_mon_ratio * data[i,10] # adjust measurement by month to year ratio
    }
}

# Look for PM10 measures that have a raw measurement but no temporally-adjusted concentration
sum(!is.na(data$pm10ugm3) & is.na(data$pm10_adj))

```

##### Missing PM2.5 Ref Monitor Data
PM2.5 measures in PCWS started in March 1987, but reference background monitoring didn't start until June 1988. This leaves `r sum(!is.na(data$pm25ugm3) & is.na(data$pm25_adj))` homes without reference values to temporally correct. To correct, we will test to see if we can use the monthly ratio of the closest reference dates from a full monitoring year to fill in March 1987-May 1988 (eg using all of 1989).

```{r, echo=FALSE, message=FALSE, warning=FALSE}

pm25_ref_replaceavg <- full_join(pm25_ref_monavg,pm25_ref_yearavg,by=c("year"))
pm25_ref_replaceavg$ratio <- 0
pm25_ref_replaceavg$ratio <- pm25_ref_replaceavg$pm25_mon_avg/pm25_ref_replaceavg$pm25_year_avg

pm25_ref_replaceavg_88 <- pm25_ref_replaceavg %>%
  filter(year==1988)
pm25_ref_replaceavg_89 <- pm25_ref_replaceavg %>%
  filter(year==1989)
pm25_ref_replaceavg_90 <- pm25_ref_replaceavg %>%
  filter(year==1990)
pm25_ref_replaceavg_91 <- pm25_ref_replaceavg %>%
  filter(year==1991)
pm25_ref_replaceavg_92 <- pm25_ref_replaceavg %>%
  filter(year==1992)

pm25_ref_replaceavg_ratios <- full_join(pm25_ref_replaceavg_89,pm25_ref_replaceavg_90,by=c("mon"))
pm25_ref_replaceavg_ratios <- full_join(pm25_ref_replaceavg_ratios,pm25_ref_replaceavg_91,by=c("mon"))
pm25_ref_replaceavg_ratios <- full_join(pm25_ref_replaceavg_ratios,pm25_ref_replaceavg_92,by=c("mon"))

```

The pearson correlations of month to month ratios for years of 89 to 90, 90 to 91, and 91 to 92 are `r cor(pm25_ref_replaceavg_ratios$ratio.x, pm25_ref_replaceavg_ratios$ratio.y, method="pearson")`, `r cor(pm25_ref_replaceavg_ratios$ratio.y, pm25_ref_replaceavg_ratios$ratio.x.x, method="pearson")`
, and `r cor(pm25_ref_replaceavg_ratios$ratio.x.x, pm25_ref_replaceavg_ratios$ratio.y.y, method="pearson")`, respectively.

This shows that month to year variation between years is wide!

Let's see if we can impute PM2.5 based on the PM10 for that year by looking at this relationship in other years.

```{r PDEQ Background Monitor Averaging Files, echo=FALSE, message=FALSE, warning=FALSE}
# Join monthly and yearly averages of reference monitors to measurement data
ref_data <- full_join(no2_ref_monavg,no2_ref_yearavg,by=c("year"="year"))
ref_data <- full_join(ref_data,pm25_ref_monavg,by=c("year"="year","mon"="mon"))
ref_data <- full_join(ref_data,pm25_ref_yearavg,by=c("year"="year"))
ref_data <- full_join(ref_data,pm10_ref_monavg,by=c("year"="year","mon"="mon"))
ref_data <- full_join(ref_data,pm10_ref_yearavg,by=c("year"="year"))

ref_data <- ref_data %>%
  mutate(no2_mon_ratio = no2_mon_avg/no2_year_avg,
         pm25_mon_ratio = pm25_mon_avg/pm25_year_avg,
         pm10_mon_ratio = pm10_mon_avg/pm10_year_avg,
         no2_mon_ratio_sqd = no2_mon_ratio*no2_mon_ratio,
         pm10_mon_ratio_sqd = pm10_mon_ratio*pm10_mon_ratio)

summary(lm(pm25_mon_ratio~
             no2_mon_ratio + 
             pm10_mon_ratio,
           data=filter(ref_data,year<=1992)))

# Make yearly avg file
ref_data_year <- full_join(no2_ref_yearavg,pm25_ref_yearavg,by=c("year"="year"))
ref_data_year <- full_join(ref_data_year,pm10_ref_yearavg,by=c("year"="year"))


```

There are simply not enough data points to impute or rather extrapolate on the early end of PM2.5 data. As per discussion with Paloma on 2/13/19 in office hours, we'll simply exclude these PCWS data that do not have a PM2.5 reference background monitor (ie anything before June 1988).

We will continue creating the annual average for each household to be used as the outcome in the linear regressions.

#### PCWS LUR Creation
PCWS data is collected 1987-1992, however there are some homes that have no address data, notably ALL homes done in 1992 have NO address, thus cannot be used.

Due to there not being a background monitor available for PM2.5 until June 1988, we will exclude all values from 1988 and earlier (eg only PM2.5 values from 1989-1991)

For homes measured in more than 1 year, only keep year with the most measurements that year. For homes measured the same # of times in different years, keep the older as it will be closer to more life years of CRS early life stage (Birth + ID1, 1980-1992).

```{r PCWS LUR Outcome Creation, echo=FALSE, message=FALSE, warning=FALSE}

# Create the annual average pollutant values for all homes, regardless of number of measures/year
# NO2
pcws_ap_no2 <- data %>%
  filter(!is.na(no2_adj)) %>%
  group_by(hhid,styear) %>%
  summarise(no2_adj=mean(no2_adj, na.rm=T),
            no2_adj_n=n_distinct(no2ppb)) %>%
  mutate(hhid_x=paste0(hhid,"_A")) %>%
  dplyr::ungroup() %>%
  dplyr::select(-hhid) %>%
  group_by(hhid_x) %>%
  filter(no2_adj_n==max(no2_adj_n)) %>%
  filter(styear==min(styear)) %>%
  dplyr::ungroup()

# PM2.5
pcws_ap_pm25 <- data %>%
  filter(!is.na(pm25_adj)) %>%
  filter(styear>1988) %>%
  group_by(hhid,styear) %>%
  summarise(pm25_adj=mean(pm25_adj, na.rm=T),
            pm25_adj_n=n_distinct(pm25ugm3)) %>%
  mutate(hhid_x=paste0(hhid,"_A")) %>%
  dplyr::ungroup() %>%
  dplyr::select(-hhid) %>%
  group_by(hhid_x) %>%
  filter(pm25_adj_n==max(pm25_adj_n)) %>%
  filter(styear==min(styear)) %>%
  dplyr::ungroup()

# PM10
pcws_ap_pm10 <- data %>%
  filter(!is.na(pm10_adj)) %>%
  group_by(hhid,styear) %>%
  summarise(pm10_adj=mean(pm10_adj, na.rm=T),
            pm10_adj_n=n_distinct(pm10ugm3)) %>%
  mutate(hhid_x=paste0(hhid,"_A")) %>%
  dplyr::ungroup() %>%
  dplyr::select(-hhid) %>%
  group_by(hhid_x) %>%
  filter(pm10_adj_n==max(pm10_adj_n)) %>%
  filter(styear==min(styear)) %>%
  dplyr::ungroup()

# Combine all single measures back into full dataset
pcws_ap <- full_join(pcws_ap_no2, pcws_ap_pm25, by=c("hhid_x","styear"))
pcws_ap <- full_join(pcws_ap, pcws_ap_pm10, by=c("hhid_x","styear"))
pcws_ap <- pcws_ap %>%
  dplyr::select(hhid_x,styear,no2_adj,pm25_adj,pm10_adj,everything())

# Check to make sure each home only in dataset once
pcws_ap %>%
  group_by(hhid_x) %>%
  summarise(n_homes = n_distinct(hhid_x)) %>%
  arrange(desc(n_homes))

# All homes are only in this dataset once!
```

```{r PCWS LUR Predictor Assembly, echo=FALSE, message=FALSE, warning=FALSE}

pcws_lur_preds_pull <- function(folder_year){
  # Pull out predictors from each year folder, assign a 'styear' to them for merging to pollutant measurement dataset
  filenames=list.files(path=paste0("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/PCWS/Data/LUR/Predictors/",folder_year), full.names=TRUE)

  datalist = lapply(filenames, function(x){read.csv(file=x,header=T)})

  # Combine all predictor files and creates a styear field and deletes all "....x....y" styear artifacts from merging
  pcws_lur_preds <- datalist %>%
    Reduce(function(x,y) left_join(x,y,by="hhid_x"), .) %>% # merge all predictor csvs together
    mutate(year = styear.x) %>%
    dplyr::select(-starts_with("styear")) %>%
    mutate(styear = year) %>%
    dplyr::select(-year)
  
  pcws_lur_preds

}

# Save the LUR predictor file for each folder year
pcws_lur_preds_1987 <- pcws_lur_preds_pull('1987')
pcws_lur_preds_1988 <- pcws_lur_preds_pull('1988')
pcws_lur_preds_1989 <- pcws_lur_preds_pull('1989')
pcws_lur_preds_1990 <- pcws_lur_preds_pull('1990')
pcws_lur_preds_1991 <- pcws_lur_preds_pull('1991')



# Combine all the predictor sets
pcws_lur_preds <- bind_rows(pcws_lur_preds_1987,
                             pcws_lur_preds_1988,
                             pcws_lur_preds_1989,
                             pcws_lur_preds_1990,
                             pcws_lur_preds_1991)

# Combine air pollution values and predictors
pcws_lur_data <- inner_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))

# Check for homes iwth no addresses and make sure they really don't have anything
pcws_lur_data_nonjoin <- anti_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))
# 21 homes with no address data, many unique to 1992, which we know have no address data

# Correct any predictor values (starting with the 6th column) that are NA to 0
pcws_lur_data[, 6:ncol(pcws_lur_data)][is.na(pcws_lur_data[, 6:ncol(pcws_lur_data)])] <- 0
# 
# # Remove predictor columns with all 0s, as this will cause the regression to fail
# pcws_lur_data <- pcws_lur_data[, apply(pcws_lur_data, 2, function(x) !all(x==0))] 

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
# Also drop count variables
pcws_lur_data <- pcws_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2)) %>%
  dplyr::select(-no2_adj_n ,-pm25_adj_n ,-pm10_adj_n) 

# NOTE - we are dropping observations that start in 1992 - there are results data but no address info
pcws_lur_data <- filter(pcws_lur_data, styear!=1992)

# Read in monthly meteorological variables
atmos_data <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/Temp_Wind Data_NOAA/NOAAValues_TempWindAvgs_LURModelingUpdated2019.csv")

atmos_data_yearavgs <- atmos_data %>%
  group_by(Year) %>%
  summarise(WindSpeedmph = mean(WindSpeedmph),
            MaxTempF = mean(MaxTempF),
            MinTempF = mean(MinTempF),
            PrecipInch = sum(PrecipInch),
            RHpct_Hour11 = mean(RHpct_Hour11))

pcws_lur_data <- inner_join(pcws_lur_data, atmos_data_yearavgs, by=c("styear"="Year"))


# # # # TEST DROPPING THE DENSITY SO JUST POP IS THERE TO MAKE SURE MODEL WORKS HOW USED TO
pcws_lur_data <- dplyr::select(pcws_lur_data, -contains("_dens_"))



```


```{r PCWS LUR Building, Diagnostics, and Internal Validation Functions Set Up, echo=FALSE, message=FALSE, warning=FALSE}

# Define function to build LUR model from PCWS base dataset
escape_lur_mod_pcws <- function(pollutant, pred_to_drop=NULL) {
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  pcws_lur_data_TEMP <- filter(pcws_lur_data, !is.na(eval(parse(text=pollutant_outcome))))
  
  # Remove predictor columns with all 0s, as this will cause the regression to fail
  pcws_lur_data_TEMP <- pcws_lur_data_TEMP[, apply(pcws_lur_data_TEMP, 2, function(x) !all(x==0))]

  # Remove all predictors with the same value
  uniquelength <- sapply(pcws_lur_data_TEMP,function(x) length(unique(x)))
  pcws_lur_data_TEMP <- subset(pcws_lur_data_TEMP, select=uniquelength>1)
  
  # Remove specific predictor if needed after initial model fit, default is NULL
  ifelse(any(length(pred_to_drop)>0), 
       pcws_lur_data_TEMP <- dplyr::select(pcws_lur_data_TEMP, -pred_to_drop),
       pcws_lur_data_TEMP)
  
  # Create an expected coefficient direction field variable (eg "expected_coef_no2")
  expected_coef <- paste0("expected_coef_",pollutant)
  # And then create a specific file and field designation for the pollutant model df of coefficients (eg"pred_coef_test$expected_coef_no2")
  pred_coef_test.expected_coef <- as.character(paste0('pred_coef_test$', expected_coef))

  # Read in the expected coefficients pre-determined from previous research/literature
  preds_expected_coefs <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/lur/preds_expected_coefs.csv", header = T)

  preds_expected_coefs$pred <- as.character(preds_expected_coefs$pred)
  
  # Univariate regressions to find starting variable for pollutant
  
  # Pull out predictor names
  pred <- colnames(pcws_lur_data_TEMP[6:ncol(pcws_lur_data_TEMP)])
  
  # Run simple regressions with all predictors
  # Pull resulting adjusted R2s
  adjr2<-apply(pcws_lur_data_TEMP[6:ncol(pcws_lur_data_TEMP)], 2, function(x) summary(lm(eval(parse(text=pollutant_outcome)) ~ x,data=pcws_lur_data_TEMP))$adj.r.squared)
  
  # Pull resulting coefficients
  # coef<-apply(pcws_lur_data_TEMP[6:ncol(pcws_lur_data_TEMP)], 2, function(x) coef(summary(lm(eval(parse(text=pollutant_outcome)) ~ x,data=pcws_lur_data_TEMP)))[2,1])
  
    coef<-apply(pcws_lur_data_TEMP[6:ncol(pcws_lur_data_TEMP)], 2, function(x) tidy(lm(eval(parse(text=pollutant_outcome)) ~ x,data=pcws_lur_data_TEMP))[2,2])
  
  pcws_lur_simple_out <- data.frame(pred, coef, adjr2) # put the estimated adj r2 and coefs in data frame
  
  rownames(pcws_lur_simple_out) <- NULL
  
  # Create a predictor direction for the coefficients in the regression
  pcws_lur_simple_out <- pcws_lur_simple_out %>%
    mutate(coef_direction = ifelse(coef>0,"positive","negative")) 
  
  # Merge by predictor name to get expected coef direction data 
  pcws_lur_simple_out <- inner_join(pcws_lur_simple_out, preds_expected_coefs, by=c("pred")) 
  
  # Filter out predictors that do not meet expected coefficient direction (those without a determined direction stay regardless) and choose the highest adjusted R2 value of those that remain for the seed predictor for this model
  pcws_lur_simple_out <- pcws_lur_simple_out %>%
    filter(eval(parse(text=expected_coef)) == "undetermined" | 
             coef_direction == eval(parse(text=expected_coef))) %>%
    arrange(desc(adjr2))
  
  # Define the seed predictor
  seed_pred <- pcws_lur_simple_out$pred[1]
  
  # Reload all the predictors into a list, but cut out the seed predictor so it can be moved to the first position
  predictors <- pred[!pred %in% eval(seed_pred)]
  predictors <- c(seed_pred, predictors, "styear")
  
  # Seed model
  mod_form <- formula(paste0(pollutant_outcome,"~",seed_pred))
  mod <- lm(mod_form, pcws_lur_data_TEMP)
  
  # Seed adj R2
  adjr2 <- rep(0,length(predictors))
  adjr2[1] <- summary(mod)$adj.r.squared
  
  # For all preds in the list
    # Add in new pred, test model and assumptions
      # If assumptions are met, then keep this updated model with new pred, update it as the working model and make it next starting model
  
      # If assumptions are NOT met, then go back to previously saved working by removing the most recently added predictor
  
  for (i in 2:length(predictors)){
    
    mod_form <- update(mod_form,  as.formula(paste('~ . +', predictors[i])))
    mod <- lm(mod_form,pcws_lur_data_TEMP)
  
    adjr2[i] <- broom::glance(summary(mod))[1,2]
    
    # if (adjr2[i] > (adjr2[i-1][[1]] + 0.01)) {  
    if (adjr2[i][[1]] > (adjr2[i-1][[1]] + 0.01)) {
      print(paste0("Hooray! This predictor increased model adj R2 test by >0.01"))
      
      pred_coef <- tidy(mod)
    
      # Create a predictor direction for the coefficients in the regression
      pred_coef$coef_direction <- ifelse(pred_coef$estimate>0,"positive","negative")
      pred_coef$pred <- pred_coef$term
      
      # Merge by predictor name to get expected coef direction data
      pred_coef_test <- inner_join(pred_coef, preds_expected_coefs, by=c("pred"))
    
      if(all((as.character(eval(parse(text=pred_coef_test.expected_coef))) == "undetermined") |
         (as.character(eval(parse(text=pred_coef_test.expected_coef))) == pred_coef_test$coef_direction))) {     
          print(paste0("Hooray! Coefficients for predictor and previous predictors all meet predetermined directions. This predictor will be added to the model."))
      } else {            
          # code if 1 passes, 2 fails
        print(paste0("Boo! This predictor increased the adj R2 >0.01 compared to the previous model, but the coefficient for this predictor or another didn't match the predetermined direction."))
        
          # Turn formular in text, then find the first (+) sign searching from the right (eg. the most recently added predictor that's no good), and cut this section of "+ predictorname" and change it back to formula and model
          mod_form_text <- as.character(mod_form[3])
          pred_to_cut <- paste0(" + ",predictors[i])
          mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
          mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
          mod <- lm(mod_form,pcws_lur_data_TEMP)

      }
  } else {            
      # code if 1 fails
          print(paste0("Boo! This predictor didn't increase the adj R2 >0.01 compared to the previous model."))
    
          # Turn formular in text, then find the first (+) sign searching from the right (eg. the most recently added predictor that's no good), and cut this section of "+ predictorname" and change it back to formula and model
          mod_form_text <- as.character(mod_form[3])
          pred_to_cut <- paste0(" + ",predictors[i])
          mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
          mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
          mod <- lm(mod_form,pcws_lur_data_TEMP)
  
  }
  
  }
  
  pvaltest_preds <- tidy(mod)

  while(any(is.na(pvaltest_preds$p.value))){
    
  var_with_NA_pvalue <- pvaltest_preds[(which(pvaltest_preds$p.value == max(pvaltest_preds$p.value))),1]  # get the var with max pvalue
  
  mod_form_text <- as.character(mod_form[3])
  pred_to_cut <- paste0(" + ",var_with_NA_pvalue)
  mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
  mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
  mod <- lm(mod_form,pcws_lur_data_TEMP)
  pvaltest_preds <- tidy(mod)

  }
  
  # Pulling predictors one by one with pvalues>0.1
  pvaltest_preds <- tidy(mod)
  
  # Filter out the intercept as a candidate for this approach
  pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")

  while(any(pvaltest_preds$p.value > 0.1)){
    
    var_with_max_pvalue <- pvaltest_preds[(which(pvaltest_preds$p.value == max(pvaltest_preds$p.value))),1]  # get the var with max pvalue
    
        formula.new <- as.formula(
        paste(
            paste(deparse(mod_form), collapse=""),
            paste(var_with_max_pvalue, collapse="-"),
            sep="-"
        )
    )

    mod <- lm(formula.new,pcws_lur_data_TEMP)
    mod_form <-  eval(mod$call[[2]])

    # # Cut out the predictor with pvalue > 0.1
    # mod_form_text <- as.character(mod_form[3])
    # mod_form_text <- gsub("++0","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+  ","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub(var_with_max_pvalue,"",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+0","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+  ","",mod_form_text,fixed = TRUE)
    # # cut any + signs that end the model text
    # mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
    # mod <- lm(mod_form,taps_lur_data_TEMP)
    pvaltest_preds <- tidy(mod)

    # Filter out the intercept as a candidate for this approach for the next 'pass'
    pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")
  }
  
  alias(mod)
  #the linearly dependent variables
  ld.vars <- attributes(alias(mod)$Complete)$dimnames[[1]]

  while(any(length(ld.vars > 0))) {
    # remove the linearly dependent variables
    formula.new <- as.formula(
        paste(
            paste(deparse(mod_form), collapse=""),
            paste(ld.vars, collapse="-"),
            sep="-"
        )
    )

    mod <- lm(formula.new,pcws_lur_data_TEMP)
    mod_form <-  eval(mod$call[[2]])

    #Take out any predictors that are COMPLETELY linearly related
    alias(mod)
    #the linearly dependent variables
    ld.vars <- attributes(alias(mod)$Complete)$dimnames[[1]]

  }
  
  # Pulling predictors one by one with pvalues>0.1
  pvaltest_preds <- tidy(mod)
  

  # Check for VIF being <3 for all predictors, but only if there is more than 1!
  if(length(mod$coefficients) > 2) {
    
    # Create VIFs
    all_vifs <- vif(mod)
    signif_all <- names(all_vifs)
    
    while(any(all_vifs > 3)){ #NOTE if vif>3, then exclude from model, starting with largest VIF first as needed
    var_with_max_vif <- names(which(all_vifs == max(all_vifs)))  # get the var with max vif
  
    # Cut out the predictor with pvalue > 0.1
    # mod_form_text <- as.character(mod_form[3])
    # pred_to_cut <- var_with_max_vif
    # mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
    # mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
    # mod <- lm(mod_form,taps_lur_data_TEMP)
    formula.new <- as.formula(
        paste(
            paste(deparse(mod_form), collapse=""),
            paste(var_with_max_vif, collapse="-"),
            sep="-"
        )
    )

    mod <- lm(formula.new,pcws_lur_data_TEMP)
    mod_form <-  eval(mod$call[[2]])
    
    all_vifs <- vif(mod)
    pvaltest_preds <- tidy(mod)

    }
  }
  
  # Pulling predictors one by one with pvalues>0.1
  pvaltest_preds <- tidy(mod)
  
  # Filter out the intercept as a candidate for this approach
  pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")

  
  while(any(pvaltest_preds$p.value > 0.1)){
    
    var_with_max_pvalue <- pvaltest_preds[(which(pvaltest_preds$p.value == max(pvaltest_preds$p.value))),1]  # get the var with max pvalue
    
        formula.new <- as.formula(
        paste(
            paste(deparse(mod_form), collapse=""),
            paste(var_with_max_pvalue, collapse="-"),
            sep="-"
        )
    )

    mod <- lm(formula.new,pcws_lur_data_TEMP)
    mod_form <-  eval(mod$call[[2]])

    # # Cut out the predictor with pvalue > 0.1
    # mod_form_text <- as.character(mod_form[3])
    # mod_form_text <- gsub("++0","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+  ","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub(var_with_max_pvalue,"",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+0","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+  ","",mod_form_text,fixed = TRUE)
    # # cut any + signs that end the model text
    # mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
    # mod <- lm(mod_form,taps_lur_data_TEMP)
    pvaltest_preds <- tidy(mod)

    # Filter out the intercept as a candidate for this approach for the next 'pass'
    pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")
  }
  

# We assessed a high Cook’s D value (i.e. >1), indicating an influential observation, because it could be caused by an (extreme) high or low concentration of one of the site(s) or by an included predictor variable with extreme values or many zero values. 

# 1 - In such case, the developed model was applied to all sites minus the site with the high Cook’s D value and the changes in model structure (parameter estimates and p-values of included variables, and model R2)were evaluated. 

# 2 - If the high Cook’s D value was caused by one of the included predictor variables (indicated by a large change in parameter estimate for that variable without the site), a new LUR model was developed using all sites but without offering that specific predictor variable to the model. 

# Cook's D plot
# identify D values 4/(n-k-1) 
cutoff <- 4/((nrow(pcws_lur_data_TEMP)-length(mod$coefficients)-2)) 
plot(mod, which=4, cook.levels=cutoff, labels.id = pcws_lur_data_TEMP$hhid_x)

cooksd <- cooks.distance(mod)

if (any(!is.na(cooksd) & cooksd>1)) {
    print("An observation(s) has a Cook's Distances >1! Review the plot to investigate.  Here are next steps from Beelen et al 2013 (the ESCAPE NO2/x LUR paper) : We assessed a high Cook’s D value (i.e. >1), indicating an influential observation, because it could be caused by an (extreme) high or low concentration of one of the site(s) or by an included predictor variable with extreme values or many zero values. In such case, the developed model was applied to all sites minus the site with the high Cook’s D value and the changes in model structure (parameter estimates and p-values of included variables, and model R2)were evaluated. If the high Cook’s D value was caused by one of the included predictor variables (indicated by a large change in parameter estimate for that variable without the site), a new LUR model was developed using all sites but without offering that specific predictor variable to the model.")

  cutoff <- 4/((nrow(pcws_lur_data_TEMP)-length(mod$coefficients)-2)) 
  plot(mod, which=4, cook.levels=cutoff, labels.id = pcws_lur_data_TEMP$hhid_x)
  
  # SECTION DOES NOT WORK - MAY RETURN TO LATER
  # var_with_max_cooksd <- names(which(cooksd == max(cooksd)))  # get the var with max cooksd
  # 
  # pcws_lur_data_TEMP <- pcws_lur_data_TEMP[-as.numeric(var_with_max_cooksd), ]


  } else {
  print("All Cook's Distances are <1; no need to investigate for outsize influence.")
}

  return(mod)
}

# Define function to get out model data set to manually update as needed
escape_lur_diag_data <- function(pollutant) {
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  pcws_lur_data_TEMP <- filter(pcws_lur_data, !is.na(eval(parse(text=pollutant_outcome))))
  
  # Remove predictor columns with all 0s, as this will cause the regression to fail
  pcws_lur_data_TEMP <- pcws_lur_data_TEMP[, apply(pcws_lur_data_TEMP, 2, function(x) !all(x==0))]
  
  return(pcws_lur_data_TEMP)
}

# Define function to read in address shapefile for spatial autocorrelation checks
pcws_addrs_load <- function(){
  setwd("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/PCWS/Data/LUR/Shapefiles")
  addrs <- st_read("P5_Addresses.shp", stringsAsFactors = F)
  addrs$hhid_x <- paste(addrs$hhid, addrs$hhidx, sep="_") #make unique address ID

  addrs <- addrs  %>% 
    st_set_crs(NA) %>% 
    st_set_crs(2868) %>%
    st_transform(addrs, crs = 2868)
}

# Define function to test for spatial autocorrelation (Global and Local with residual map)
spatial_auto_diag <- function(pollutant, pollutant_unit){
  
  pcws_resids <- data.frame("hhid_x" = pcws_lur_data_TEMP$hhid_x,
                           "resids" = mod$residuals)
  
  pcws_resids <- inner_join(pcws_addrs, pcws_resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  pcws_resids <- as(pcws_resids, "Spatial")
  
  pcws_resids <- spTransform(pcws_resids, CRS("+proj=longlat +datum=WGS84"))
  
  pcws_resids$long <- pcws_resids@coords[,1]
  pcws_resids$lat <- pcws_resids@coords[,2]
  
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(pcws_resids@data$long, pcws_resids@data$lat)
  m.I <- moransI(coords,5,pcws_resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)

  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,pcws_resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(pcws_resids@data$long, pcws_resids@data$lat)
  m.I <- moransI(coords,3,pcws_resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,pcws_resids@data$resids, scatter.plot = T)
  l.m.I

  return(list(moransIval=moransIval, moransIpvalue=moransIpvalue))
}

spatial_auto_residmap <- function(pollutant, pollutant_unit){
  
  pcws_resids <- data.frame("hhid_x" = pcws_lur_data_TEMP$hhid_x,
                           "resids" = mod$residuals)
  
  pcws_resids <- inner_join(pcws_addrs, pcws_resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  pcws_resids <- as(pcws_resids, "Spatial")
  
  pcws_resids <- spTransform(pcws_resids, CRS("+proj=longlat +datum=WGS84"))
  
  pcws_resids$long <- pcws_resids@coords[,1]
  pcws_resids$lat <- pcws_resids@coords[,2]
  
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(pcws_resids@data$long, pcws_resids@data$lat)
  m.I <- moransI(coords,5,pcws_resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)

  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,pcws_resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(pcws_resids@data$long, pcws_resids@data$lat)
  m.I <- moransI(coords,3,pcws_resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,pcws_resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Map of residuals
  palette <- colorNumeric("RdYlBu", pcws_resids$resids, n = 5)

  leaflet(data = pcws_resids) %>% 
    addTiles() %>%
    addCircleMarkers(stroke = F, fillOpacity = 0.75,
                color = ~palette(resids),
                label = ~hhid_x,
                popup = paste(pollutant, "Model Resid:", format(round(pcws_resids$resids, 2), nsmall = 2), "<br>")) %>%
    leaflet::addLegend(pal = palette,
              values = ~resids,
              title = paste(pollutant, "Model Resids. (",pollutant_unit,")"),
              opacity = 0.75,
              position = 'topright')
  }

# Define function for internal leave-one-out cross validation
internal_valid_loocv <- function(){
  # Define training control
  train_control <- trainControl(method="LOOCV")
  # train the model
  model_loocv <- train(formula(mod) ,data=pcws_lur_data_TEMP, 
                       trControl=train_control, method="lm")
  # summarize results
  print(model_loocv)
  
  return(model_loocv)

}

# Create function to assemble outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
pcws_mod_results_df <- function(pollutant, pollutant_unit) {
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")

  mod_formula <- as.character(as.formula(
  paste0("y ~ ", formatC(coefficients(mod)[1], format = "E", digits = 2, flag = "#"),
    paste0(sprintf("%+1.3g*%s",
                  coefficients(mod)[-1],
                  names(coefficients(mod)[-1])),
          collapse="")
  )
)[3])


r2 <- summary(mod)$r.squared
n_sites <- length(rownames(pcws_lur_data_TEMP))

meas_conc_mean <- formatC(mean(eval(parse(text=paste0("pcws_lur_data_TEMP$", pollutant_outcome)))), format = "fg", digits = 3, flag = "#")
meas_conc_min <- formatC(min(eval(parse(text=paste0("pcws_lur_data_TEMP$", pollutant_outcome)))), format = "fg", digits = 3, flag = "#")
meas_conc_max <- formatC(max(eval(parse(text=paste0("pcws_lur_data_TEMP$", pollutant_outcome)))), format = "fg", digits = 3, flag = "#")
meas_conc_mean_range <- paste0(meas_conc_mean, " (", meas_conc_min, "-", meas_conc_max, ")")

rootmse_loocv <- internal_valid_loocv()$results[,"RMSE"]
r2_loocv <- internal_valid_loocv()$results[,"Rsquared"]

moransIval <- formatC(unlist(spatial_auto_diag(pollutant = pollutant, pollutant_unit = pollutant_unit)[1]), format = "f", digits = 2, flag = "#")
moransIpvalue <- formatC(unlist(spatial_auto_diag(pollutant = pollutant, pollutant_unit = pollutant_unit)[2]), format = "f", digits = 2, flag = "#")

moransI_values <- paste0(moransIval, " (", moransIpvalue, ")")

# Create results output table row
pcws_results_df <- data.frame(
  "LUR_Model"= mod_formula,
  "R2_of_Model" = formatC(r2, format = "fg", digits = 2, flag = "#"),
  "R2_LOOCV" = formatC(r2_loocv, format = "fg", digits = 2, flag = "#"),
  "RMSE_LOOCV" = formatC(rootmse_loocv, format = "g", digits = 3),
  "N_Sites" = n_sites,
  "Morans_I_Values" = moransI_values,
  "Measured_Concentrations" = meas_conc_mean_range)

  return(pcws_results_df)
}


# Define functions for mixed effects model adaptation of final non-mixed model accounting for year and season (Winter, Summer, Intermediate) using all data
# NO2
escape_lur_mix_mod_no2 <- function(){
  # Merge raw data to predictors by year
  data_mix_mod <- data %>%
    mutate(hhid_x=paste0(hhid,"_A")) %>%
    filter(styear != 1992)
  
  data_mix_mod <- inner_join(data_mix_mod, pcws_lur_preds, by=c("hhid_x","styear"))
  
  # Join atmospheric data by year and month
  data_mix_mod <- inner_join(data_mix_mod, atmos_data, by=c("styear"="Year", "stmon"="Month"))

  # Correct any predictor values (starting with the 5th column) that are NA to 0
  data_mix_mod[, 27:ncol(data_mix_mod)][is.na(data_mix_mod[, 27:ncol(data_mix_mod)])] <- 0

  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  data_mix_mod <- data_mix_mod %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  
  mix_mod <- update(mod, no2ppb ~ . + (1 | hhid) + factor(styear) + WindSpeedmph + MaxTempF + MinTempF + PrecipInch + RHpct_Hour11, data=filter(data_mix_mod, !is.na(no2ppb)))
  
  mix_mod <- lmer(formula(mix_mod), data=filter(data_mix_mod, !is.na(no2ppb)))
}

# NO2 - Homes with Multi Observations (>1)
escape_lur_mix_mod_no2_multiobs <- function(){
  # # Merge raw data to predictors by year
  # data_mix_mod <- data %>%
  #   mutate(hhid_x=paste0(hhid,"_A")) %>%
  #   filter(styear != 1992)
  # 
  # # Join to PCWS air pollution and filter for those with >1 obsv
  # data_mix_mod <- left_join(data_mix_mod, pcws_ap, by=c("hhid_x","styear")) %>%
  #   filter(no2_adj_n>1)
  data_mix_mod <- data %>%
    filter(!is.na(no2_adj)) %>%
    dplyr::group_by(hhid,styear) %>%
    mutate(no2_byseason3 = n_distinct(stssn3)) %>%
    filter(no2_byseason3>1) %>%
    mutate(hhid_x=paste0(hhid,"_A")) %>%
    dplyr::ungroup() 
  
  data_mix_mod <- inner_join(data_mix_mod, pcws_lur_preds, by=c("hhid_x","styear"))
  
  # Join atmospheric data by year and month
  data_mix_mod <- inner_join(data_mix_mod, atmos_data, by=c("styear"="Year", "stmon"="Month"))

  # Correct any predictor values (starting with the 5th column) that are NA to 0
  data_mix_mod[, 27:ncol(data_mix_mod)][is.na(data_mix_mod[, 27:ncol(data_mix_mod)])] <- 0

  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  data_mix_mod <- data_mix_mod %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  
  mix_mod <- update(mod, no2ppb ~ . + (1 | hhid) + factor(styear) + WindSpeedmph + MaxTempF + MinTempF + PrecipInch + RHpct_Hour11, data=filter(data_mix_mod, !is.na(no2ppb)))
  
  mix_mod <- lmer(formula(mix_mod), data=filter(data_mix_mod, !is.na(no2ppb)))
}


# PM2.5
escape_lur_mix_mod_pm25 <- function(){
  # Merge raw data to predictors by year
  data_mix_mod <- data %>%
    mutate(hhid_x=paste0(hhid,"_A"))  %>%
    filter(styear != 1992)

  
  data_mix_mod <- inner_join(data_mix_mod, pcws_lur_preds, by=c("hhid_x","styear"))
  
  # Correct any predictor values (starting with the 5th column) that are NA to 0
  data_mix_mod[, 27:ncol(data_mix_mod)][is.na(data_mix_mod[, 27:ncol(data_mix_mod)])] <- 0

  # Join atmospheric data by year and month
  data_mix_mod <- inner_join(data_mix_mod, atmos_data, by=c("styear"="Year", "stmon"="Month"))

  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  data_mix_mod <- data_mix_mod %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  
  mix_mod <- update(mod, pm25ugm3 ~ . + (1 | hhid) + factor(styear) + WindSpeedmph + MaxTempF + MinTempF + PrecipInch + RHpct_Hour11, data=filter(data_mix_mod, !is.na(pm25ugm3)))
  
  mix_mod <- lmer(formula(mix_mod), data=filter(data_mix_mod, !is.na(pm25ugm3)))
}

# PM10
escape_lur_mix_mod_pm10 <- function(){
  # Merge raw data to predictors by year
  data_mix_mod <- data %>%
    mutate(hhid_x=paste0(hhid,"_A")) %>%
    filter(styear != 1992)
  
  data_mix_mod <- inner_join(data_mix_mod, pcws_lur_preds, by=c("hhid_x","styear"))
  
  # Join atmospheric data by year and month
  data_mix_mod <- inner_join(data_mix_mod, atmos_data, by=c("styear"="Year", "stmon"="Month"))

  # Correct any predictor values (starting with the 5th column) that are NA to 0
  data_mix_mod[, 27:ncol(data_mix_mod)][is.na(data_mix_mod[, 27:ncol(data_mix_mod)])] <- 0

  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  data_mix_mod <- data_mix_mod %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  
  mix_mod <- update(mod, pm10ugm3 ~ . + (1 | hhid) + factor(styear) + WindSpeedmph + MaxTempF + MinTempF + PrecipInch + RHpct_Hour11, data=filter(data_mix_mod, !is.na(pm10ugm3)))
  
  mix_mod <- lmer(formula(mix_mod), data=filter(data_mix_mod, !is.na(pm10ugm3)))
}
```

### PCWS LUR Creation - NO2

```{r PCWS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - NO2, echo=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
pcws_addrs <- pcws_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_pcws(pollutant = 'no2')
pcws_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'no2')
summary(mod)

AICc(mod)

# General diagnostics
ols_plot_diagnostics(mod)

# Spatial autocorrelation diagnostics
spatial_auto_residmap(pollutant = 'no2', pollutant_unit = 'ppb')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_no2()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

# Finalize model with naming
pcws_no2_mod <- mod
pcws_no2_mix_mod <- mix_mod

# Create df of outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
pcws_no2_results_df <- pcws_mod_results_df(pollutant = 'no2', pollutant_unit = 'ppb')

# Add the AICc values for linear and mixed mdoels for comparison
pcws_no2_results_df <- data.frame(pcws_no2_results_df,
                                  "AICc" = AICc(mod),
                                  "AICc_mix" = AICc(mix_mod))

# Create output of predicted values for comparison to external model predictions
pcws_no2_mod_predicted <- data.frame('hhid_x' = pcws_lur_data_TEMP$hhid_x,
                                     pcws_no2_mod_predicted = fitted(mod))


```



### PCWS LUR Creation - PM2.5
```{r PCWS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - PM2.5, echo=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
pcws_addrs <- pcws_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_pcws(pollutant = 'pm25')
pcws_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm25')
summary(mod)
AICc(mod)

# General diagnostics
ols_plot_diagnostics(mod)

# Spatial autocorrelation diagnostics
spatial_auto_diag(pollutant = 'pm25', pollutant_unit = 'ug_m3')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_pm25()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

# Finalize model with naming
pcws_pm25_mod <- mod
pcws_pm25_mix_mod <- mix_mod

# Create df of outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
pcws_pm25_results_df <- pcws_mod_results_df(pollutant = 'pm25', pollutant_unit = 'ug/m3')

# Add the AICc values for linear and mixed mdoels for comparison
pcws_pm25_results_df <- data.frame(pcws_pm25_results_df,
                                  "AICc" = AICc(mod),
                                  "AICc_mix" = AICc(mix_mod))

# Create output of predicted values for comparison to external model predictions
pcws_pm25_mod_predicted <- data.frame('hhid_x' = pcws_lur_data_TEMP$hhid_x,
                                     pcws_pm25_mod_predicted = fitted(mod))

```

### PCWS LUR Creation - PM10
```{r PCWS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - PM10, echo=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
pcws_addrs <- pcws_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_pcws(pollutant = 'pm10')
pcws_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm10')
summary(mod)

# General diagnostics
ols_plot_diagnostics(mod)

# Spatial autocorrelation diagnostics
spatial_auto_diag(pollutant = 'pm10', pollutant_unit = 'ug_m3')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_pm10()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

# Finalize model with naming
pcws_pm10_mod <- mod
pcws_pm10_mix_mod <- mix_mod

# Create df of outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
pcws_pm10_results_df <- pcws_mod_results_df(pollutant = 'pm10', pollutant_unit = 'ug/m3')

# Add the AICc values for linear and mixed mdoels for comparison
pcws_pm10_results_df <- data.frame(pcws_pm10_results_df,
                                  "AICc" = AICc(mod),
                                  "AICc_mix" = AICc(mix_mod))

# Create output of predicted values for comparison to external model predictions
pcws_pm10_mod_predicted <- data.frame('hhid_x' = pcws_lur_data_TEMP$hhid_x,
                                     pcws_pm10_mod_predicted = fitted(mod))

```

## Senstivity Analysis - PCWS LUR Creation - NO2 - Only Including Homes w/ Multi Measures in Different Seasons (ESCAPE/TAPS 3 seasons)

For homes measured more than once in more than 1 year, only keep year with the most measurements that year. For homes measured the same # of times in different years, keep the older as it will be closer to more life years of CRS early life stage (Birth + ID1, 1980-1992).

NO2 will be the test case as this dataset has the most homes measured more than once/year.

```{r PCWS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - NO2 - Sensitivity Analysis, echo=TRUE, message=FALSE, warning=FALSE}


pcws_ap_no2_multi <- data %>%
  filter(!is.na(no2_adj)) %>%
  dplyr::group_by(hhid,styear) %>%
  mutate(no2_byseason3 = n_distinct(stssn3)) %>%
  filter(no2_byseason3>1) %>%
  mutate(hhid_x=paste0(hhid,"_A")) %>%
  dplyr::ungroup() %>%
  dplyr::select(-hhid) %>%
  group_by(hhid_x, styear) %>%
  filter(no2_byseason3==max(no2_byseason3)) %>%
  filter(styear==min(styear)) %>%
  summarise(no2_adj=mean(no2_adj, na.rm=T),
            pm25_adj=mean(pm25_adj, na.rm=T),
            pm10_adj=mean(pm10_adj, na.rm=T)) %>%
  dplyr::ungroup()


# Combine air pollution values and predictors
pcws_lur_data <- inner_join(pcws_ap_no2_multi, pcws_lur_preds, by=c("hhid_x","styear"))

# Check for homes iwth no addresses and make sure they really don't have anything
pcws_lur_data_nonjoin <- anti_join(pcws_ap_no2_multi, pcws_lur_preds, by=c("hhid_x","styear"))
# 21 homes with no address data, many unique to 1992, which we know have no address data

# Join atmospheric data
pcws_lur_data <- inner_join(pcws_lur_data, atmos_data_yearavgs, by=c("styear"="Year"))

# Correct any predictor values (starting with the 6th column) that are NA to 0
pcws_lur_data[, 6:ncol(pcws_lur_data)][is.na(pcws_lur_data[, 6:ncol(pcws_lur_data)])] <- 0
# 
# # Remove predictor columns with all 0s, as this will cause the regression to fail
# pcws_lur_data <- pcws_lur_data[, apply(pcws_lur_data, 2, function(x) !all(x==0))] 

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
pcws_lur_data <- pcws_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

# NOTE - we are dropping observations that start in 1992 - there are results data but no address info
pcws_lur_data <- filter(pcws_lur_data, styear!=1992)

# # # # TEST DROPPING THE DENSITY SO JUST POP IS THERE TO MAKE SURE MODEL WORKS HOW USED TO
pcws_lur_data <- dplyr::select(pcws_lur_data, -contains("_dens_"))



# Create address set to work on 
pcws_addrs <- pcws_addrs_load()

# # Subset out multi measures/year only
# pcws_lur_data <- pcws_lur_data %>%
#   filter(no2_adj_n > 1) %>%
#   dplyr::select(-no2_adj_n ,-pm25_adj_n ,-pm10_adj_n)

# Model building and summary stats
mod <- escape_lur_mod_pcws(pollutant = 'no2')

# Based on output, test model variables and home with Cook's D > 1: "737_A"
pcws_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'no2')

summary(mod)

hhids_to_drop = c("737_A")

summary(lm(mod,data = filter(pcws_lur_data_TEMP, !(hhid_x %in% hhids_to_drop))))

# OLD PRE-CENSUS ERROR
# Substantial change in lu_br_5000 redevelop model without this predictor
mod <- escape_lur_mod_pcws(pollutant = 'no2', pred_to_drop = c('lu_br_5000'))

# # NEW POST CENSUS ERROR
# # No changes in predictors without 737_A, removing 737_A permanently and refitting model from start
# hhids_to_drop = c("737_A")

# pcws_lur_data <- filter(pcws_lur_data, !(hhid_x %in% hhids_to_drop))
# 
# mod <- escape_lur_mod_pcws(pollutant = 'no2')

pcws_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'no2')

summary(mod)

# # General diagnostics
# ols_plot_diagnostics(mod) # doesn't work for wahtever reason

# Spatial autocorrelation diagnostics
spatial_auto_diag(pollutant = 'no2', pollutant_unit = 'ppb')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_no2_multiobs()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

# Finalize model with naming
pcws_no2_mod_multiobs <- mod

# Create df of outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
pcws_no2_multiobs_results_df <- pcws_mod_results_df(pollutant = 'no2', pollutant_unit = 'ppb')

# Add the AICc values for linear and mixed mdoels for comparison
pcws_no2_multiobs_results_df <- data.frame(pcws_no2_multiobs_results_df,
                                  "AICc" = AICc(mod),
                                  "AICc_mix" = AICc(mix_mod))

# Create output of predicted values for comparison to external model predictions
pcws_no2_multiobs_mod_predicted <- data.frame('hhid_x' = pcws_lur_data_TEMP$hhid_x,
                                     pcws_no2_multiobs_mod_predicted = fitted(mod))

```

In comparing the PCWS NO2 model (any # air pollution measure/observations) vs with >1 observation:

`r summary(pcws_no2_mod)$adj.r.squared` compared to `r summary(pcws_no2_mod_multiobs)$adj.r.squared`

We can see that excluding single-measure homes improves adjusted r-squared value slightly.

`r AICc(pcws_no2_mod)` compared to `r AICc(pcws_no2_mod_multiobs)`

We can see that excluding single-measure homes improves adjusted AICc values as well.

This suggests that more representative annual averages = better fitting models with predictors with annual precision.

## TAPS

#### TAPS Data Load

NOTE - data cleaning and averaging code is in TAPS_Data_Cleanup.R

```{r TAPS Data Load, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

data <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/Results/TAPSData_AllObs.csv", sep=",")

taps_ap <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/Results/TAPSData.csv", sep=",") %>%
  mutate(hhid_x = paste(HHID, HHIDX, sep="_")) %>%
  filter(hhid_x != "QF44_A") # Dropped due to no GIS point taken and poor site location next to fire station
```

```{r Summary Statistics  of Air Pollution Levels for TAPS, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

taps_measures_data <- inner_join(data, taps_ap, by=c("HHID_X"="hhid_x"))



no2 <- ggplot(data = taps_measures_data, aes(y=no2_adj)) +
  geom_boxplot() +
  labs(y=expression(paste(NO[2]," Conc. (ppb)"))) +
  theme_bw() +
  theme(axis.text.y=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        axis.text.x = element_blank()) +
  scale_y_continuous(limits=c(0,60), breaks=seq(0,60,by=10))

jpeg("taps_no2_measures.jpeg", width = 600, height = 1000, res = 300)
no2
print(no2)
dev.off()

nox <- ggplot(data = taps_measures_data, aes(y=nox)) +
  geom_boxplot() +
  labs(y=expression(paste(NO[x]," Conc. (ppb)"))) +
  theme_bw() +
  theme(axis.text.y=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        axis.text.x = element_blank())

jpeg("taps_nox_measures.jpeg", width = 600, height = 1000, res = 300)
nox
print(nox)
dev.off()



```



```{r TAPS LUR Predictor Assembly, echo=FALSE, message=FALSE, warning=FALSE}

taps_lur_preds_pull <- function(folder_year){
  # Pull out predictors from each year folder, assign a 'styear' to them for merging to pollutant measurement dataset
  filenames=list.files(path=paste0("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Predictors/",folder_year), full.names=TRUE)

  datalist = lapply(filenames, function(x){read.csv(file=x,header=T)})

  # Combine all predictor files and creates a styear field and deletes all "....x....y" styear artifacts from merging
  taps_lur_preds <- datalist %>%
    Reduce(function(x,y) left_join(x,y,by="hhid_x"), .) %>% # merge all predictor csvs together
    mutate(year = styear.x) %>%
    dplyr::select(-starts_with("styear")) %>%
    mutate(styear = year) %>%
    dplyr::select(-year)
  
  # Read in monthly meteorological variables
  atmos_data <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/Temp_Wind Data_NOAA/NOAAValues_TempWindAvgs_LURModelingUpdated2019.csv")
  
  atmos_data_yearavgs <- atmos_data %>%
    group_by(Year) %>%
    summarise(WindSpeedmph = mean(WindSpeedmph),
              MaxTempF = mean(MaxTempF),
              MinTempF = mean(MinTempF),
              PrecipInch = sum(PrecipInch),
              RHpct_Hour11 = mean(RHpct_Hour11))

  taps_lur_preds <- inner_join(taps_lur_preds, atmos_data_yearavgs, by=c("styear"="Year"))

  taps_lur_preds

}

taps_lur_preds <- taps_lur_preds_pull('2015')



# Check for homes iwth no addresses and make sure they really don't have anything
taps_lur_data_nonjoin <- anti_join(taps_ap, taps_lur_preds, by=c("hhid_x"))
# 1 home doesn't join, QF44, as it has locational address data - this was actually by a fire house (constant intermittent diesel source) so it was excluded due to poor location choice intially

# Combine air pollution values and predictors
taps_lur_data <- inner_join(taps_ap, taps_lur_preds, by=c("hhid_x"))

# Drop HHID, HHIDX, and uncorrected columns
taps_lur_data <- taps_lur_data %>%
  dplyr::select(hhid_x, everything(), -HHID, -HHIDX) %>%
  dplyr::select(-ends_with('unadj'))

# Correct any predictor values (starting with the 6th column) that are NA to 0
taps_lur_data[, 6:ncol(taps_lur_data)][is.na(taps_lur_data[, 6:ncol(taps_lur_data)])] <- 0

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
taps_lur_data <- taps_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

# # # # # TEST TO DROP DENS FIELDS


taps_lur_data <- dplyr::select(taps_lur_data, -contains("dens"))

```


```{r TAPS LUR Building, Diagnostics, and Internal Validation Functions Set Up, echo=FALSE, message=FALSE, warning=FALSE}

# Define function to build LUR model from PCWS base dataset
escape_lur_mod_taps <- function(pollutant, pred_to_drop=NULL) {
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  taps_lur_data_TEMP <- filter(taps_lur_data, !is.na(eval(parse(text=pollutant_outcome))))
  
  # Remove predictor columns with all 0s, as this will cause the regression to fail
  taps_lur_data_TEMP <- taps_lur_data_TEMP[, apply(taps_lur_data_TEMP, 2, function(x) !all(x==0))]
  
  # Remove all predictors with the same value
  uniquelength <- sapply(taps_lur_data_TEMP,function(x) length(unique(x)))
  taps_lur_data_TEMP <- subset(taps_lur_data_TEMP, select=uniquelength>1)

  # Remove specific predictor if needed after initial model fit, default is NULL
  ifelse(any(length(pred_to_drop)>0), 
       taps_lur_data_TEMP <- dplyr::select(taps_lur_data_TEMP, -pred_to_drop),
       taps_lur_data_TEMP)

  # Create an expected coefficient direction field variable (eg "expected_coef_no2")
  expected_coef <- paste0("expected_coef_",pollutant)
  # And then create a specific file and field designation for the pollutant model df of coefficients (eg"pred_coef_test$expected_coef_no2")
  pred_coef_test.expected_coef <- as.character(paste0('pred_coef_test$', expected_coef))

  # Read in the expected coefficients pre-determined from previous research/literature
  preds_expected_coefs <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/lur/preds_expected_coefs.csv", header = T)

  preds_expected_coefs$pred <- as.character(preds_expected_coefs$pred)
  
  # Univariate regressions to find starting variable for pollutant
  
  # Pull out predictor names
  pred <- colnames(taps_lur_data_TEMP[6:ncol(taps_lur_data_TEMP)])
  
  # Run simple regressions with all predictors
  # Pull resulting adjusted R2s
  adjr2<-apply(taps_lur_data_TEMP[6:ncol(taps_lur_data_TEMP)], 2, function(x) summary(lm(eval(parse(text=pollutant_outcome)) ~ x,data=taps_lur_data_TEMP))$adj.r.squared)
  
  # Pull resulting coefficients
  # coef<-apply(taps_lur_data_TEMP[6:ncol(taps_lur_data_TEMP)], 2, function(x) coef(summary(lm(eval(parse(text=pollutant_outcome)) ~ x,data=taps_lur_data_TEMP)))[2,1])
  
    coef<-apply(taps_lur_data_TEMP[6:ncol(taps_lur_data_TEMP)], 2, function(x) tidy(lm(eval(parse(text=pollutant_outcome)) ~ x,data=taps_lur_data_TEMP))[[2,2]])
    
        # coef<-apply(taps_lur_data_TEMP[6:ncol(taps_lur_data_TEMP)], 2, function(x) tidy(lm(eval(parse(text=pollutant_outcome)) ~ x,data=taps_lur_data_TEMP))[2,2])
  
  
    tidy(lm(eval(parse(text=pollutant_outcome)) ~ Xcoord,data=taps_lur_data_TEMP))[[2,2]]
    
  taps_lur_simple_out <- data.frame(pred, coef, adjr2) # put the estimated adj r2 and coefs in data frame
  
  rownames(taps_lur_simple_out) <- NULL
  
  # Create a predictor direction for the coefficients in the regression
  taps_lur_simple_out <- taps_lur_simple_out %>%
    mutate(coef_direction = ifelse(coef>0,"positive","negative")) 
  
  # Merge by predictor name to get expected coef direction data 
  taps_lur_simple_out <- left_join(taps_lur_simple_out, preds_expected_coefs, by=c("pred")) 
  
  # Filter out predictors that do not meet expected coefficient direction (those without a determined direction stay regardless) and choose the highest adjusted R2 value of those that remain for the seed predictor for this model
  taps_lur_simple_out <- taps_lur_simple_out %>%
    filter(eval(parse(text=expected_coef)) == "undetermined" | 
             coef_direction == eval(parse(text=expected_coef))) %>%
    arrange(desc(adjr2))
  
  # Define the seed predictor
  seed_pred <- taps_lur_simple_out$pred[1]
  
  # Reload all the predictors into a list, but cut out the seed predictor so it can be moved to the first position
  predictors <- pred[!pred %in% eval(seed_pred)]
  predictors <- c(seed_pred, predictors)
  
  # Seed model
  mod_form <- formula(paste0(pollutant_outcome,"~",seed_pred))
  mod <- lm(mod_form, taps_lur_data_TEMP)
  
  # Seed adj R2
  adjr2 <- rep(0,length(predictors))
  adjr2[1] <- summary(mod)$adj.r.squared
  
  # For all preds in the list
    # Add in new pred, test model and assumptions
      # If assumptions are met, then keep this updated model with new pred, update it as the working model and make it next starting model
  
      # If assumptions are NOT met, then go back to previously saved working by removing the most recently added predictor
  
  for (i in 2:length(predictors)){
    
    mod_form <- update(mod_form,  as.formula(paste('~ . +', predictors[i])))
    mod <- lm(mod_form,taps_lur_data_TEMP)
  
    adjr2[i] <- broom::glance(summary(mod))[1,2]
    
    if (adjr2[i][[1]] > (adjr2[i-1][[1]] + 0.01)) {  
      print(paste0("Hooray! This predictor increased model adj R2 test by >0.01"))
      
      pred_coef <- tidy(mod)
    
      # Create a predictor direction for the coefficients in the regression
      pred_coef$coef_direction <- ifelse(pred_coef$estimate>0,"positive","negative")
      pred_coef$pred <- pred_coef$term
      
      # Merge by predictor name to get expected coef direction data
      pred_coef_test <- inner_join(pred_coef, preds_expected_coefs, by=c("pred"))
    
      if(all((as.character(eval(parse(text=pred_coef_test.expected_coef))) == "undetermined") |
         (as.character(eval(parse(text=pred_coef_test.expected_coef))) == pred_coef_test$coef_direction))) {     
          print(paste0("Hooray! Coefficients for predictor and previous predictors all meet predetermined directions. This predictor will be added to the model."))
      } else {            
          # code if 1 passes, 2 fails
        print(paste0("Boo! This predictor increased the adj R2 >0.01 compared to the previous model, but the coefficient for this predictor or another didn't match the predetermined direction."))
        
          # Turn formular in text, then find the first (+) sign searching from the right (eg. the most recently added predictor that's no good), and cut this section of "+ predictorname" and change it back to formula and model
          mod_form_text <- as.character(mod_form[3])
          pred_to_cut <- paste0(" + ",predictors[i])
          mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
          mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
          mod <- lm(mod_form,taps_lur_data_TEMP)

      }
  } else {            
      # code if 1 fails
          print(paste0("Boo! This predictor didn't increase the adj R2 >0.01 compared to the previous model."))
    
          # Turn formular in text, then find the first (+) sign searching from the right (eg. the most recently added predictor that's no good), and cut this section of "+ predictorname" and change it back to formula and model
          mod_form_text <- as.character(mod_form[3])
          pred_to_cut <- paste0(" + ",predictors[i])
          mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
          mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
          mod <- lm(mod_form,taps_lur_data_TEMP)
  
  }
  
  }
  
  pvaltest_preds <- tidy(mod)

  while(any(is.na(pvaltest_preds$p.value))){
    
  var_with_NA_pvalue <- pvaltest_preds[(which(pvaltest_preds$p.value == max(pvaltest_preds$p.value))),1]  # get the var with max pvalue
  
  mod_form_text <- as.character(mod_form[3])
  pred_to_cut <- paste0(" + ",var_with_NA_pvalue)
  mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
  mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
  mod <- lm(mod_form,taps_lur_data_TEMP)
  pvaltest_preds <- tidy(mod)

  }
  
  # Pulling predictors one by one with pvalues>0.1
  pvaltest_preds <- tidy(mod)
  
  # Filter out the intercept as a candidate for this approach
  pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")

  while(any(pvaltest_preds$p.value > 0.1)){
    
    var_with_max_pvalue <- pvaltest_preds[(which(pvaltest_preds$p.value == max(pvaltest_preds$p.value))),1]  # get the var with max pvalue
    
        formula.new <- as.formula(
        paste(
            paste(deparse(mod_form), collapse=""),
            paste(var_with_max_pvalue, collapse="-"),
            sep="-"
        )
    )

    mod <- lm(formula.new,taps_lur_data_TEMP)
    mod_form <-  eval(mod$call[[2]])

    # # Cut out the predictor with pvalue > 0.1
    # mod_form_text <- as.character(mod_form[3])
    # mod_form_text <- gsub("++0","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+  ","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub(var_with_max_pvalue,"",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+0","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+  ","",mod_form_text,fixed = TRUE)
    # # cut any + signs that end the model text
    # mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
    # mod <- lm(mod_form,taps_lur_data_TEMP)
    pvaltest_preds <- tidy(mod)

    # Filter out the intercept as a candidate for this approach for the next 'pass'
    pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")
  }
  
  alias(mod)
  #the linearly dependent variables
  ld.vars <- attributes(alias(mod)$Complete)$dimnames[[1]]

  while(any(length(ld.vars > 0))) {
    # remove the linearly dependent variables
    formula.new <- as.formula(
        paste(
            paste(deparse(mod_form), collapse=""),
            paste(ld.vars, collapse="-"),
            sep="-"
        )
    )

    mod <- lm(formula.new,taps_lur_data_TEMP)
    mod_form <-  eval(mod$call[[2]])

    #Take out any predictors that are COMPLETELY linearly related
    alias(mod)
    #the linearly dependent variables
    ld.vars <- attributes(alias(mod)$Complete)$dimnames[[1]]

  }
  
  # Pulling predictors one by one with pvalues>0.1
  pvaltest_preds <- tidy(mod)
  

  # Check for VIF being <3 for all predictors, but only if there is more than 1!
  if(length(mod$coefficients) > 2) {
    
    # Create VIFs
    all_vifs <- vif(mod)
    signif_all <- names(all_vifs)
    
    while(any(all_vifs > 3)){ #NOTE if vif>3, then exclude from model, starting with largest VIF first as needed
    var_with_max_vif <- names(which(all_vifs == max(all_vifs)))  # get the var with max vif
  
    # Cut out the predictor with pvalue > 0.1
    # mod_form_text <- as.character(mod_form[3])
    # pred_to_cut <- var_with_max_vif
    # mod_form_text <- gsub(pred_to_cut,"",mod_form_text,fixed = TRUE)
    # mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
    # mod <- lm(mod_form,taps_lur_data_TEMP)
    formula.new <- as.formula(
        paste(
            paste(deparse(mod_form), collapse=""),
            paste(var_with_max_vif, collapse="-"),
            sep="-"
        )
    )

    mod <- lm(formula.new,taps_lur_data_TEMP)
    mod_form <-  eval(mod$call[[2]])
    
    all_vifs <- vif(mod)
    pvaltest_preds <- tidy(mod)

    }
  }
  
    # Pulling predictors one by one with pvalues>0.1
  pvaltest_preds <- tidy(mod)
  
  # Filter out the intercept as a candidate for this approach
  pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")

  
  while(any(pvaltest_preds$p.value > 0.1)){
    
    var_with_max_pvalue <- pvaltest_preds[(which(pvaltest_preds$p.value == max(pvaltest_preds$p.value))),1]  # get the var with max pvalue
    
        formula.new <- as.formula(
        paste(
            paste(deparse(mod_form), collapse=""),
            paste(var_with_max_pvalue, collapse="-"),
            sep="-"
        )
    )

    mod <- lm(formula.new,taps_lur_data_TEMP)
    mod_form <-  eval(mod$call[[2]])

    # # Cut out the predictor with pvalue > 0.1
    # mod_form_text <- as.character(mod_form[3])
    # mod_form_text <- gsub("++0","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+  ","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub(var_with_max_pvalue,"",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+0","",mod_form_text,fixed = TRUE)
    # mod_form_text <- gsub("+  ","",mod_form_text,fixed = TRUE)
    # # cut any + signs that end the model text
    # mod_form <- as.formula(paste0(mod_form[2], mod_form[1], mod_form_text))
    # mod <- lm(mod_form,taps_lur_data_TEMP)
    pvaltest_preds <- tidy(mod)

    # Filter out the intercept as a candidate for this approach for the next 'pass'
    pvaltest_preds <- filter(pvaltest_preds, term != "(Intercept)")
  }
  

# We assessed a high Cook’s D value (i.e. >1), indicating an influential observation, because it could be caused by an (extreme) high or low concentration of one of the site(s) or by an included predictor variable with extreme values or many zero values. 

# 1 - In such case, the developed model was applied to all sites minus the site with the high Cook’s D value and the changes in model structure (parameter estimates and p-values of included variables, and model R2)were evaluated. 

# 2 - If the high Cook’s D value was caused by one of the included predictor variables (indicated by a large change in parameter estimate for that variable without the site), a new LUR model was developed using all sites but without offering that specific predictor variable to the model. 

# Cook's D plot
# identify D values 4/(n-k-1) 
cutoff <- 4/((nrow(taps_lur_data_TEMP)-length(mod$coefficients)-2)) 
plot(mod, which=4, cook.levels=cutoff, labels.id = taps_lur_data_TEMP$hhid_x)

cooksd <- cooks.distance(mod)

if (any(!is.na(cooksd) & cooksd>1)) {
    print("An observation(s) has a Cook's Distances >1! Review the plot to investigate.  Here are next steps from Beelen et al 2013 (the ESCAPE NO2/x LUR paper) : We assessed a high Cook’s D value (i.e. >1), indicating an influential observation, because it could be caused by an (extreme) high or low concentration of one of the site(s) or by an included predictor variable with extreme values or many zero values. In such case, the developed model was applied to all sites minus the site with the high Cook’s D value and the changes in model structure (parameter estimates and p-values of included variables, and model R2)were evaluated. If the high Cook’s D value was caused by one of the included predictor variables (indicated by a large change in parameter estimate for that variable without the site), a new LUR model was developed using all sites but without offering that specific predictor variable to the model.")

  cutoff <- 4/((nrow(taps_lur_data_TEMP)-length(mod$coefficients)-2)) 
  plot(mod, which=4, cook.levels=cutoff, labels.id = taps_lur_data_TEMP$hhid_x)
  
  # SECTION DOES NOT WORK - MAY RETURN TO LATER
  # var_with_max_cooksd <- names(which(cooksd == max(cooksd)))  # get the var with max cooksd
  # 
  # taps_lur_data_TEMP <- taps_lur_data_TEMP[-as.numeric(var_with_max_cooksd), ]


  } else {
  print("All Cook's Distances are <1; no need to investigate for outsize influence.")
}

  return(mod)
}

# Define function to get out model data set to manually update as needed
escape_lur_diag_data <- function(pollutant) {
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  taps_lur_data_TEMP <- filter(taps_lur_data, !is.na(eval(parse(text=pollutant_outcome))))
  
  # Remove predictor columns with all 0s, as this will cause the regression to fail
  taps_lur_data_TEMP <- taps_lur_data_TEMP[, apply(taps_lur_data_TEMP, 2, function(x) !all(x==0))]
  
  return(taps_lur_data_TEMP)
}

# Define function to read in address shapefile for spatial autocorrelation checks
taps_addrs_load <- function(){
  setwd("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Shapefiles")
  addrs <- st_read("Sites.shp", stringsAsFactors = F)
  addrs$hhid_x <- paste(addrs$HHID, addrs$HHIDX, sep="_") #make unique address ID
  addrs <- subset(addrs, addrs$hhid_x != "QF44_A") #dropped as this was only measured once without GPS coords
  
  addrs$hhid_x <- paste(addrs$HHID, addrs$HHIDX, sep="_") #make unique address ID
  addrs <- subset(addrs, select = c(hhid_x))

  addrs <- addrs  %>% 
    st_set_crs(NA) %>% 
    st_set_crs(2868) %>%
    st_transform(addrs, crs = 2868)
}

# Define function to test for spatial autocorrelation (Global and Local with residual map)
spatial_auto_diag <- function(pollutant, pollutant_unit){
  
  taps_resids <- data.frame("hhid_x" = taps_lur_data_TEMP$hhid_x,
                           "resids" = mod$residuals)
  
  taps_resids <- inner_join(taps_addrs, taps_resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  taps_resids <- as(taps_resids, "Spatial")
  
  taps_resids <- spTransform(taps_resids, CRS("+proj=longlat +datum=WGS84"))
  
  taps_resids$long <- taps_resids@coords[,1]
  taps_resids$lat <- taps_resids@coords[,2]
  
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(taps_resids@data$long, taps_resids@data$lat)
  m.I <- moransI(coords,5,taps_resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)

  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,taps_resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(taps_resids@data$long, taps_resids@data$lat)
  m.I <- moransI(coords,3,taps_resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,taps_resids@data$resids, scatter.plot = T)
  l.m.I
  
  return(list(moransIval = moransIval, moransIpvalue = moransIpvalue))

}

spatial_auto_residmap <- function(pollutant, pollutant_unit){
  
  taps_resids <- data.frame("hhid_x" = taps_lur_data_TEMP$hhid_x,
                           "resids" = mod$residuals)
  
  taps_resids <- inner_join(taps_addrs, taps_resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  taps_resids <- as(taps_resids, "Spatial")
  
  taps_resids <- spTransform(taps_resids, CRS("+proj=longlat +datum=WGS84"))
  
  taps_resids$long <- taps_resids@coords[,1]
  taps_resids$lat <- taps_resids@coords[,2]
  
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(taps_resids@data$long, taps_resids@data$lat)
  m.I <- moransI(coords,5,taps_resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,taps_resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(taps_resids@data$long, taps_resids@data$lat)
  m.I <- moransI(coords,3,taps_resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,taps_resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Map of residuals
  palette <- colorNumeric("RdYlBu", taps_resids$resids, n = 5)

  leaflet(data = taps_resids) %>% 
    addTiles() %>%
    addCircleMarkers(stroke = F, fillOpacity = 0.75,
                color = ~palette(resids),
                label = ~hhid_x,
                popup = paste(pollutant, "Model Resid:", format(round(taps_resids$resids, 2), nsmall = 2), "<br>")) %>%
    leaflet::addLegend(pal = palette,
              values = ~resids,
              title = paste(pollutant, "Model Resids. (",pollutant_unit,")"),
              opacity = 0.75,
              position = 'topright') 
}

# Define function for internal leave-one-out cross validation
internal_valid_loocv <- function(){
  # Define training control
  train_control <- trainControl(method="LOOCV")
  # train the model
  model_loocv <- train(formula(mod) ,data=taps_lur_data_TEMP, 
                       trControl=train_control, method="lm")
  # summarize results
  print(model_loocv)
  
  return(model_loocv)

}

# Create function to assemble outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
taps_mod_results_df <- function(pollutant, pollutant_unit) {
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")

  mod_formula <- as.character(as.formula(
  paste0("y ~ ", formatC(coefficients(mod)[1], format = "E", digits = 2, flag = "#"),
    paste0(sprintf("%+1.3g*%s",
                  coefficients(mod)[-1],
                  names(coefficients(mod)[-1])),
          collapse="")
  )
)[3])

r2 <- summary(mod)$r.squared
n_sites <- length(rownames(taps_lur_data_TEMP))

meas_conc_mean <- formatC(mean(eval(parse(text=paste0("taps_lur_data_TEMP$", pollutant_outcome)))), format = "fg", digits = 3, flag = "#")
meas_conc_min <- formatC(min(eval(parse(text=paste0("taps_lur_data_TEMP$", pollutant_outcome)))), format = "fg", digits = 3, flag = "#")
meas_conc_max <- formatC(max(eval(parse(text=paste0("taps_lur_data_TEMP$", pollutant_outcome)))), format = "fg", digits = 3, flag = "#")
meas_conc_mean_range <- paste0(meas_conc_mean, " (", meas_conc_min, "-", meas_conc_max, ")")

rootmse_loocv <- internal_valid_loocv()$results[,"RMSE"]
r2_loocv <- internal_valid_loocv()$results[,"Rsquared"]

moransIval <- formatC(unlist(spatial_auto_diag(pollutant = pollutant, pollutant_unit = pollutant_unit)[1]), format = "f", digits = 2, flag = "#")
moransIpvalue <- formatC(unlist(spatial_auto_diag(pollutant = pollutant, pollutant_unit = pollutant_unit)[2]), format = "f", digits = 2, flag = "#")

moransI_values <- paste0(moransIval, " (", moransIpvalue, ")")

# Create results output table row
taps_results_df <- data.frame(
  "LUR_Model"= mod_formula,
  "R2_of_Model" = formatC(r2, format = "fg", digits = 2, flag = "#"),
  "R2_LOOCV" = formatC(r2_loocv, format = "fg", digits = 2, flag = "#"),
  "RMSE_LOOCV" = formatC(rootmse_loocv, format = "g", digits = 3),
  "N_Sites" = n_sites,
  "Morans_I_Values" = moransI_values,
  "Measured_Concentrations" = meas_conc_mean_range)

  return(taps_results_df)
}


# Define functions for mixed effects model adaptation of final non-mixed model accounting for year and season (Winter, Summer, Intermediate) using all data
# NO2
escape_lur_mix_mod_taps_no2 <- function(){
  # Merge raw data to predictors by year
  data_mix_mod <- data
  
  # Assign 1 of 3 season values (Winter=1=Jan-Feb+Nov-Dec, Intermed=2=March-April+Sept-Oct, Summer)
  # SamplePeriods = 1=Oct, 2=Nov, 3=late Nov (eg Dec), 4=Jan, 5=Feb, 6=March, 7=April, 8=May, 9=June, 10=July, 11=Aug, 12=Sept
  data_mix_mod$stssn3 <- ifelse((data_mix_mod$SamplePeriod==1 | data_mix_mod$SamplePeriod==6 | data_mix_mod$SamplePeriod==7 | data_mix_mod$SamplePeriod==12), 2,
                     ifelse((data_mix_mod$SamplePeriod==2 | data_mix_mod$SamplePeriod==3 | data_mix_mod$SamplePeriod==4 | data_mix_mod$SamplePeriod==5), 1, 3))

  data_mix_mod$stssn3 <- factor(data_mix_mod$stssn3,
                    levels = c(1,2,3),
                    labels = c("Winter", "Intermed.", "Summer"))
  
  data_mix_mod$styear <- 2015

  # Assign month based on SamplePeriods = 1=Oct, 2=Nov, 3=late Nov (eg Dec), 4=Jan, 5=Feb, 6=March, 7=April, 8=May, 9=June, 10=July, 11=Aug, 12=Sept
  data_mix_mod$stmon <- ifelse(data_mix_mod$SamplePeriod < 4, data_mix_mod$SamplePeriod+9,  data_mix_mod$SamplePeriod-3)
    
  # Join atmospheric data by year and month
  data_mix_mod <- inner_join(data_mix_mod, atmos_data, by=c("stmon"="Month","styear"="Year")) %>%
    mutate(hhid_x = HHID_X)

  data_mix_mod$SamplePeriod <- NULL

  data_mix_mod <- inner_join(data_mix_mod, dplyr::select(taps_lur_preds, -WindSpeedmph, -MaxTempF, -MinTempF, -PrecipInch, -RHpct_Hour11), by=c("hhid_x"))

  # Set unique numeric ID for random effect by home
  setDT(data_mix_mod)[, hhid := .GRP, by = hhid_x]
  
  # Correct any predictor values (starting with the 5th column) that are NA to 0
    data_mix_mod[, grep("distintvair1", colnames(data_mix_mod)):ncol(data_mix_mod)][is.na(data_mix_mod[, grep("distintvair1", colnames(data_mix_mod)):ncol(data_mix_mod)])] <- 0

  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  data_mix_mod <- data_mix_mod %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))
  
  mix_mod <- update(mod, no2 ~ . + (1 | hhid) + WindSpeedmph + MaxTempF + MinTempF + PrecipInch + RHpct_Hour11, data=filter(data_mix_mod, !is.na(no2)))
  
  mix_mod <- lmer(formula(mix_mod), data=filter(data_mix_mod, !is.na(no2)))
}

# NOx
escape_lur_mix_mod_taps_nox <- function(){
  # Merge raw data to predictors by year
  data_mix_mod <- data
  
  # Assign 1 of 3 season values (Winter=1=Jan-Feb+Nov-Dec, Intermed=2=March-April+Sept-Oct, Summer)
  # SamplePeriods = 1=Oct, 2=Nov, 3=late Nov (eg Dec), 4=Jan, 5=Feb, 6=March, 7=April, 8=May, 9=June, 10=July, 11=Aug, 12=Sept
  data_mix_mod$stssn3 <- ifelse((data_mix_mod$SamplePeriod==1 | data_mix_mod$SamplePeriod==6 | data_mix_mod$SamplePeriod==7 | data_mix_mod$SamplePeriod==12), 2,
                     ifelse((data_mix_mod$SamplePeriod==2 | data_mix_mod$SamplePeriod==3 | data_mix_mod$SamplePeriod==4 | data_mix_mod$SamplePeriod==5), 1, 3))

  data_mix_mod$stssn3 <- factor(data_mix_mod$stssn3,
                    levels = c(1,2,3),
                    labels = c("Winter", "Intermed.", "Summer"))
  
  data_mix_mod$styear <- 2015

  # Assign month based on SamplePeriods = 1=Oct, 2=Nov, 3=late Nov (eg Dec), 4=Jan, 5=Feb, 6=March, 7=April, 8=May, 9=June, 10=July, 11=Aug, 12=Sept
  data_mix_mod$stmon <- ifelse(data_mix_mod$SamplePeriod < 4, data_mix_mod$SamplePeriod+9,  data_mix_mod$SamplePeriod-3)
    
  # Join atmospheric data by year and month
  data_mix_mod <- inner_join(data_mix_mod, atmos_data, by=c("stmon"="Month","styear"="Year")) %>%
    mutate(hhid_x = HHID_X)

  data_mix_mod$SamplePeriod <- NULL

  data_mix_mod <- inner_join(data_mix_mod, dplyr::select(taps_lur_preds, -WindSpeedmph, -MaxTempF, -MinTempF, -PrecipInch, -RHpct_Hour11), by=c("hhid_x"))

  # Set unique numeric ID for random effect by home
  setDT(data_mix_mod)[, hhid := .GRP, by = hhid_x]
  
  # Correct any predictor values (starting with the 5th column) that are NA to 0
    data_mix_mod[, grep("distintvair1", colnames(data_mix_mod)):ncol(data_mix_mod)][is.na(data_mix_mod[, grep("distintvair1", colnames(data_mix_mod)):ncol(data_mix_mod)])] <- 0

  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  data_mix_mod <- data_mix_mod %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))
  
  mix_mod <- update(mod, nox ~ . + (1 | hhid) + WindSpeedmph + MaxTempF + MinTempF + PrecipInch + RHpct_Hour11, data=filter(data_mix_mod, !is.na(nox)))
  
  mix_mod <- lmer(formula(mix_mod), data=filter(data_mix_mod, !is.na(nox)))
}



# PM2.5
escape_lur_mix_mod_taps_pm25 <- function(){
  # Merge raw data to predictors by year
  data_mix_mod <- data
  
  # Assign 1 of 3 season values (Winter=1=Jan-Feb+Nov-Dec, Intermed=2=March-April+Sept-Oct, Summer)
  # SamplePeriods = 1=Oct, 2=Nov, 3=late Nov (eg Dec), 4=Jan, 5=Feb, 6=March, 7=April, 8=May, 9=June, 10=July, 11=Aug, 12=Sept
  data_mix_mod$stssn3 <- ifelse((data_mix_mod$SamplePeriod==1 | data_mix_mod$SamplePeriod==6 | data_mix_mod$SamplePeriod==7 | data_mix_mod$SamplePeriod==12), 2,
                     ifelse((data_mix_mod$SamplePeriod==2 | data_mix_mod$SamplePeriod==3 | data_mix_mod$SamplePeriod==4 | data_mix_mod$SamplePeriod==5), 1, 3))

  data_mix_mod$stssn3 <- factor(data_mix_mod$stssn3,
                    levels = c(1,2,3),
                    labels = c("Winter", "Intermed.", "Summer"))
  
  data_mix_mod$styear <- 2015

  # Assign month based on SamplePeriods = 1=Oct, 2=Nov, 3=late Nov (eg Dec), 4=Jan, 5=Feb, 6=March, 7=April, 8=May, 9=June, 10=July, 11=Aug, 12=Sept
  data_mix_mod$stmon <- ifelse(data_mix_mod$SamplePeriod < 4, data_mix_mod$SamplePeriod+9,  data_mix_mod$SamplePeriod-3)
    
  # Join atmospheric data by year and month
  data_mix_mod <- inner_join(data_mix_mod, atmos_data, by=c("stmon"="Month","styear"="Year")) %>%
    mutate(hhid_x = HHID_X)

  data_mix_mod$SamplePeriod <- NULL

  data_mix_mod <- inner_join(data_mix_mod, dplyr::select(taps_lur_preds, -WindSpeedmph, -MaxTempF, -MinTempF, -PrecipInch, -RHpct_Hour11), by=c("hhid_x"))

  # Set unique numeric ID for random effect by home
  setDT(data_mix_mod)[, hhid := .GRP, by = hhid_x]
  
  # Correct any predictor values (starting with the 5th column) that are NA to 0
    data_mix_mod[, grep("distintvair1", colnames(data_mix_mod)):ncol(data_mix_mod)][is.na(data_mix_mod[, grep("distintvair1", colnames(data_mix_mod)):ncol(data_mix_mod)])] <- 0

  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  data_mix_mod <- data_mix_mod %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))
  mix_mod <- update(mod, pm25 ~ . + (1 | hhid) + WindSpeedmph + MaxTempF + MinTempF + PrecipInch + RHpct_Hour11, data=filter(data_mix_mod, !is.na(pm25)))
  
  mix_mod <- lmer(formula(mix_mod), data=filter(data_mix_mod, !is.na(pm25)))
}

# PM10
escape_lur_mix_mod_taps_pm10 <- function(){
  # Merge raw data to predictors by year
  data_mix_mod <- data
  
  # Assign 1 of 3 season values (Winter=1=Jan-Feb+Nov-Dec, Intermed=2=March-April+Sept-Oct, Summer)
  # SamplePeriods = 1=Oct, 2=Nov, 3=late Nov (eg Dec), 4=Jan, 5=Feb, 6=March, 7=April, 8=May, 9=June, 10=July, 11=Aug, 12=Sept
  data_mix_mod$stssn3 <- ifelse((data_mix_mod$SamplePeriod==1 | data_mix_mod$SamplePeriod==6 | data_mix_mod$SamplePeriod==7 | data_mix_mod$SamplePeriod==12), 2,
                     ifelse((data_mix_mod$SamplePeriod==2 | data_mix_mod$SamplePeriod==3 | data_mix_mod$SamplePeriod==4 | data_mix_mod$SamplePeriod==5), 1, 3))

  data_mix_mod$stssn3 <- factor(data_mix_mod$stssn3,
                    levels = c(1,2,3),
                    labels = c("Winter", "Intermed.", "Summer"))
  
  data_mix_mod$styear <- 2015

  # Assign month based on SamplePeriods = 1=Oct, 2=Nov, 3=late Nov (eg Dec), 4=Jan, 5=Feb, 6=March, 7=April, 8=May, 9=June, 10=July, 11=Aug, 12=Sept
  data_mix_mod$stmon <- ifelse(data_mix_mod$SamplePeriod < 4, data_mix_mod$SamplePeriod+9,  data_mix_mod$SamplePeriod-3)
    
  # Join atmospheric data by year and month
  data_mix_mod <- inner_join(data_mix_mod, atmos_data, by=c("stmon"="Month","styear"="Year")) %>%
    mutate(hhid_x = HHID_X)

  data_mix_mod$SamplePeriod <- NULL

  data_mix_mod <- inner_join(data_mix_mod, dplyr::select(taps_lur_preds, -WindSpeedmph, -MaxTempF, -MinTempF, -PrecipInch, -RHpct_Hour11), by=c("hhid_x"))

  # Set unique numeric ID for random effect by home
  setDT(data_mix_mod)[, hhid := .GRP, by = hhid_x]
  
  # Correct any predictor values (starting with the 5th column) that are NA to 0
    data_mix_mod[, grep("distintvair1", colnames(data_mix_mod)):ncol(data_mix_mod)][is.na(data_mix_mod[, grep("distintvair1", colnames(data_mix_mod)):ncol(data_mix_mod)])] <- 0

  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  data_mix_mod <- data_mix_mod %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))
  mix_mod <- update(mod, pm10 ~ . + (1 | hhid) + WindSpeedmph + MaxTempF + MinTempF + PrecipInch + RHpct_Hour11, data=filter(data_mix_mod, !is.na(pm10)))
  
  mix_mod <- lmer(formula(mix_mod), data=filter(data_mix_mod, !is.na(pm10)))
}
```


### TAPS LUR Creation - NO2

```{r TAPS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - NO2,  message=FALSE, warning=FALSE}

# Create address set to work on 
taps_addrs <- taps_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_taps(pollutant = 'no2')
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'no2')
summary(mod)
AICc(mod)

# General diagnostics
ols_plot_diagnostics(mod)

# Spatial autocorrelation diagnostics
spatial_auto_residmap(pollutant = 'no2', pollutant_unit = 'ppb')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_taps_no2()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

# Finalize model with naming
taps_no2_mod <- mod
taps_no2_mix_mod <- mix_mod

# Create df of outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
taps_no2_results_df <- taps_mod_results_df(pollutant = 'no2', pollutant_unit = 'ppb')

# Add the AICc values for linear and mixed mdoels for comparison
taps_no2_results_df <- data.frame(taps_no2_results_df,
                                  "AICc" = AICc(mod),
                                  "AICc_mix" = AICc(mix_mod))

# Create output of predicted values for comparison to external model predictions
taps_no2_mod_predicted <- data.frame('hhid_x' = taps_lur_data_TEMP$hhid_x,
                                     taps_no2_mod_predicted = fitted(mod))
```

### TAPS LUR Creation - NOx

```{r TAPS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - NOx, eval=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
taps_addrs <- taps_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_taps(pollutant = 'nox')
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'nox')
summary(mod)

# NOTE - We get roads_rl values of different sizes based on following ESCAPE rules, we will rewrite manually with roads_rl_1000 - roads_rl_100

#"When a variable is included, other buffer sizes of the same variable can be offered to the model, both smaller and larger buffers. In model development the original sizes can be offered to judge whether they provide additional explained variability. Due to co-linearity, slopes may be instable but the predicted values and R2 are valid. In the final model, we will rewrite the model using ‘outer or inner rings’ of buffers. For example, if urban green with a 1000m buffer is included in the model first (see Table 4) and then urban green within a 100m buffer with both negative signs, the final model will be written as urban green in a 100m buffer and urban green in 1000m buffer minus urban green in 100m buffer. This will result in more interpretable regression slopes."

mod <- update(mod, . ~ . -roads_rl_1000 -roads_rl_100 +(roads_rl_1000-roads_rl_100),
              data = taps_lur_data_TEMP)
AICc(mod)
summary(mod)

# General diagnostics
ols_plot_diagnostics(mod)

# Spatial autocorrelation diagnostics
spatial_auto_residmap(pollutant = 'nox', pollutant_unit = 'ppb')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_taps_nox()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

# Finalize model with naming
taps_nox_mod <- mod
taps_nox_mix_mod <- mix_mod

# Create df of outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
taps_nox_results_df <- taps_mod_results_df(pollutant = 'nox', pollutant_unit = 'ppb')

# Add the AICc values for linear and mixed mdoels for comparison
taps_nox_results_df <- data.frame(taps_nox_results_df,
                                  "AICc" = AICc(mod),
                                  "AICc_mix" = AICc(mix_mod))

# Create output of predicted values for comparison to external model predictions
taps_nox_mod_predicted <- data.frame('hhid_x' = taps_lur_data_TEMP$hhid_x,
                                     taps_nox_mod_predicted = fitted(mod))

```

### TAPS LUR Creation - PM2.5

```{r TAPS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - PM2.5, eval=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
taps_addrs <- taps_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_taps(pollutant = 'pm25')

# Based on output, test model variables and home with Cook's D > 1: LE46_A
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm25')

summary(mod)

hhids_to_drop = c("LE46_A")

summary(lm(mod,data = filter(taps_lur_data_TEMP, !(hhid_x %in% hhids_to_drop))))
# Substantial change in Ycoord, redevelop model without this predictor
mod <- escape_lur_mod_taps(pollutant = 'pm25', pred_to_drop = c('Ycoord'))

# Continue with normal model diagnostics

summary(mod)

# # General diagnostics
# ols_plot_diagnostics(mod) # doesn't work for whatever reason

# Spatial autocorrelation diagnostics
spatial_auto_residmap(pollutant = 'pm25', pollutant_unit = 'ug/m3')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_taps_pm25()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

# Finalize model with naming
taps_pm25_mod <- mod
taps_pm25_mix_mod <- mix_mod

# Create df of outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
taps_pm25_results_df <- taps_mod_results_df(pollutant = 'pm25', pollutant_unit = 'ug/m3')

# Add the AICc values for linear and mixed mdoels for comparison
taps_pm25_results_df <- data.frame(taps_pm25_results_df,
                                  "AICc" = AICc(mod),
                                  "AICc_mix" = AICc(mix_mod))

# Create output of predicted values for comparison to external model predictions
taps_pm25_mod_predicted <- data.frame('hhid_x' = taps_lur_data_TEMP$hhid_x,
                                     taps_pm25_mod_predicted = fitted(mod))


```

### TAPS LUR Creation - PM10

```{r TAPS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - PM10, eval=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
taps_addrs <- taps_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_taps(pollutant = 'pm10')

# Based on output, test model variables and home with Cook's D > 1: CH_25A and DF87_A
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm10')

summary(mod)

hhids_to_drop = c("CH25_A","DF87_A")

summary(lm(mod,data = filter(taps_lur_data_TEMP, !(hhid_x %in% hhids_to_drop))))

# Substantial change in distintvair1, redevelop model without this predictor
mod <- escape_lur_mod_taps(pollutant = 'pm10', pred_to_drop = c('distintvair1'))

# Continue with normal model diagnostics
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm10')

summary(mod)

# General diagnostics
ols_plot_diagnostics(mod)

# Spatial autocorrelation diagnostics
spatial_auto_residmap(pollutant = 'pm10', pollutant_unit = 'ug/m3')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_taps_pm10()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

# Finalize model with naming
taps_pm10_mod <- mod
taps_pm10_mix_mod <- mix_mod

# Create df of outputs of model formula, R2, LOOCV R2, LOOCV RMSE, N of sites, Moran's I, Moran's I pvalue, Measured Conc Mean(Range) ala ESCAPE papers for results tables
taps_pm10_results_df <- taps_mod_results_df(pollutant = 'pm10', pollutant_unit = 'ug/m3')

# Add the AICc values for linear and mixed mdoels for comparison
taps_pm10_results_df <- data.frame(taps_pm10_results_df,
                                  "AICc" = AICc(mod),
                                  "AICc_mix" = AICc(mix_mod))

# Create output of predicted values for comparison to external model predictions
taps_pm10_mod_predicted <- data.frame('hhid_x' = taps_lur_data_TEMP$hhid_x,
                                     taps_pm10_mod_predicted = fitted(mod))


```


```{r Internal Validation Model Results Table Creation, echo=TRUE, message=FALSE, warning=FALSE}

# Combine results dfs from PCWS and TAPS for all pollutants
nox_results_dfs_paper <- do.call("rbind", list(pcws_no2_results_df,
                                           pcws_no2_multiobs_results_df,
                                           taps_no2_results_df,
                                           taps_nox_results_df)) %>%
  mutate(Model_Dataset = c("PCWS","PCWS: Only >1 Obs.","TAPS","TAPS"),
         Pollutant = c("NO2","NO2","NO2","NOx"),
         R2_of_Model = 100 * as.numeric(as.character(R2_of_Model)),
         R2_of_Model = formatC(R2_of_Model, format = "fg", digits = 1),
         R2_of_Model = paste0(R2_of_Model,"%"),
         R2_LOOCV = 100 * as.numeric(as.character(R2_LOOCV)),
         R2_LOOCV = formatC(R2_LOOCV, format = "fg", digits = 1),
         R2_LOOCV = paste0(R2_LOOCV,"%"),
         AICc = comma(formatC(AICc, format = "fg", digits = 1)),
         AICc_mix = comma(formatC(AICc_mix, format = "fg", digits = 1))) %>%
  dplyr::select(Pollutant, Model_Dataset, LUR_Model, R2_of_Model, R2_LOOCV, RMSE_LOOCV, AICc, AICc_mix, N_Sites, Morans_I_Values) 

pm_results_dfs_paper <- do.call("rbind", list(pcws_pm25_results_df,
                                           taps_pm25_results_df,
                                           pcws_pm10_results_df,
                                           taps_pm10_results_df)) %>%
  mutate(Model_Dataset = c("PCWS","TAPS","PCWS","TAPS"),
         Pollutant = c("PM2.5","PM2.5","PM10","PM10"),
         R2_of_Model = 100 * as.numeric(as.character(R2_of_Model)),
         R2_of_Model = formatC(R2_of_Model, format = "fg", digits = 1),
         R2_of_Model = paste0(R2_of_Model,"%"),
         R2_LOOCV = 100 * as.numeric(as.character(R2_LOOCV)),
         R2_LOOCV = formatC(R2_LOOCV, format = "fg", digits = 1),
         R2_LOOCV = paste0(R2_LOOCV,"%"),
         AICc = comma(formatC(AICc, format = "fg", digits = 1)),
         AICc_mix = comma(formatC(AICc_mix, format = "fg", digits = 1))) %>%
  dplyr::select(Pollutant, Model_Dataset, LUR_Model, R2_of_Model, R2_LOOCV, RMSE_LOOCV, AICc, AICc_mix, N_Sites, Morans_I_Values) 
# NOTE - I'm taking out Measured_Concentrations as this makes the table just too wide and is already shown earlier in section on outcome levels by study and year


# Create tables from df
formattable(nox_results_dfs_paper)
formattable(pm_results_dfs_paper)

```

```{r Internal Validation Model Results Bar Chart Creation, echo=TRUE, message=FALSE, warning=FALSE}

# Combine results dfs from PCWS and TAPS for all pollutants
nox_results_dfs_ppt <- do.call("rbind", list(pcws_no2_results_df,
                                           pcws_no2_multiobs_results_df,
                                           taps_no2_results_df,
                                           taps_nox_results_df)) %>%
  mutate(Model_Dataset = c("PCWS","PCWS: Only >1 Obs.","TAPS","TAPS"),
         Pollutant = c("NO2","NO2","NO2","NOx"),
         R2_of_Model = 100 * as.numeric(as.character(R2_of_Model)),
         R2_of_Model = formatC(R2_of_Model, format = "fg", digits = 1)) %>%
  dplyr::select(Pollutant, Model_Dataset, R2_of_Model, N_Sites) 

nox_results_dfs_ppt_NoPCWSMultiObs <- do.call("rbind", list(pcws_no2_results_df,
                                           taps_no2_results_df,
                                           taps_nox_results_df)) %>%
  mutate(Model_Dataset = c("PCWS","TAPS","TAPS"),
         Pollutant = c("NO2","NO2","NOx"),
         R2_of_Model = 100 * as.numeric(as.character(R2_of_Model)),
         R2_of_Model = formatC(R2_of_Model, format = "fg", digits = 1)) %>%
  dplyr::select(Pollutant, Model_Dataset, R2_of_Model, N_Sites)

pm_results_dfs_ppt <- do.call("rbind", list(pcws_pm25_results_df,
                                           taps_pm25_results_df,
                                           pcws_pm10_results_df,
                                           taps_pm10_results_df)) %>%
  mutate(Model_Dataset = c("PCWS","TAPS","PCWS","TAPS"),
         Pollutant = c("PM2.5","PM2.5","PM10","PM10"),
         R2_of_Model = 100 * as.numeric(as.character(R2_of_Model)),
         R2_of_Model = formatC(R2_of_Model, format = "fg", digits = 1)) %>%
  dplyr::select(Pollutant, Model_Dataset, R2_of_Model, N_Sites) 

results_dfs_ppt <- bind_rows(nox_results_dfs_ppt, pm_results_dfs_ppt)
results_dfs_ppt_NoPCWSMultiObs <- bind_rows(nox_results_dfs_ppt_NoPCWSMultiObs, pm_results_dfs_ppt)

# Create graphs from dfs for ppt
results_dfs_ppt_NoPCWSMultiObs_graph <- ggplot(results_dfs_ppt_NoPCWSMultiObs,aes(x=Pollutant, y=as.numeric(R2_of_Model), fill=Model_Dataset)) +
  geom_bar(stat="identity", position=position_dodge2(width=0.9, preserve = 'single'),geom_params = list(width=.5)) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 10)) + scale_fill_brewer(palette="Paired") + 
  labs(fill="Model Dataset") +
  xlab("Pollutant") +
  ylab(expression(paste("Model ",R^2, " (%)"))) + 
  scale_x_discrete(labels=c("NO2" = expression(NO[2]),
                            "NOx" = expression(NO[x]),
                            "PM2.5" = expression(PM[2.5]),
                            "PM10" = expression(PM[10]))) +
  theme_set(theme_bw()) +
  theme(axis.text=element_text(size=12),
        axis.title.x=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        legend.title=element_text(size=14,face="bold"),
        legend.text=element_text(size=12,face="bold"), 
        legend.position="top")


jpeg("lur_internal_results_dfs_ppt.jpeg", width = 1500, height = 1500, res = 300)
results_dfs_ppt_NoPCWSMultiObs_graph
print(results_dfs_ppt_NoPCWSMultiObs_graph)
dev.off()


```

## External Validation - Evaluating LUR Models for Predicting Air Pollutant Levels into the Future and the Past

In this part, for each pollutant except NOx, we will use the model developed from PCWS, update with 4 approaches, and then externally validate by predicting TAPS data...and vice versa.

For each pollutant, the model with: 
1. the lowest Kolmogorov-Smirnov D; 
2. the highest externally adjusted R2; and 
3. lowest root-mean-squared error values 
will have residuals tested for spatial autocorrelation (Moran’s I). 

**Update 7/24/19-after talking with Paloma, will use model with least significant spatial autocorr as it's expected that these models will have some, just becomes discussion point.

If there is statistically significant spatial autocorrelation (p-value <0.05) detected in the residuals of that model, the next model will be evaluated for spatial autocorrelation, and so on. 

I will use this model to predict air pollutant exposures for TCRS participants at birth and age 6 home addresses. 

Before external validation testing, we need to confirm predictors in final models from each dataset are in the respective predictor datasets (eg pcws/taps_lur_preds). If they are not, we will add them in with 0 values, but this will only be done for Methods 1 and 4!


```{r Reload Full PCWS Dataset, echo=FALSE, message=FALSE, warning=FALSE}

# Combine air pollution values and predictors
pcws_lur_data <- inner_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))

# Check for homes iwth no addresses and make sure they really don't have anything
pcws_lur_data_nonjoin <- anti_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))
# 21 homes with no address data, many unique to 1992, which we know have no address data

# Join atmospheric data
pcws_lur_data <- inner_join(pcws_lur_data, atmos_data_yearavgs, by=c("styear"="Year"))

# Correct any predictor values (starting with the 6th column) that are NA to 0
pcws_lur_data[, 6:ncol(pcws_lur_data)][is.na(pcws_lur_data[, 6:ncol(pcws_lur_data)])] <- 0
# 
# # Remove predictor columns with all 0s, as this will cause the regression to fail
# pcws_lur_data <- pcws_lur_data[, apply(pcws_lur_data, 2, function(x) !all(x==0))] 

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
# Also drop count variables
pcws_lur_data <- pcws_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2)) %>%
  dplyr::select(-no2_adj_n ,-pm25_adj_n ,-pm10_adj_n) 

# NOTE - we are dropping observations that start in 1992 - there are results data but no address info
pcws_lur_data <- filter(pcws_lur_data, styear!=1992)
```

```{r Confirm All Model Predictors in Ext. Val. Datasets, echo=TRUE, message=FALSE, warning=FALSE}

# Define function to check if TAPS-based model variables are in PCWS predictor dataset
pcws_extval_predscheck <- function(pollutant){
  # Get model name together
  mod <- paste("taps",pollutant,"mod",sep = "_")
  
  # Pull out all finalized model predictors in models
  mod_preds <- filter(tidy(eval(parse(text=mod))), term != "(Intercept)")
  mod_preds <- mod_preds[,"term"]
  
  # Check if all predictors are in ext val predictor set, if they're not, add them in with 0 for all values
  for(i in 1:length(mod_preds)){
    if(!(mod_preds[i] %in% names(pcws_lur_preds))){
      print(paste("HOLD UP!", mod_preds[i], "is NOT in the pcws_lur_preds dataset - will add it with all 0 values for Methods 1 and 4!"))
      # NOTE THIS CODE DOESN'T WORK
      # missingvar_df <- data.frame(0, row.names = c(eval(parse(text=mod_preds[i]))))
      # mod_preds[i] <- rep(0, length(pcws_lur_preds$hhid_x))
      # # missing_var <- paste0("pcws_lur_preds$",mod_preds[i]))
      # pcws_lur_preds <- bind_cols(pcws_lur_preds,
      #                              mod_preds[i] <- rep(0, length(pcws_lur_preds$hhid_x)))

      
    } else {
      print(paste(mod_preds[i],"from TAPS",pollutant,"model is already in the pcws_lur_preds dataset!"))
    }
  }
} 

# Check if TAPS-based model variables are in PCWS predictor dataset
pcws_extval_predscheck(pollutant = 'no2')
pcws_extval_predscheck(pollutant = 'pm25')
pcws_extval_predscheck(pollutant = 'pm10')

# Define function to check if PCWS-based model variables are in TAPS predictor dataset
taps_extval_predscheck <- function(pollutant){
  # Get model name together
  mod <- paste("pcws",pollutant,"mod",sep = "_")
  
  # Pull out all finalized model predictors in models
  mod_preds <- filter(tidy(eval(parse(text=mod))), term != "(Intercept)")
  mod_preds <- mod_preds[,"term"]
  
  # Check if all predictors are in ext val predictor set, if they're not, add them in with 0 for all values
  for(i in 1:length(mod_preds)){
    if(!(mod_preds[i] %in% names(taps_lur_preds))){
      print(paste("HOLD UP!", mod_preds[i], "is NOT in the taps_lur_preds data set - will add it with all 0 values for Methods 1 and 4!"))
      taps_lur_preds[,mod_preds[i]] <- 0
    } else {
      print(paste(mod_preds[i],"from PCWS",pollutant,"model is already in the taps_lur_preds data set!"))
    }
  }
} 

# Check if PCWS-based model variables are in TAPS predictor dataset
taps_extval_predscheck(pollutant = 'no2')
taps_extval_predscheck(pollutant = 'pm25')
taps_extval_predscheck(pollutant = 'pm10')
```

### Ext. Val. of PCWS Models by Predicting TAPS

#### Method 1 - Account for regional changes in air pollution levels - apply a temporal scalar or trend based on a continuously running background monitor

Scale 2015 TAPS values back to 1989 using an absolute difference adjustment NOT RATIO (explicit as per ESCAPE Exposure Manual pg 43 and not clarified in any other pubs) from continuously-running PDEQ monitor sites - 1989 is chosen as this is the first available PM2.5 year for PCWS model and because this is approximately the 'half way' mark of all years PCWS measured pollutants (1987-1991)

Predict the scaled TAPS outcomes using the PCWS-based model 

```{r Ext. Val. of PCWS Models - Method 1, echo=TRUE, message=FALSE, warning=FALSE}
 
# Define external validation and spatial autocorrelation function
extval_method1_diag <- function(year_mod_devel, evalmodel_dataset, test_dataset, pollutant, pollutant_unit, predicting_pcws_no2_multisens){
  
  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  lur_data <- paste0(test_dataset, "_lur_data")
  
  # Create a temporary dataset for analysis
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         lur_data_TEMP <- pcws_ap_no2_multi %>%
    inner_join(., pcws_lur_preds, by=c('styear','hhid_x')),
    lur_data_TEMP <- filter(eval(parse(text=lur_data)), !is.na(eval(parse(text=pollutant_outcome)))))

  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))
  
  # Merge the reference data to the temporary dataset on year=styear, note that taps_ap doesn't have a 'styear' field since they're all the same, so we're band-aiding it here by an if statement for 2 styaer fields when merging pcws
  ifelse("styear.x" %in% names(lur_data_TEMP),
         lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear.x'='year')),
                                     lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear'='year')))
  
  # Update lur dataset levels with differences by difference b/w year of outcome to predict minus year of model devel.
  lur_data_TEMP <- lur_data_TEMP %>%
    mutate(no2_adj = no2_adj + (no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2])),
           pm25_adj = pm25_adj + (pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3])),
           pm10_adj = pm10_adj + (pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4])))

  # Evalute PCWS-based model on prediction potential with PCWS data
  mod_name <- paste(evalmodel_dataset, pollutant, "mod", sep="_")
  
  print(summary(lm(eval(parse(text=mod_name)), data=lur_data_TEMP)))
  print(kable(summary(lm(eval(parse(text=mod_name)), data=lur_data_TEMP))$coef, digits=2))

  # Create predicted values
  lur_data_TEMP$predicted <- predict(eval(parse(text=mod_name)), newdata=lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  outcome_name <- paste0("lur_data_TEMP$",pollutant_outcome)
  
  print(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name))))
  
  rootmse <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[1])
  r2 <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[2])

  print(ks.test(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome)))))
  
  ksd <- as.numeric(ks.test(predict(eval(parse(text=mod_name))), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))$statistic)
    
  # Create output df of predicted values
  predicted_df <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                          "predicted" = lur_data_TEMP$predicted) 
  
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method1_pred_",test_dataset,"_multisens")), 
                                     colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method1_pred_",test_dataset)))

  # Side by side plots of densities with x limits based on both datasets
  obsvd_plot <- ggplot(lur_data_TEMP, aes(x=eval(parse(text=outcome_name)))) +
  geom_histogram(fill='black', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  pred_plot <- ggplot(lur_data_TEMP, aes(x=predicted)) +
  geom_histogram(fill='red', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  print(ggarrange(obsvd_plot, pred_plot, 
          labels = c("Observed Values", "Predicted Values"),
          ncol = 1, nrow = 2))

  # Residuals
  lur_data_TEMP$residuals <- eval(parse(text=outcome_name)) - lur_data_TEMP$predicted
  
  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = lur_data_TEMP$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]
   
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,5,resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)
  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,3,resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,resids@data$resids, scatter.plot = T)
  l.m.I
  
  return(list(ksd=ksd, r2=r2, rootmse=rootmse, moransIval=moransIval, moransIpvalue=moransIpvalue, predicted_df=predicted_df))
}

extval_method1_residmap <- function(year_mod_devel, evalmodel_dataset, test_dataset, pollutant, pollutant_unit, predicting_pcws_no2_multisens){

  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  lur_data <- paste0(test_dataset, "_lur_data")
  
  # Create a temporary dataset for analysis
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         lur_data_TEMP <- pcws_ap_no2_multi %>%
    inner_join(., pcws_lur_preds, by=c('styear','hhid_x')),
    lur_data_TEMP <- filter(eval(parse(text=lur_data)), !is.na(eval(parse(text=pollutant_outcome)))))

  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))
  
  # Merge the reference data to the temporary dataset on year=styear, note that taps_ap doesn't have a 'styear' field since they're all the same, so we're band-aiding it here by an if statement for 2 styaer fields when merging pcws
  ifelse("styear.x" %in% names(lur_data_TEMP),
         lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear.x'='year')),
                                     lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear'='year')))
  
  # Update lur dataset levels with differences by difference b/w year of outcome to predict minus year of model devel.
  lur_data_TEMP <- lur_data_TEMP %>%
    mutate(no2_adj = no2_adj + (no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2])),
           pm25_adj = pm25_adj + (pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3])),
           pm10_adj = pm10_adj + (pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4])))

  # Evalute PCWS-based model on prediction potential with PCWS data
  mod_name <- paste(evalmodel_dataset, pollutant, "mod", sep="_")

  # Create predicted values
  lur_data_TEMP$predicted <- predict(eval(parse(text=mod_name)), newdata=lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  outcome_name <- paste0("lur_data_TEMP$",pollutant_outcome)
  
  # Residuals
  lur_data_TEMP$residuals <- eval(parse(text=outcome_name)) - lur_data_TEMP$predicted
  
  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = lur_data_TEMP$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]
  
  # Map of residuals
  palette <- colorNumeric("RdYlBu", resids$resids, n = 5)

  leaflet(data = resids) %>% 
    addTiles() %>%
    addCircleMarkers(stroke = F, fillOpacity = 0.75,
                color = ~palette(resids),
                label = ~hhid_x,
                popup = paste(pollutant, "Model Resid:", format(round(resids$resids, 2), nsmall = 2), "<br>")) %>%
    leaflet::addLegend(pal = palette,
              values = ~resids,
              title = paste(pollutant, "Model Resids. (",pollutant_unit,")"),
              opacity = 0.75,
              position = 'topright') 
  
}

extval_method1_no2_multisens_diag <- function(year_mod_devel, evalmodel_dataset, test_dataset, pollutant, pollutant_unit){
  
  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  lur_data <- paste0(test_dataset, "_lur_data")
  
  lur_data_TEMP <- filter(eval(parse(text=lur_data)), !is.na(eval(parse(text=pollutant_outcome))))
  
  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))

  # Merge the reference data to the temporary dataset on year=styear, note that taps_ap doesn't have a 'styear' field since they're all the same, so we're band-aiding it here by an if statement for 2 styaer fields when merging pcws
  ifelse("styear.x" %in% names(lur_data_TEMP),
         lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear.x'='year')),
                                     lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear'='year')))
  
  # Update lur dataset levels with differences by difference b/w year of outcome to predict minus year of model devel.
  lur_data_TEMP <- lur_data_TEMP %>%
    mutate(no2_adj = no2_adj + (no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2])),
           pm25_adj = pm25_adj + (pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3])),
           pm10_adj = pm10_adj + (pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4])))

  # # Evalute PCWS-based model on prediction potential with PCWS data
  # mod_name <- paste(evalmodel_dataset, pollutant, "mod", sep="_")
  
  print(summary(lm(pcws_no2_mod_multiobs, data=lur_data_TEMP)))
  print(kable(summary(lm(pcws_no2_mod_multiobs, data=lur_data_TEMP))$coef, digits=2))

  # Create predicted values
  lur_data_TEMP$predicted <- predict(pcws_no2_mod_multiobs, newdata=lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  outcome_name <- paste0("lur_data_TEMP$",pollutant_outcome)
  
  print(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name))))
  
  rootmse <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[1])
  r2 <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[2])

  print(ks.test(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome)))))
  
  ksd <- as.numeric(ks.test(predict(pcws_no2_mod_multiobs), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))$statistic)

    # Create output df of predicted values
  predicted_df <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                          "predicted" = lur_data_TEMP$predicted) 
  
  colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method1_multisens_pred_",test_dataset))

  # Side by side plots of densities with x limits based on both datasets
  obsvd_plot <- ggplot(lur_data_TEMP, aes(x=eval(parse(text=outcome_name)))) +
  geom_histogram(fill='black', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  pred_plot <- ggplot(lur_data_TEMP, aes(x=predicted)) +
  geom_histogram(fill='red', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  print(ggarrange(obsvd_plot, pred_plot, 
          labels = c("Observed Values", "Predicted Values"),
          ncol = 1, nrow = 2))

  # Residuals
  lur_data_TEMP$residuals <- eval(parse(text=outcome_name)) - lur_data_TEMP$predicted
  
  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = lur_data_TEMP$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]
   
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,5,resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)
  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,3,resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,resids@data$resids, scatter.plot = T)
  l.m.I

  return(list(ksd=ksd, r2=r2, rootmse=rootmse, moransIval=moransIval, moransIpvalue=moransIpvalue, predicted_df=predicted_df))
}


# NO2
pcws_no2_extval_method1_diag <- extval_method1_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method1_residmap(year_mod_devel = 1989,
                    evalmodel_dataset = 'pcws',
                    test_dataset = 'taps',
                    pollutant = 'no2',
                    pollutant_unit = 'ppb',
                    predicting_pcws_no2_multisens = 'No')

pcws_no2_extval_method1_multisens_diag <- extval_method1_no2_multisens_diag(year_mod_devel = 1989,
                                                                  evalmodel_dataset = 'pcws',
                                                                  test_dataset = 'taps',
                                                                  pollutant = 'no2',
                                                                  pollutant_unit = 'ppb')

# PM2.5
pcws_pm25_extval_method1_diag <- extval_method1_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'pm25',
                                                    pollutant_unit = 'ug/m3', predicting_pcws_no2_multisens = 'No')

extval_method1_residmap(year_mod_devel = 1989,
                    evalmodel_dataset = 'pcws',
                    test_dataset = 'taps',
                    pollutant = 'pm25',
                    pollutant_unit = 'ug/m3', 
                    predicting_pcws_no2_multisens = 'No')


# PM10
pcws_pm10_extval_method1_diag <- extval_method1_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'pm10',
                                                    pollutant_unit = 'ug/m3', predicting_pcws_no2_multisens = 'No')

extval_method1_residmap(year_mod_devel = 1989,
                    evalmodel_dataset = 'pcws',
                    test_dataset = 'taps',
                    pollutant = 'pm10',
                    pollutant_unit = 'ug/m3', 
                    predicting_pcws_no2_multisens = 'No')

# Combine diagnostic outputs and identifiers
pcws_extval_method1_diag_df <- data.frame("pcws_no2_extval_method1" = unlist(pcws_no2_extval_method1_diag[c(1:5)]),
                                          "pcws_no2_extval_method1_multisens" = unlist(pcws_no2_extval_method1_multisens_diag[c(1:5)]),
                                          "pcws_pm25_extval_method1" = unlist(pcws_pm25_extval_method1_diag[c(1:5)]),
                                          "pcws_pm10_extval_method1" = unlist(pcws_pm10_extval_method1_diag[c(1:5)]))

# Combine outputs into a dataframe intended for paper-based table and format
pcws_extval_method1_diag_df_paper <- data.frame(t(pcws_extval_method1_diag_df)) 

pcws_extval_method1_diag_df_paper <- pcws_extval_method1_diag_df_paper%>%
   mutate("Model"= row.names(pcws_extval_method1_diag_df_paper))

# Combine predicted values from each model 
datalist <- list(unnest(pcws_no2_extval_method1_diag$predicted_df),
                                         unnest(pcws_no2_extval_method1_multisens_diag$predicted_df),
                                         unnest(pcws_pm25_extval_method1_diag$predicted_df),
                                         unnest(pcws_pm10_extval_method1_diag$predicted_df))

pcws_extval_method1_pred_taps_df <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all predictor csvs together
```


#### Method 2 - Adjust for changes in the location of predictors - include year-specific predictor data

Get GIS predictors for TAPS locations in 1989 and predict them to get R2 etc
```{r Ext. Val. of PCWS Models - Method 2, echo=TRUE, message=FALSE, warning=FALSE}

# Define external validation and spatial autocorrelation function
extval_method2_diag <- function(year_mod_devel, evalmodel_dataset, test_dataset, pollutant, pollutant_unit, predicting_pcws_no2_multisens){
  
  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  
  ifelse(test_dataset == 'pcws', 
         lur_preds <- pcws_lur_preds_pull(eval(parse(text=year_mod_devel))),
         lur_preds <- taps_lur_preds_pull(eval(parse(text=year_mod_devel))))
         
  
  # Combine air pollution values and predictors
  ifelse(test_dataset == 'pcws', 
         lur_data <- inner_join(pcws_ap, lur_preds, by=c("hhid_x")),
         lur_data <- inner_join(taps_ap, lur_preds, by=c("hhid_x")))
  
  # Drop HHID, HHIDX, and uncorrected columns
  lur_data <- lur_data %>%
    dplyr::select(hhid_x, everything()) %>%
    dplyr::select(-ends_with('unadj')) 
  
  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)][is.na(lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)])] <- 0
  
  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  lur_data <- lur_data %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  # Create a temporary dataset for analysis
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         lur_data_TEMP <- pcws_ap_no2_multi %>%
    inner_join(., pcws_lur_preds, by=c('styear','hhid_x')) %>%
    dplyr::select(-styear),
    lur_data_TEMP <- filter(lur_data, !is.na(eval(parse(text=pollutant_outcome)))))

  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))

  # Evalute PCWS-based model on prediction potential with PCWS data
  mod_name <- paste(evalmodel_dataset, pollutant, "mod", sep="_")
  
  print(summary(lm(eval(parse(text=mod_name)), data=lur_data_TEMP)))
  print(kable(summary(lm(eval(parse(text=mod_name)), data=lur_data_TEMP))$coef, digits=2))

  # Create predicted values
  lur_data_TEMP$predicted <- predict(eval(parse(text=mod_name)), newdata=lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  outcome_name <- paste0("lur_data_TEMP$",pollutant_outcome)
  
  print(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name))))
  
  rootmse <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[1])
  r2 <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[2])

  print(ks.test(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome)))))
  
  ksd <- as.numeric(ks.test(predict(eval(parse(text=mod_name))), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))$statistic)
  
  # Create output df of predicted values
  predicted_df <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                          "predicted" = lur_data_TEMP$predicted) 
  
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method2_pred_",test_dataset,"_multisens")), 
                                     colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method2_pred_",test_dataset)))

  # Side by side plots of densities with x limits based on both datasets
  obsvd_plot <- ggplot(lur_data_TEMP, aes(x=eval(parse(text=outcome_name)))) +
  geom_histogram(fill='black', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  pred_plot <- ggplot(lur_data_TEMP, aes(x=predicted)) +
  geom_histogram(fill='red', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  print(ggarrange(obsvd_plot, pred_plot, 
          labels = c("Observed Values", "Predicted Values"),
          ncol = 1, nrow = 2))

  # Residuals
  lur_data_TEMP$residuals <- eval(parse(text=outcome_name)) - lur_data_TEMP$predicted
  
  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = lur_data_TEMP$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]
   
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,5,resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)
  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,3,resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,resids@data$resids, scatter.plot = T)
  l.m.I
  
  return(list(ksd=ksd, r2=r2, rootmse=rootmse, moransIval=moransIval, moransIpvalue=moransIpvalue, predicted_df=predicted_df))
}

extval_method2_residmap <- function(year_mod_devel, evalmodel_dataset, test_dataset, pollutant, pollutant_unit, predicting_pcws_no2_multisens){

  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  
  ifelse(test_dataset == 'pcws', 
         lur_preds <- pcws_lur_preds_pull(eval(parse(text=year_mod_devel))),
         lur_preds <- taps_lur_preds_pull(eval(parse(text=year_mod_devel))))
         
  
  # Combine air pollution values and predictors
  ifelse(test_dataset == 'pcws', 
         lur_data <- inner_join(pcws_ap, lur_preds, by=c("hhid_x")),
         lur_data <- inner_join(taps_ap, lur_preds, by=c("hhid_x")))
  
  # Drop HHID, HHIDX, and uncorrected columns
  lur_data <- lur_data %>%
    dplyr::select(hhid_x, everything()) %>%
    dplyr::select(-ends_with('unadj')) 
  
  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)][is.na(lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)])] <- 0
  
  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  lur_data <- lur_data %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  # Create a temporary dataset for analysis
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         lur_data_TEMP <- pcws_ap_no2_multi %>%
    inner_join(., pcws_lur_preds, by=c('styear','hhid_x')) %>%
    dplyr::select(-styear),
    lur_data_TEMP <- filter(lur_data, !is.na(eval(parse(text=pollutant_outcome)))))

  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))

  # Evalute PCWS-based model on prediction potential with PCWS data
  mod_name <- paste(evalmodel_dataset, pollutant, "mod", sep="_")

  # Create predicted values
  lur_data_TEMP$predicted <- predict(eval(parse(text=mod_name)), newdata=lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  outcome_name <- paste0("lur_data_TEMP$",pollutant_outcome)

  # Residuals
  lur_data_TEMP$residuals <- eval(parse(text=outcome_name)) - lur_data_TEMP$predicted
  
  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = lur_data_TEMP$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]

  # Map of residuals
  palette <- colorNumeric("RdYlBu", resids$resids, n = 5)

  leaflet(data = resids) %>% 
    addTiles() %>%
    addCircleMarkers(stroke = F, fillOpacity = 0.75,
                color = ~palette(resids),
                label = ~hhid_x,
                popup = paste(pollutant, "Model Resid:", format(round(resids$resids, 2), nsmall = 2), "<br>")) %>%
    leaflet::addLegend(pal = palette,
              values = ~resids,
              title = paste(pollutant, "Model Resids. (",pollutant_unit,")"),
              opacity = 0.75,
              position = 'topright') 
}

extval_method2_no2_multisens_diag <- function(year_mod_devel, evalmodel_dataset, test_dataset, pollutant, pollutant_unit){

    # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  
  ifelse(test_dataset == 'pcws', 
         lur_preds <- pcws_lur_preds_pull(eval(parse(text=year_mod_devel))),
         lur_preds <- taps_lur_preds_pull(eval(parse(text=year_mod_devel))))
         
  
  # Combine air pollution values and predictors
  ifelse(test_dataset == 'pcws', 
         lur_data <- inner_join(pcws_ap, lur_preds, by=c("hhid_x")),
         lur_data <- inner_join(taps_ap, lur_preds, by=c("hhid_x")))
  
  # Drop HHID, HHIDX, and uncorrected columns
  lur_data <- lur_data %>%
    dplyr::select(hhid_x, everything()) %>%
    dplyr::select(-ends_with('unadj')) 
  
  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)][is.na(lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)])] <- 0
  
  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  lur_data <- lur_data %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  # Subset out only pollutant of interest
  lur_data_TEMP <- filter(lur_data, !is.na(eval(parse(text=pollutant_outcome))))
  
  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))

  # # Evalute PCWS-based model on prediction potential with PCWS data
  # mod_name <- paste(evalmodel_dataset, pollutant, "mod", sep="_")
  
  print(summary(lm(pcws_no2_mod_multiobs, data=lur_data_TEMP)))
  print(kable(summary(lm(pcws_no2_mod_multiobs, data=lur_data_TEMP))$coef, digits=2))

  # Create predicted values
  lur_data_TEMP$predicted <- predict(pcws_no2_mod_multiobs, newdata=lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  outcome_name <- paste0("lur_data_TEMP$",pollutant_outcome)
  
  print(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name))))
  
  rootmse <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[1])
  r2 <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[2])

  print(ks.test(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome)))))
  
  ksd <- as.numeric(ks.test(predict(pcws_no2_mod_multiobs), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))$statistic)
  
  # Create output df of predicted values
  predicted_df <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                          "predicted" = lur_data_TEMP$predicted) 
  
  colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method2_multisens_pred_",test_dataset))


  # Side by side plots of densities with x limits based on both datasets
  obsvd_plot <- ggplot(lur_data_TEMP, aes(x=eval(parse(text=outcome_name)))) +
  geom_histogram(fill='black', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  pred_plot <- ggplot(lur_data_TEMP, aes(x=predicted)) +
  geom_histogram(fill='red', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  print(ggarrange(obsvd_plot, pred_plot, 
          labels = c("Observed Values", "Predicted Values"),
          ncol = 1, nrow = 2))

  # Residuals
  lur_data_TEMP$residuals <- eval(parse(text=outcome_name)) - lur_data_TEMP$predicted
  
  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = lur_data_TEMP$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]
   
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,5,resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)
  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,3,resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,resids@data$resids, scatter.plot = T)
  l.m.I
  
  return(list(ksd=ksd, r2=r2, rootmse=rootmse, moransIval=moransIval, moransIpvalue=moransIpvalue, predicted_df=predicted_df))
}


# NO2
pcws_no2_extval_method2_diag <- extval_method2_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method2_residmap(year_mod_devel = 1989,
                    evalmodel_dataset = 'pcws',
                    test_dataset = 'taps',
                    pollutant = 'no2',
                    pollutant_unit = 'ppb',
                    predicting_pcws_no2_multisens = 'No')

pcws_no2_extval_method2_multisens_diag <- extval_method2_no2_multisens_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb')

# PM2.5
pcws_pm25_extval_method2_diag <- extval_method2_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'pm25',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method2_residmap(year_mod_devel = 1989,
                    evalmodel_dataset = 'pcws',
                    test_dataset = 'taps',
                    pollutant = 'pm25',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')


# PM10
pcws_pm10_extval_method2_diag <- extval_method2_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'pm10',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method2_residmap(year_mod_devel = 1989,
                    evalmodel_dataset = 'pcws',
                    test_dataset = 'taps',
                    pollutant = 'pm10',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')

# Combine diagnostic outputs and identifiers
pcws_extval_method2_diag_df <- data.frame("pcws_no2_extval_method2" = unlist(pcws_no2_extval_method2_diag[c(1:5)]),
                                          "pcws_no2_extval_method2_multisens" = unlist(pcws_no2_extval_method2_multisens_diag[c(1:5)]),
                                          "pcws_pm25_extval_method2" = unlist(pcws_pm25_extval_method2_diag[c(1:5)]),
                                          "pcws_pm10_extval_method2" = unlist(pcws_pm10_extval_method2_diag[c(1:5)]))

# Combine outputs into a dataframe intended for paper-based table and format
pcws_extval_method2_diag_df_paper <- data.frame(t(pcws_extval_method2_diag_df)) 

pcws_extval_method2_diag_df_paper <- pcws_extval_method2_diag_df_paper%>%
   mutate("Model"= row.names(pcws_extval_method2_diag_df_paper))

# Combine predicted values from each model 
datalist <- list(unnest(pcws_no2_extval_method2_diag$predicted_df),
                                         unnest(pcws_no2_extval_method2_multisens_diag$predicted_df),
                                         unnest(pcws_pm25_extval_method2_diag$predicted_df),
                                         unnest(pcws_pm10_extval_method2_diag$predicted_df))

pcws_extval_method2_pred_taps_df <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all predictor csvs together
```

#### Method 3 - Combine Methods 1 + 2
```{r Ext. Val. of PCWS Models - Method 3, echo=TRUE, message=FALSE, warning=FALSE}

# Define external validation and spatial autocorrelation function
extval_method3_diag <- function(year_mod_devel, evalmodel_dataset, test_dataset, pollutant, pollutant_unit, predicting_pcws_no2_multisens){

  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  
  ifelse(test_dataset == 'pcws', 
         lur_preds <- pcws_lur_preds_pull(eval(parse(text=year_mod_devel))),
         lur_preds <- taps_lur_preds_pull(eval(parse(text=year_mod_devel))))
         
  # Combine air pollution values and predictors
  ifelse(test_dataset == 'pcws', 
         lur_data <- inner_join(pcws_ap, lur_preds, by=c("hhid_x")),
         lur_data <- inner_join(taps_ap, lur_preds, by=c("hhid_x")))
  
  # Drop HHID, HHIDX, and uncorrected columns
  lur_data <- lur_data %>%
    dplyr::select(hhid_x, everything()) %>%
    dplyr::select(-ends_with('unadj')) 

  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)][is.na(lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)])] <- 0
  
  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  lur_data <- lur_data %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  # Create a temporary dataset for analysis
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         lur_data_TEMP <- pcws_ap_no2_multi %>%
    inner_join(., pcws_lur_preds, by=c('styear','hhid_x')),
    lur_data_TEMP <- filter(lur_data, !is.na(eval(parse(text=pollutant_outcome)))))

  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))
  
  # Merge the reference data to the temporary dataset on year=styear, note that taps_ap doesn't have a 'styear' field since they're all the same, so we're band-aiding it here by an if statement for 2 styaer fields when merging pcws
  ifelse("styear.x" %in% names(lur_data_TEMP),
         lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear.x'='year')),
                                     lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear'='year')))
  
  # Update lur dataset levels with differences by difference b/w year of outcome to predict minus year of model devel.
  lur_data_TEMP <- lur_data_TEMP %>%
    mutate(no2_adj = no2_adj + (no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2])),
           pm25_adj = pm25_adj + (pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3])),
           pm10_adj = pm10_adj + (pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4])))

  # Evalute PCWS-based model on prediction potential with PCWS data
  mod_name <- paste(evalmodel_dataset, pollutant, "mod", sep="_")
  
  print(summary(lm(eval(parse(text=mod_name)), data=lur_data_TEMP)))
  print(kable(summary(lm(eval(parse(text=mod_name)), data=lur_data_TEMP))$coef, digits=2))

  # Create predicted values
  lur_data_TEMP$predicted <- predict(eval(parse(text=mod_name)), newdata=lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  outcome_name <- paste0("lur_data_TEMP$",pollutant_outcome)
  
  print(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name))))
  
  rootmse <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[1])
  r2 <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[2])

  print(ks.test(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome)))))
  
  ksd <- as.numeric(ks.test(predict(eval(parse(text=mod_name))), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))$statistic)

  # Create output df of predicted values
  predicted_df <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                          "predicted" = lur_data_TEMP$predicted) 
  
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method3_pred_",test_dataset,"_multisens")), 
                                     colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method3_pred_",test_dataset)))

  # Side by side plots of densities with x limits based on both datasets
  obsvd_plot <- ggplot(lur_data_TEMP, aes(x=eval(parse(text=outcome_name)))) +
  geom_histogram(fill='black', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  pred_plot <- ggplot(lur_data_TEMP, aes(x=predicted)) +
  geom_histogram(fill='red', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  print(ggarrange(obsvd_plot, pred_plot, 
          labels = c("Observed Values", "Predicted Values"),
          ncol = 1, nrow = 2))

  # Residuals
  lur_data_TEMP$residuals <- eval(parse(text=outcome_name)) - lur_data_TEMP$predicted
  
  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = lur_data_TEMP$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]
   
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,5,resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)
  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,3,resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,resids@data$resids, scatter.plot = T)
  l.m.I
  
  return(list(ksd=ksd, r2=r2, rootmse=rootmse, moransIval=moransIval, moransIpvalue=moransIpvalue, predicted_df=predicted_df))
}

extval_method3_residmap <- function(year_mod_devel, evalmodel_dataset, test_dataset, pollutant, pollutant_unit, predicting_pcws_no2_multisens){

  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  
  ifelse(test_dataset == 'pcws', 
         lur_preds <- pcws_lur_preds_pull(eval(parse(text=year_mod_devel))),
         lur_preds <- taps_lur_preds_pull(eval(parse(text=year_mod_devel))))
         
  # Combine air pollution values and predictors
  ifelse(test_dataset == 'pcws', 
         lur_data <- inner_join(pcws_ap, lur_preds, by=c("hhid_x")),
         lur_data <- inner_join(taps_ap, lur_preds, by=c("hhid_x")))
  
  # Drop HHID, HHIDX, and uncorrected columns
  lur_data <- lur_data %>%
    dplyr::select(hhid_x, everything()) %>%
    dplyr::select(-ends_with('unadj')) 
  
  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)][is.na(lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)])] <- 0
  
  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  lur_data <- lur_data %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  # Create a temporary dataset for analysis
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         lur_data_TEMP <- pcws_ap_no2_multi %>%
    inner_join(., pcws_lur_preds, by=c('styear','hhid_x')),
    lur_data_TEMP <- filter(lur_data, !is.na(eval(parse(text=pollutant_outcome)))))

  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))
  
  # Merge the reference data to the temporary dataset on year=styear, note that taps_ap doesn't have a 'styear' field since they're all the same, so we're band-aiding it here by an if statement for 2 styaer fields when merging pcws
  ifelse("styear.x" %in% names(lur_data_TEMP),
         lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear.x'='year')),
                                     lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear'='year')))
    
  # Update lur dataset levels with differences by difference b/w year of outcome to predict minus year of model devel.
  lur_data_TEMP <- lur_data_TEMP %>%
    mutate(no2_adj = no2_adj + (no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2])),
           pm25_adj = pm25_adj + (pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3])),
           pm10_adj = pm10_adj + (pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4])))
  
  # Evalute PCWS-based model on prediction potential with PCWS data
  mod_name <- paste(evalmodel_dataset, pollutant, "mod", sep="_")
  
  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  outcome_name <- paste0("lur_data_TEMP$",pollutant_outcome)

  # Create predicted values
  lur_data_TEMP$predicted <- predict(eval(parse(text=mod_name)), newdata=lur_data_TEMP)

  # Residuals
  lur_data_TEMP$residuals <- eval(parse(text=outcome_name)) - lur_data_TEMP$predicted
  
  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = lur_data_TEMP$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]

  # Map of residuals
  palette <- colorNumeric("RdYlBu", resids$resids, n = 5)

  leaflet(data = resids) %>% 
    addTiles() %>%
    addCircleMarkers(stroke = F, fillOpacity = 0.75,
                color = ~palette(resids),
                label = ~hhid_x,
                popup = paste(pollutant, "Model Resid:", format(round(resids$resids, 2), nsmall = 2), "<br>")) %>%
    leaflet::addLegend(pal = palette,
              values = ~resids,
              title = paste(pollutant, "Model Resids. (",pollutant_unit,")"),
              opacity = 0.75,
              position = 'topright') 
}

extval_method3_no2_multisens_diag <- function(year_mod_devel, evalmodel_dataset, test_dataset, pollutant, pollutant_unit){

  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  
  ifelse(test_dataset == 'pcws', 
         lur_preds <- pcws_lur_preds_pull(eval(parse(text=year_mod_devel))),
         lur_preds <- taps_lur_preds_pull(eval(parse(text=year_mod_devel))))
         
  # Combine air pollution values and predictors
  ifelse(test_dataset == 'pcws', 
         lur_data <- inner_join(pcws_ap, lur_preds, by=c("hhid_x")),
         lur_data <- inner_join(taps_ap, lur_preds, by=c("hhid_x")))
  
  # Drop HHID, HHIDX, and uncorrected columns
  lur_data <- lur_data %>%
    dplyr::select(hhid_x, everything()) %>%
    dplyr::select(-ends_with('unadj')) 

  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)][is.na(lur_data[, grep("distintvair1", colnames(lur_data)):ncol(lur_data)])] <- 0
  
  # NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
  lur_data <- lur_data %>%
    dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

  # Subset out only pollutant of interest
  lur_data_TEMP <- filter(lur_data, !is.na(eval(parse(text=pollutant_outcome))))
  
  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))
  
  # Merge the reference data to the temporary dataset on year=styear, note that taps_ap doesn't have a 'styear' field since they're all the same, so we're band-aiding it here by an if statement for 2 styaer fields when merging pcws
  ifelse("styear.x" %in% names(lur_data_TEMP),
         lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear.x'='year')),
                                     lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear'='year')))
    
  # Update lur dataset levels with differences by difference b/w year of outcome to predict minus year of model devel.
  lur_data_TEMP <- lur_data_TEMP %>%
    mutate(no2_adj = no2_adj + (no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2])),
           pm25_adj = pm25_adj + (pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3])),
           pm10_adj = pm10_adj + (pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4])))


  # # Evalute PCWS-based model on prediction potential with PCWS data
  # mod_name <- paste(evalmodel_dataset, pollutant, "mod", sep="_")
  
  print(summary(lm(pcws_no2_mod_multiobs, data=lur_data_TEMP)))
  print(kable(summary(lm(pcws_no2_mod_multiobs, data=lur_data_TEMP))$coef, digits=2))

  # Create predicted values
  lur_data_TEMP$predicted <- predict(pcws_no2_mod_multiobs, newdata=lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  outcome_name <- paste0("lur_data_TEMP$",pollutant_outcome)
  
  print(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name))))
  
  rootmse <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[1])
  r2 <- as.numeric(postResample(lur_data_TEMP$predicted, eval(parse(text=outcome_name)))[2])

  print(ks.test(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome)))))
  
  ksd <- as.numeric(ks.test(predict(pcws_no2_mod_multiobs), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))$statistic)
    
  # Create output df of predicted values
  predicted_df <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                          "predicted" = lur_data_TEMP$predicted) 
  
  colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method3_multisens_pred_",test_dataset))


  # Side by side plots of densities with x limits based on both datasets
  obsvd_plot <- ggplot(lur_data_TEMP, aes(x=eval(parse(text=outcome_name)))) +
  geom_histogram(fill='black', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  pred_plot <- ggplot(lur_data_TEMP, aes(x=predicted)) +
  geom_histogram(fill='red', alpha=0.5) +
  xlim(range(c(lur_data_TEMP$predicted, eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  print(ggarrange(obsvd_plot, pred_plot, 
          labels = c("Observed Values", "Predicted Values"),
          ncol = 1, nrow = 2))

  # Residuals
  lur_data_TEMP$residuals <- eval(parse(text=outcome_name)) - lur_data_TEMP$predicted
  
  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = lur_data_TEMP$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]
   
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,5,resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)
  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,3,resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,resids@data$resids, scatter.plot = T)
  l.m.I
  
  return(list(ksd=ksd, r2=r2, rootmse=rootmse, moransIval=moransIval, moransIpvalue=moransIpvalue, predicted_df=predicted_df))
}



# NO2
pcws_no2_extval_method3_diag <- extval_method3_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method3_residmap(year_mod_devel = 1989,
                    evalmodel_dataset = 'pcws',
                    test_dataset = 'taps',
                    pollutant = 'no2',
                    pollutant_unit = 'ppb',
                    predicting_pcws_no2_multisens = 'No')

pcws_no2_extval_method3_multisens_diag <- extval_method3_no2_multisens_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb')

# PM2.5
pcws_pm25_extval_method3_diag <- extval_method3_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'pm25',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method3_residmap(year_mod_devel = 1989,
                    evalmodel_dataset = 'pcws',
                    test_dataset = 'taps',
                    pollutant = 'pm25',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')


# PM10
pcws_pm10_extval_method3_diag <- extval_method3_diag(year_mod_devel = 1989,
                                                    evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'pm10',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method3_residmap(year_mod_devel = 1989,
                    evalmodel_dataset = 'pcws',
                    test_dataset = 'taps',
                    pollutant = 'pm10',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')

# Combine diagnostic outputs and identifiers
pcws_extval_method3_diag_df <- data.frame("pcws_no2_extval_method3" = unlist(pcws_no2_extval_method3_diag[c(1:5)]),
                                          "pcws_no2_extval_method3_multisens" = unlist(pcws_no2_extval_method3_multisens_diag[c(1:5)]),
                                          "pcws_pm25_extval_method3" = unlist(pcws_pm25_extval_method3_diag[c(1:5)]),
                                          "pcws_pm10_extval_method3" = unlist(pcws_pm10_extval_method3_diag[c(1:5)]))

# Combine outputs into a dataframe intended for paper-based table and format
pcws_extval_method3_diag_df_paper <- data.frame(t(pcws_extval_method3_diag_df)) 

pcws_extval_method3_diag_df_paper <- pcws_extval_method3_diag_df_paper%>%
   mutate("Model"= row.names(pcws_extval_method3_diag_df_paper))

# Combine predicted values from each model 
datalist <- list(unnest(pcws_no2_extval_method3_diag$predicted_df),
                                         unnest(pcws_no2_extval_method3_multisens_diag$predicted_df),
                                         unnest(pcws_pm25_extval_method3_diag$predicted_df),
                                         unnest(pcws_pm10_extval_method3_diag$predicted_df))

pcws_extval_method3_pred_taps_df <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all predictor csvs together
```



#### Method 4 - Account for changes in the predictor-air pollution level relationships - calibrate predictor coefficients to the prediction-year outcome data 

Refit a regression to TAPS data in 2015 using predictors ONLY (NO coeffs!) from PCWS models and get R2, etc

```{r Ext. Val. of PCWS Models - Method 4, echo=TRUE, message=FALSE, warning=FALSE}

extval_method4_diag <- function(evalmodel_dataset, test_dataset, pollutant, pollutant_unit, predicting_pcws_no2_multisens){
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  lur_data <- paste0(test_dataset, "_lur_data")
  
  # Create a temporary dataset for analysis
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         lur_data_TEMP <- pcws_ap_no2_multi %>%
    inner_join(., pcws_lur_preds, by=c('styear','hhid_x')) %>%
    dplyr::select(-styear),
    lur_data_TEMP <- filter(eval(parse(text=lur_data)), !is.na(eval(parse(text=pollutant_outcome)))))
  
  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))
  
  # Evalute PCWS-based model on prediction potential with PCWS data
  mod_name <- paste(evalmodel_dataset,pollutant,"mod",sep="_")
  
  # Extract model variables only
  mod_form <- as.formula(eval(parse(text=mod_name)))

  # Refit model variables with external data
  mod <- lm(mod_form, data = lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  print(summary(mod))
  
  r2 <- as.numeric(summary(mod)$r.squared)
  
  print(kable(summary(mod)$coef, digits=2))

  rootmse <- as.numeric(modelr::rmse(mod, data = lur_data_TEMP))
  
  print(rmse(mod, data = lur_data_TEMP))
  
  ksd <- as.numeric(ks.test(predict(eval(parse(text=mod_name))), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))$statistic)

  print(ks.test(predict(mod), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome)))))
  
  # Create output df of predicted values
  lur_data_TEMP$predicted <- predict.lm(mod, data = lur_data_TEMP)
  
  predicted_df <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                          "predicted" = lur_data_TEMP$predicted) 
  
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method4_pred_",test_dataset,"_multisens")), 
                                     colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method4_pred_",test_dataset)))
  
  # Side by side plots of densities with x limits based on both datasets
  obsvd_plot <- ggplot(lur_data_TEMP, aes(x=eval(parse(text=pollutant_outcome)))) +
  geom_histogram(fill='black', alpha=0.5) +
  xlim(range(c(predict(mod), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  pred_plot <- ggplot(lur_data_TEMP, aes(x=predict(mod))) +
  geom_histogram(fill='red', alpha=0.5) +
  xlim(range(c(predict(mod), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  print(ggarrange(obsvd_plot, pred_plot, 
          labels = c("Observed Values", "Predicted Values"),
          ncol = 1, nrow = 2))


  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = mod$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]

  
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,5,resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)
  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,3,resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,resids@data$resids, scatter.plot = T)
  l.m.I
  
  return(list(ksd=ksd, r2=r2, rootmse=rootmse, moransIval=moransIval, moransIpvalue=moransIpvalue, predicted_df=predicted_df))
}

extval_method4_residmap <- function(evalmodel_dataset, test_dataset, pollutant, pollutant_unit, predicting_pcws_no2_multisens){
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  lur_data <- paste0(test_dataset, "_lur_data")
  
  # Create a temporary dataset for analysis
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         lur_data_TEMP <- pcws_ap_no2_multi %>%
    inner_join(., pcws_lur_preds, by=c('styear','hhid_x')) %>%
    dplyr::select(-styear),
    lur_data_TEMP <- filter(eval(parse(text=lur_data)), !is.na(eval(parse(text=pollutant_outcome)))))
  
  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))
  
  # Evalute PCWS-based model on prediction potential with PCWS data
  mod_name <- paste(evalmodel_dataset,pollutant,"mod",sep="_")
  
  # Extract model variables only
  mod_form <- as.formula(eval(parse(text=mod_name)))

  # Refit model variables with external data
  mod <- lm(mod_form, data = lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  print(summary(mod))

  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = mod$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]

  # Map of residuals
  palette <- colorNumeric("RdYlBu", resids$resids, n = 5)

  leaflet(data = resids) %>% 
    addTiles() %>%
    addCircleMarkers(stroke = F, fillOpacity = 0.75,
                color = ~palette(resids),
                label = ~hhid_x,
                popup = paste(pollutant, "Model Resid:", format(round(resids$resids, 2), nsmall = 2), "<br>")) %>%
    leaflet::addLegend(pal = palette,
              values = ~resids,
              title = paste(pollutant, "Model Resids. (",pollutant_unit,")"),
              opacity = 0.75,
              position = 'topright')
  }

extval_method4_mod <- function(evalmodel_dataset, test_dataset, pollutant, pollutant_unit, predicting_pcws_no2_multisens){
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  lur_data <- paste0(test_dataset, "_lur_data")
  
  # Create a temporary dataset for analysis
  ifelse(all(predicting_pcws_no2_multisens=='Yes' & test_dataset=='pcws'), 
         lur_data_TEMP <- pcws_ap %>%
           filter(no2_adj_n > 1 & !is.na(no2_adj)) %>%
    inner_join(., pcws_lur_preds, by=c('styear','hhid_x')) %>%
    dplyr::select(-styear, -no2_adj_n, -pm25_adj_n, -pm10_adj_n),
    lur_data_TEMP <- filter(eval(parse(text=lur_data)), !is.na(eval(parse(text=pollutant_outcome)))))
  
  # Correct any predictor values (starting with the 6th column) that are NA to 0
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))
  
  # Evalute PCWS-based model on prediction potential with PCWS data
  mod_name <- paste(evalmodel_dataset,pollutant,"mod",sep="_")
  
  # Extract model variables only
  mod_form <- as.formula(eval(parse(text=mod_name)))

  # Refit model variables with external data
  mod <- lm(mod_form, data = lur_data_TEMP)
  
  mod

}

extval_method4_no2_multisens_diag <- function(evalmodel_dataset, test_dataset, pollutant, pollutant_unit){
  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a lur dataset for only said pollutant where only pollutant values not NA are there, then remove predictors with all 0s in that subset
  lur_data <- paste0(test_dataset, "_lur_data")
  
  lur_data_TEMP <- filter(eval(parse(text=lur_data)), !is.na(eval(parse(text=pollutant_outcome))))
  
  # Create address set to work on 
  ifelse(test_dataset=='pcws', addrs <- pcws_addrs_load(), addrs <- taps_addrs_load())
  
  # Add in missing predictors
  ifelse(!('lu_hr_1000' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_hr_1000 <- 0, print("Not missing lu_ahr_1000"))
  ifelse(!('lu_ag_100' %in% colnames(lur_data_TEMP)), lur_data_TEMP$lu_ag_100 <- 0, print("Not missing lu_ag_100"))
  ifelse(!('WindSpeedmph' %in% colnames(lur_data_TEMP)), lur_data_TEMP$WindSpeedmph <- 0, print("Not missing WindSpeedmph"))
  
  # # Evalute PCWS-based model on prediction potential with PCWS data
  # mod_name <- paste(evalmodel_dataset,pollutant,"mod",sep="_")
  
  # Extract model variables only
  mod_form <- as.formula(pcws_no2_mod_multiobs)

  # Refit model variables with external data
  mod <- lm(mod_form, data = lur_data_TEMP)

  # Calculate R^2, RMSE, Kolmogorov-Smirnov Test Stat
  print(summary(mod))
  
  r2 <- as.numeric(summary(mod)$r.squared)
  
  print(kable(summary(mod)$coef, digits=2))

  rootmse <- as.numeric(rmse(mod, data = lur_data_TEMP))
  
  print(rmse(mod, data = lur_data_TEMP))
  
  ksd <- as.numeric(ks.test(predict(pcws_no2_mod_multiobs), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))$statistic)

  print(ks.test(predict(mod), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome)))))
  
    # Create output df of predicted values
  lur_data_TEMP$predicted <- predict.lm(mod, data = lur_data_TEMP)
  
  predicted_df <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                          "predicted" = lur_data_TEMP$predicted) 
  
  colnames(predicted_df) <- c('hhid_x', paste0(evalmodel_dataset,"_",pollutant,"_extval_method4_multisens_pred_",test_dataset))
  
  # Side by side plots of densities with x limits based on both datasets
  obsvd_plot <- ggplot(lur_data_TEMP, aes(x=eval(parse(text=pollutant_outcome)))) +
  geom_histogram(fill='black', alpha=0.5) +
  xlim(range(c(predict(mod), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  pred_plot <- ggplot(lur_data_TEMP, aes(x=predict(mod))) +
  geom_histogram(fill='red', alpha=0.5) +
  xlim(range(c(predict(mod), eval(parse(text=paste0('lur_data_TEMP$',pollutant_outcome))))))
  
  print(ggarrange(obsvd_plot, pred_plot, 
          labels = c("Observed Values", "Predicted Values"),
          ncol = 1, nrow = 2))


  # Run the spatial autocorrelation diagnostics function
  resids <- data.frame("hhid_x" = lur_data_TEMP$hhid_x,
                           "resids" = mod$residuals)
  
  resids <- inner_join(addrs, resids, by = c("hhid_x")) %>%
    dplyr::select(hhid_x, resids)
  
  resids <- as(resids, "Spatial")
  
  resids <- spTransform(resids, CRS("+proj=longlat +datum=WGS84"))
  
  resids$long <- resids@coords[,1]
  resids$lat <- resids@coords[,2]

  
  # Global Moran's I with 5 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,5,resids@data$resids)
  print("Global Moran's I stats with 5 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  moransIval <- as.numeric(m.I$Morans.I)
  moransIpvalue <- as.numeric(m.I$p.value.randomization)
  
  # Local Moran's I with 5 nearest neighbors
  print("Local Moran's I with 5 nearest neighbors")
  l.m.I <- l.moransI(coords,5,resids@data$resids, scatter.plot = T)
  l.m.I
  
  # Global Moran's I with 3 nearest neighbors
  coords <- cbind(resids@data$long, resids@data$lat)
  m.I <- moransI(coords,3,resids@data$resids)
  print("Global Moran's I stats with 3 nearest neighbors")
  print(t(as.matrix(m.I[2:7])))
  
  # Local Moran's I with 3 nearest neighbors
  print("Local Moran's I with 3 nearest neighbors")
  l.m.I <- l.moransI(coords,3,resids@data$resids, scatter.plot = T)
  l.m.I
  
  return(list(ksd=ksd, r2=r2, rootmse=rootmse, moransIval=moransIval, moransIpvalue=moransIpvalue, predicted_df=predicted_df))
}


# NO2
pcws_no2_extval_method4_diag <- extval_method4_diag(evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method4_residmap(evalmodel_dataset = 'pcws',
                        test_dataset = 'taps',
                        pollutant = 'no2',
                        pollutant_unit = 'ppb',
                        predicting_pcws_no2_multisens = 'No')

pcws_no2_mod_extval_method4 <- extval_method4_mod(evalmodel_dataset = 'pcws',
                                                  test_dataset = 'taps',
                                                  pollutant = 'no2',
                                                  pollutant_unit = 'ppb',
                                                  predicting_pcws_no2_multisens = 'No')

pcws_no2_extval_method4_multisens_diag <- extval_method4_no2_multisens_diag(evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb')

# PM2.5
pcws_pm25_extval_method4_diag <- extval_method4_diag(evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'pm25',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method4_residmap(evalmodel_dataset = 'pcws',
                        test_dataset = 'taps',
                        pollutant = 'pm25',
                        pollutant_unit = 'ug/m3',
                        predicting_pcws_no2_multisens = 'No')

pcws_pm25_mod_extval_method4 <- extval_method4_mod(evalmodel_dataset = 'pcws',
                                                  test_dataset = 'taps',
                                                  pollutant = 'pm25',
                                                  pollutant_unit = 'ug/m3',
                                                  predicting_pcws_no2_multisens = 'No')

# PM10
pcws_pm10_extval_method4_diag <- extval_method4_diag(evalmodel_dataset = 'pcws',
                                                    test_dataset = 'taps',
                                                    pollutant = 'pm10',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method4_residmap(evalmodel_dataset = 'pcws',
                        test_dataset = 'taps',
                        pollutant = 'pm10',
                        pollutant_unit = 'ug/m3',
                        predicting_pcws_no2_multisens = 'No')

pcws_pm10_mod_extval_method4 <- extval_method4_mod(evalmodel_dataset = 'pcws',
                                                  test_dataset = 'taps',
                                                  pollutant = 'pm10',
                                                  pollutant_unit = 'ug/m3',
                                                  predicting_pcws_no2_multisens = 'No')

# Combine diagnostic outputs and identifiers
pcws_extval_method4_diag_df <- data.frame("pcws_no2_extval_method4" = unlist(pcws_no2_extval_method4_diag[c(1:5)]),
                                          "pcws_no2_extval_method4_multisens" = unlist(pcws_no2_extval_method4_multisens_diag[c(1:5)]),
                                          "pcws_pm25_extval_method4" = unlist(pcws_pm25_extval_method4_diag[c(1:5)]),
                                          "pcws_pm10_extval_method4" = unlist(pcws_pm10_extval_method4_diag[c(1:5)]))

# Combine outputs into a dataframe intended for paper-based table and format
pcws_extval_method4_diag_df_paper <- data.frame(t(pcws_extval_method4_diag_df)) 

pcws_extval_method4_diag_df_paper <- pcws_extval_method4_diag_df_paper%>%
   mutate("Model"= row.names(pcws_extval_method4_diag_df_paper))

# Combine predicted values from each model 
datalist <- list(unnest(pcws_no2_extval_method4_diag$predicted_df),
                                         unnest(pcws_no2_extval_method4_multisens_diag$predicted_df),
                                         unnest(pcws_pm25_extval_method4_diag$predicted_df),
                                         unnest(pcws_pm10_extval_method4_diag$predicted_df))

pcws_extval_method4_pred_taps_df <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all predictor csvs together
```

### Ext. Val. of TAPS Models by Predicting PCWS

#### Method 1 - Account for regional changes in air pollution levels - apply a temporal scalar or trend based on a continuously running background monitor

Scale PCWS values from respective years up to 2015 using an absolute difference adjustment NOT RATIO (explicit as per ESCAPE Exposure Manual pg 43 and not clarified in any other pubs) from continuously-running PDEQ monitor sites

Predict the scaled PCWS outcomes using the TAPS-based model 

```{r Ext. Val. of TAPS Models - Method 1, echo=TRUE, message=FALSE, warning=FALSE}

# NO2
taps_no2_extval_method1_diag <- extval_method1_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method1_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'no2',
                    pollutant_unit = 'ppb',
                    predicting_pcws_no2_multisens = 'No')

# NO2: >1 Obs homes only
taps_no2_extval_method1_multisens_diag <- extval_method1_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'Yes')

extval_method1_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'no2',
                    pollutant_unit = 'ppb',
                    predicting_pcws_no2_multisens = 'Yes')


# PM2.5
taps_pm25_extval_method1_diag <- extval_method1_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'pm25',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method1_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'pm25',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')


# PM10
taps_pm10_extval_method1_diag <- extval_method1_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'pm10',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method1_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'pm10',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')

# Combine diagnostic outputs and identifiers
taps_extval_method1_diag_df <- data.frame("taps_no2_extval_method1" = unlist(taps_no2_extval_method1_diag[c(1:5)]),
                                          "taps_no2_extval_method1_multisens" = unlist(taps_no2_extval_method1_multisens_diag[c(1:5)]),
                                          "taps_pm25_extval_method1" = unlist(taps_pm25_extval_method1_diag[c(1:5)]),
                                          "taps_pm10_extval_method1" = unlist(taps_pm10_extval_method1_diag[c(1:5)]))

# Combine outputs into a dataframe intended for paper-based table and format
taps_extval_method1_diag_df_paper <- data.frame(t(taps_extval_method1_diag_df)) 

taps_extval_method1_diag_df_paper <- taps_extval_method1_diag_df_paper%>%
   mutate("Model"= row.names(taps_extval_method1_diag_df_paper))

# Combine predicted values from each model 
datalist <- list(unnest(taps_no2_extval_method1_diag$predicted_df),
                 unnest(taps_no2_extval_method1_multisens_diag$predicted_df),
                 unnest(taps_pm25_extval_method1_diag$predicted_df),
                 unnest(taps_pm10_extval_method1_diag$predicted_df))

taps_extval_method1_pred_pcws_df <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all predictor csvs together

```


#### Method 2 - Adjust for changes in the location of predictors - include year-specific predictor data

Get GIS predictors for PCWS locations in 2015 and predict them to get R2 etc

```{r Ext. Val. of TAPS Models - Method 2, echo=TRUE, message=FALSE, warning=FALSE}

# NO2
taps_no2_extval_method2_diag <- extval_method2_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method2_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'no2',
                    pollutant_unit = 'ppb',
                    predicting_pcws_no2_multisens = 'No')

# NO2: > 1 obs only
taps_no2_extval_method2_multisens_diag <- extval_method2_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'Yes')

extval_method2_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'no2',
                    pollutant_unit = 'ppb',
                    predicting_pcws_no2_multisens = 'Yes')

# PM2.5
taps_pm25_extval_method2_diag <- extval_method2_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'pm25',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method2_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'pm25',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')


# PM10
taps_pm10_extval_method2_diag <- extval_method2_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'pm10',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method2_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'pm10',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')

# Combine diagnostic outputs and identifiers
taps_extval_method2_diag_df <- data.frame("taps_no2_extval_method2" = unlist(taps_no2_extval_method2_diag[c(1:5)]),
                                          "taps_no2_extval_method2_multisens" = unlist(taps_no2_extval_method2_multisens_diag[c(1:5)]),
                                          "taps_pm25_extval_method2" = unlist(taps_pm25_extval_method2_diag[c(1:5)]),
                                          "taps_pm10_extval_method2" = unlist(taps_pm10_extval_method2_diag[c(1:5)]))

# Combine outputs into a dataframe intended for paper-based table and format
taps_extval_method2_diag_df_paper <- data.frame(t(taps_extval_method2_diag_df)) 

taps_extval_method2_diag_df_paper <- taps_extval_method2_diag_df_paper%>%
   mutate("Model"= row.names(taps_extval_method2_diag_df_paper))

# Combine predicted values from each model 
datalist <- list(unnest(taps_no2_extval_method2_diag$predicted_df),
                 unnest(taps_no2_extval_method2_multisens_diag$predicted_df),
                                         unnest(taps_pm25_extval_method2_diag$predicted_df),
                                         unnest(taps_pm10_extval_method2_diag$predicted_df))

taps_extval_method2_pred_pcws_df <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all predictor csvs together
```


#### Method 3 - Combine Methods 1 + 2
```{r Ext. Val. of TAPS Models - Method 3, echo=TRUE, message=FALSE, warning=FALSE}

# NO2
taps_no2_extval_method3_diag <- extval_method3_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method3_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'no2',
                    pollutant_unit = 'ppb',
                    predicting_pcws_no2_multisens = 'No')

# NO2: >1 obs. only
taps_no2_extval_method3_multisens_diag <- extval_method3_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'Yes')

extval_method3_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'no2',
                    pollutant_unit = 'ppb',
                    predicting_pcws_no2_multisens = 'Yes')

# PM2.5
taps_pm25_extval_method3_diag <- extval_method3_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'pm25',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method3_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'pm25',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')


# PM10
taps_pm10_extval_method3_diag <- extval_method3_diag(year_mod_devel = 2015,
                                                    evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'pm10',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method3_residmap(year_mod_devel = 2015,
                    evalmodel_dataset = 'taps',
                    test_dataset = 'pcws',
                    pollutant = 'pm10',
                    pollutant_unit = 'ug/m3',
                    predicting_pcws_no2_multisens = 'No')

# Combine diagnostic outputs and identifiers
taps_extval_method3_diag_df <- data.frame("taps_no2_extval_method3" = unlist(taps_no2_extval_method3_diag[c(1:5)]),
                                          "taps_no2_extval_method3_multisens" = unlist(taps_no2_extval_method3_multisens_diag[c(1:5)]),
                                          "taps_pm25_extval_method3" = unlist(taps_pm25_extval_method3_diag[c(1:5)]),
                                          "taps_pm10_extval_method3" = unlist(taps_pm10_extval_method3_diag[c(1:5)]))

# Combine outputs into a dataframe intended for paper-based table and format
taps_extval_method3_diag_df_paper <- data.frame(t(taps_extval_method3_diag_df)) 

taps_extval_method3_diag_df_paper <- taps_extval_method3_diag_df_paper%>%
   mutate("Model"= row.names(taps_extval_method3_diag_df_paper))

# Combine predicted values from each model 
datalist <- list(unnest(taps_no2_extval_method3_diag$predicted_df),
                 unnest(taps_no2_extval_method3_multisens_diag$predicted_df),
                                         unnest(taps_pm25_extval_method3_diag$predicted_df),
                                         unnest(taps_pm10_extval_method3_diag$predicted_df))

taps_extval_method3_pred_pcws_df <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all predictor csvs together

```


#### Method 4 - Account for changes in the predictor-air pollution level relationships - calibrate predictor coefficients to the prediction-year outcome data 

Refit a regression to PCWS data in native measurement years (1987-1991) using predictors ONLY (NO coeffs!) from TAPS models and get R2, etc

```{r Ext. Val. of TAPS Models - Method 4, echo=TRUE, message=FALSE, warning=FALSE}
# NO2
taps_no2_extval_method4_diag <- extval_method4_diag(evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method4_residmap(evalmodel_dataset = 'taps',
                        test_dataset = 'pcws',
                        pollutant = 'no2',
                        pollutant_unit = 'ppb',
                        predicting_pcws_no2_multisens = 'No')

taps_no2_mod_extval_method4 <- extval_method4_mod(evalmodel_dataset = 'taps',
                                                  test_dataset = 'pcws',
                                                  pollutant = 'no2',
                                                  pollutant_unit = 'ppb',
                                                  predicting_pcws_no2_multisens = 'No')

# NO2: > 1 obs. only
taps_no2_extval_method4_multisens_diag <- extval_method4_diag(evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'no2',
                                                    pollutant_unit = 'ppb',
                                                    predicting_pcws_no2_multisens = 'Yes')

extval_method4_residmap(evalmodel_dataset = 'taps',
                        test_dataset = 'pcws',
                        pollutant = 'no2',
                        pollutant_unit = 'ppb',
                        predicting_pcws_no2_multisens = 'Yes')

taps_no2_mod_extval_method4_multisens <- extval_method4_mod(evalmodel_dataset = 'taps',
                                                  test_dataset = 'pcws',
                                                  pollutant = 'no2',
                                                  pollutant_unit = 'ppb',
                                                  predicting_pcws_no2_multisens = 'Yes')

# PM2.5
taps_pm25_extval_method4_diag <- extval_method4_diag(evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'pm25',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method4_residmap(evalmodel_dataset = 'taps',
                        test_dataset = 'pcws',
                        pollutant = 'pm25',
                        pollutant_unit = 'ug/m3',
                        predicting_pcws_no2_multisens = 'No')

taps_pm25_mod_extval_method4 <- extval_method4_mod(evalmodel_dataset = 'taps',
                                                  test_dataset = 'pcws',
                                                  pollutant = 'pm25',
                                                  pollutant_unit = 'ug/m3',
                                                  predicting_pcws_no2_multisens = 'No')

# PM10
taps_pm10_extval_method4_diag <- extval_method4_diag(evalmodel_dataset = 'taps',
                                                    test_dataset = 'pcws',
                                                    pollutant = 'pm10',
                                                    pollutant_unit = 'ug/m3',
                                                    predicting_pcws_no2_multisens = 'No')

extval_method4_residmap(evalmodel_dataset = 'taps',
                        test_dataset = 'pcws',
                        pollutant = 'pm10',
                        pollutant_unit = 'ug/m3',
                        predicting_pcws_no2_multisens = 'No')

taps_pm10_mod_extval_method4 <- extval_method4_mod(evalmodel_dataset = 'taps',
                                                  test_dataset = 'pcws',
                                                  pollutant = 'pm10',
                                                  pollutant_unit = 'ug/m3',
                                                  predicting_pcws_no2_multisens = 'No')

# Combine diagnostic outputs and identifiers
taps_extval_method4_diag_df <- data.frame("taps_no2_extval_method4" = unlist(taps_no2_extval_method4_diag[c(1:5)]),
                                          "taps_no2_extval_method4_multisens" = unlist(taps_no2_extval_method4_multisens_diag[c(1:5)]),
                                          "taps_pm25_extval_method4" = unlist(taps_pm25_extval_method4_diag[c(1:5)]),
                                          "taps_pm10_extval_method4" = unlist(taps_pm10_extval_method4_diag[c(1:5)]))

# Combine outputs into a dataframe intended for paper-based table and format
taps_extval_method4_diag_df_paper <- data.frame(t(taps_extval_method4_diag_df)) 

taps_extval_method4_diag_df_paper <- taps_extval_method4_diag_df_paper%>%
   mutate("Model"= row.names(taps_extval_method4_diag_df_paper))

# Combine predicted values from each model 
datalist <- list(unnest(taps_no2_extval_method4_diag$predicted_df),
                 unnest(taps_no2_extval_method4_multisens_diag$predicted_df),
                                         unnest(taps_pm25_extval_method4_diag$predicted_df),
                                         unnest(taps_pm10_extval_method4_diag$predicted_df))

taps_extval_method4_pred_pcws_df <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all predictor csvs together

```
### Ext. Val. Final Results Table 
```{r External Validation Model Results Table Creation, echo=TRUE, message=FALSE, warning=FALSE}

# Combine output files from all datasets, refinement methods, and pollutants
extval_diag_dfs_paper <- do.call("rbind", list(pcws_extval_method1_diag_df_paper,
                                               pcws_extval_method2_diag_df_paper,
                                               pcws_extval_method3_diag_df_paper,
                                               pcws_extval_method4_diag_df_paper,
                                               taps_extval_method1_diag_df_paper,
                                               taps_extval_method2_diag_df_paper,
                                               taps_extval_method3_diag_df_paper,
                                               taps_extval_method4_diag_df_paper)) %>%
  mutate(KS_D = formatC(ksd, format = "f", digits = 2, flag = "#"),
         Rsquared = 100 * r2,
         Rsquared = formatC(Rsquared, format = "fg", digits = 1),
         Rsquared = paste0(Rsquared,"%"),
         RMSE = formatC(rootmse, format = "fg", digits = 3, flag = "#"),
         Morans_I = formatC(moransIval, format = "f", digits = 2, flag = "#"),
         Morans_I_pvalue = formatC(moransIpvalue, format = "E", digits = 1, flag = "#"),
         Morans_I_values = paste0(Morans_I, " (", Morans_I_pvalue, ")")) %>%
  mutate(Pollutant = ifelse(grepl("no2", Model), "NO2",
                                          ifelse(grepl("pm25", Model), "PM2.5","PM10")),
         Refinement_Method = ifelse(grepl("method4", Model), "Method 4",
                                          ifelse(grepl("method3", Model), "Method 3",
                                          ifelse(grepl("method2", Model), "Method 2", "Method 1"))),
         Model_Dataset = ifelse(str_detect(Model, "taps") & str_detect(Model, "multisens"),
                                "TAPS-PCWS: Only >1 Obs.",
                                ifelse(str_detect(Model, "taps"), "TAPS",
                                       ifelse(str_detect(Model, "pcws") & str_detect(Model, "multisens"),
                                "PCWS: Only >1 Obs.",
                                ifelse(str_detect(Model, "pcws"), "PCWS",
                                "CHECK MODEL NAMES"))))) %>%
  dplyr::select(Pollutant, Refinement_Method, Model_Dataset, KS_D, Rsquared, RMSE, Morans_I_values) %>%
    arrange(Pollutant, Refinement_Method)

# Create tables from df
extval_diag_dfs_paper_no2 <- filter(extval_diag_dfs_paper, Pollutant=='NO2') %>%
  dplyr::select(-Pollutant)

extval_diag_dfs_paper_pm25 <- filter(extval_diag_dfs_paper, Pollutant=='PM2.5') %>%
  dplyr::select(-Pollutant)

extval_diag_dfs_paper_pm10 <- filter(extval_diag_dfs_paper, Pollutant=='PM10') %>%
  dplyr::select(-Pollutant)

formattable(extval_diag_dfs_paper_no2)
formattable(extval_diag_dfs_paper_pm25)
formattable(extval_diag_dfs_paper_pm10)
```

```{r External Validation Model Results Bar Chart Creation, echo=TRUE, message=FALSE, warning=FALSE}

# Combine output files from all datasets, refinement methods, and pollutants
extval_diag_dfs_ppt <- do.call("rbind", list(pcws_extval_method1_diag_df_paper,
                                               pcws_extval_method2_diag_df_paper,
                                               pcws_extval_method3_diag_df_paper,
                                               pcws_extval_method4_diag_df_paper,
                                               taps_extval_method1_diag_df_paper,
                                               taps_extval_method2_diag_df_paper,
                                               taps_extval_method3_diag_df_paper,
                                               taps_extval_method4_diag_df_paper)) %>%
  mutate(R2_of_Model = 100 * r2,
         R2_of_Model = as.numeric(formatC(R2_of_Model, format = "fg", digits = 1)),
         RMSE = as.numeric(formatC(rootmse, format = "fg", digits = 3, flag = "#")),
         Morans_I_pvalue = as.numeric(formatC(moransIpvalue, format = "E", digits = 1, flag = "#"))) %>%
  mutate(Pollutant = ifelse(grepl("no2", Model), "NO2",
                                          ifelse(grepl("pm25", Model), "PM2.5","PM10")),
         Refinement_Method = ifelse(grepl("method4", Model), "Method 4",
                                          ifelse(grepl("method3", Model), "Method 3",
                                          ifelse(grepl("method2", Model), "Method 2", "Method 1"))),
         Model_Dataset = ifelse(str_detect(Model, "taps") & str_detect(Model, "multisens"),
                                "TAPS-PCWS: Only >1 Obs.",
                                ifelse(str_detect(Model, "taps"), "TAPS",
                                       ifelse(str_detect(Model, "pcws") & str_detect(Model, "multisens"),
                                "PCWS: Only >1 Obs.",
                                ifelse(str_detect(Model, "pcws"), "PCWS",
                                "CHECK MODEL NAMES"))))) %>%
  dplyr::select(Pollutant, Refinement_Method, Model_Dataset, R2_of_Model, RMSE, Morans_I_pvalue) %>%
  # filter(Refinement_Method == "Method 3") %>%  
  # filter(Refinement_Method != "Method 4") %>%
  # filter(Refinement_Method != "Method 2") %>%
  filter(Model_Dataset != "TAPS-PCWS: Only >1 Obs.") %>%
  filter(Model_Dataset != "PCWS: Only >1 Obs.") %>%
  group_by(Pollutant, Model_Dataset) %>%
  # filter(Model_Dataset == "PCWS") %>%
  filter((Pollutant == "NO2" & Refinement_Method == "Method 3") |
           (Pollutant == "PM2.5"& Refinement_Method == "Method 3") |
           (Pollutant == "PM10"& Refinement_Method == "Method 1")) %>%
  filter(R2_of_Model == max(R2_of_Model))

# Create graphs from dfs for ppt
extval_diag_dfs_ppt_graph <- ggplot(extval_diag_dfs_ppt,aes(x=Pollutant, y=as.numeric(R2_of_Model), fill=Model_Dataset)) +
  geom_bar(stat="identity", position=position_dodge2(width=0.9, preserve = 'single'),geom_params = list(width=.5)) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 10)) + scale_fill_brewer(palette="Reds") + 
  labs(fill="Model Dataset") +
  xlab("Pollutant") +
  ylab(expression(paste("Model ",R^2, " (%)"))) + 
  scale_x_discrete(labels=c("NO2" = expression(NO[2]),
                            "NOx" = expression(NO[x]),
                            "PM2.5" = expression(PM[2.5]),
                            "PM10" = expression(PM[10]))) +
  theme_set(theme_bw()) +
  theme(axis.text=element_text(size=12),
        axis.title.x=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        legend.title=element_text(size=14,face="bold"),
        legend.text=element_text(size=12,face="bold"), 
        legend.position="top")


jpeg("lur_external_results_dfs_ppt.jpeg", width = 1500, height = 1500, res = 300)
extval_diag_dfs_ppt_graph
print(extval_diag_dfs_ppt_graph)
dev.off()
```


Beamer Lab Meeting on 7/23/19 - after discussion, the best fitting externally-validated model for our purposes for NO2: PCWS - Method 4; PM2.5: TAPS - Method 1; PM10: PCWS - Method 4.


While the Method 4 approaches produce a good fitting model and is interesting to see how it performs much better than other methods of updating models to predict through time, it is not possible to use this approach to estimate outcomes for TCRS participants. As such, we will use the next best fitting model with the lowest K-S D, highest R-squared, lowest RMSE, and least significant Moran's I p-value.




```{r Pull Best Model Predictors, echo=FALSE, message=FALSE, warning=FALSE}
# Pull models predictors for use later in health effects testing

no2_mod_preds <- tidy(pcws_no2_mod) %>%
  filter(term != "(Intercept)")

no2_mod_preds <- no2_mod_preds$term

nox_mod_preds <- tidy(taps_nox_mod) %>%
  filter(term != "(Intercept)")

nox_mod_preds <- nox_mod_preds$term

pm25_mod_preds <- tidy(pcws_pm25_mod) %>%
  filter(term != "(Intercept)")

pm25_mod_preds <- pm25_mod_preds$term

pm10_mod_preds <- tidy(pcws_pm10_mod) %>%
  filter(term != "(Intercept)")

pm10_mod_preds <- pm10_mod_preds$term

all_mod_preds <- unique(c(no2_mod_preds, nox_mod_preds, pm25_mod_preds, pm10_mod_preds))

```

## Comparison of Predicted Values for Internal and External Validation
```{r Comparison of Predicted Values - PCWS, echo=TRUE, message=FALSE, warning=FALSE}

# Combine predicted outcomes from all models predicting PCWS values
datalist <- list(pcws_no2_mod_predicted,
                 pcws_no2_multiobs_mod_predicted,
                 pcws_pm25_mod_predicted,
                 pcws_pm10_mod_predicted,
                 taps_extval_method1_pred_pcws_df,
                 taps_extval_method2_pred_pcws_df,
                 taps_extval_method3_pred_pcws_df,
                 taps_extval_method4_pred_pcws_df)

pcws_predictions <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all prediction csvs together


# Select pollutant subset - NO2
pcws_predictions_no2 <- dplyr::select(pcws_predictions, contains("no2"))

colnames(pcws_predictions_no2) <- c("PCWS", "PCWS_Multi", "TAPS_M1", "TAPS_M1_Multi",
                  "TAPS_M2", "TAPS_M2_Multi",
                  "TAPS_M3", "TAPS_M3_Multi",
                  "TAPS_M4", "TAPS_M4_Multi")


# Select pollutant subset - PM25
pcws_predictions_pm25 <- dplyr::select(pcws_predictions, contains("pm25"))

colnames(pcws_predictions_pm25) <- c("PCWS", "TAPS_M1", "TAPS_M2", "TAPS_M3", "TAPS_M4")


# Select pollutant subset - PM10
pcws_predictions_pm10 <- dplyr::select(pcws_predictions, contains("pm10"))

colnames(pcws_predictions_pm10) <- c("PCWS", "TAPS_M1", "TAPS_M2", "TAPS_M3", "TAPS_M4")


# Pollutant prediction correlations
chart.Correlation(pcws_predictions_no2, histogram=TRUE, method= c("spearman"), pch=19)
chart.Correlation(pcws_predictions_pm25, histogram=TRUE, method= c("spearman"), pch=19)
chart.Correlation(pcws_predictions_pm10, histogram=TRUE, method= c("spearman"), pch=19)

# Correlation values

# Function to flatten correlation matrices into a table
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

pcws_no2_corr <- rcorr(as.matrix(pcws_predictions_no2), type=c("spearman"))
pcws_pm25_corr <- rcorr(as.matrix(pcws_predictions_pm25), type=c("spearman"))
pcws_pm10_corr <- rcorr(as.matrix(pcws_predictions_pm10), type=c("spearman"))

formattable(flattenCorrMatrix(pcws_no2_corr$r, pcws_no2_corr$P))
formattable(flattenCorrMatrix(pcws_pm25_corr$r, pcws_pm25_corr$P))
formattable(flattenCorrMatrix(pcws_pm10_corr$r, pcws_pm10_corr$P))


# # ON HOLD - NOT SURE IF WE'RE DOING THIS YET #

# 
# pcws_no2_corr <- rcorr(as.matrix(pcws_predictions_no2), type=c("spearman"))
# flattenCorrMatrix(pcws_no2_corr$r, pcws_no2_corr$P)
# 
# pcws_no2_corr_paper <- as.data.frame(flattenCorrMatrix(pcws_no2_corr$r, pcws_no2_corr$P)) %>%
#   mutate(Model_row = as.character(row),
#          Model_col = as.character(column),
#          Refinement_Method_row = ifelse(grepl("method4", Model_row), "Method 4",
#                                           ifelse(grepl("method3", Model_row), "Method 3",
#                                           ifelse(grepl("method2", Model_row), "Method 2",
#                                           ifelse(grepl("method1", Model_row), "Method 1", "None")))),
#          Model_Dataset_row = ifelse(str_detect(Model_row, "^taps") & str_detect(Model_row, "multisens"),
#                                 "TAPS-PCWS: Only >1 Obs.",
#                                 ifelse(str_detect(Model_row, "^taps"), "TAPS",
#                                        ifelse(str_detect(Model_row, "^pcws") & str_detect(Model_row, "multiobs"),
#                                 "PCWS: Only >1 Obs.",
#                                 ifelse(str_detect(Model_row, "^pcws"), "PCWS",
#                                 "CHECK MODEL NAMES")))),
#          Rho = formatC(cor, format = "f", digits = 2, flag = "#"),
#          Rho_pvalue = formatC(p, format = "E", digits = 1, flag = "#"),
#          Rho_values = paste0(Rho, " (", Rho_pvalue, ")"))



```

```{r Comparison of Predicted Values - TAPS, echo=TRUE, message=FALSE, warning=FALSE}

# Combine predicted outcomes from all models predicting TAPS values
datalist <- list(taps_no2_mod_predicted,
                 taps_nox_mod_predicted,
                 taps_pm25_mod_predicted,
                 taps_pm10_mod_predicted,
                 pcws_extval_method1_pred_taps_df,
                 pcws_extval_method2_pred_taps_df,
                 pcws_extval_method3_pred_taps_df,
                 pcws_extval_method4_pred_taps_df)

taps_predictions <- datalist %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)  # merge all prediction csvs together


# Select pollutant subset - NO2
taps_predictions_no2 <- dplyr::select(taps_predictions, contains("no2"))

colnames(taps_predictions_no2) <- c("TAPS", "PCWS_M1", "PCWS_M1_Multi",
                  "PCWS_M2", "PCWS_M2_Multi",
                  "PCWS_M3", "PCWS_M3_Multi",
                  "PCWS_M4", "PCWS_M4_Multi")


# Select pollutant subset - PM25
taps_predictions_pm25 <- dplyr::select(taps_predictions, contains("pm25"))

colnames(taps_predictions_pm25) <- c("TAPS", "PCWS_M1", "PCWS_M2", "PCWS_M3", "PCWS_M4")


# Select pollutant subset - PM10
taps_predictions_pm10 <- dplyr::select(taps_predictions, contains("pm10"))

colnames(taps_predictions_pm10) <- c("TAPS", "PCWS_M1", "PCWS_M2", "PCWS_M3", "PCWS_M4")


# Pollutant prediction correlations
chart.Correlation(taps_predictions_no2, histogram=TRUE, method= c("spearman"), pch=19)
chart.Correlation(taps_predictions_pm25, histogram=TRUE, method= c("spearman"), pch=19)
chart.Correlation(taps_predictions_pm10, histogram=TRUE, method= c("spearman"), pch=19)

taps_no2_corr <- rcorr(as.matrix(taps_predictions_no2), type=c("spearman"))
taps_pm25_corr <- rcorr(as.matrix(taps_predictions_pm25), type=c("spearman"))
taps_pm10_corr <- rcorr(as.matrix(taps_predictions_pm10), type=c("spearman"))

formattable(flattenCorrMatrix(taps_no2_corr$r, taps_no2_corr$P))
formattable(flattenCorrMatrix(taps_pm25_corr$r, taps_pm25_corr$P))
formattable(flattenCorrMatrix(taps_pm10_corr$r, taps_pm10_corr$P))

```

## Summary Statistics for Paper

```{r Summary Statistics of Annual Air Pollution Outcomes by Year for PCWS, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

# NO2 Summary by Year
pcws_no2_sum_annualavgs<- inner_join(pcws_ap, pcws_addrs,by=c("hhid_x")) %>%
  filter(!is.na(no2_adj)) %>%
  dplyr::group_by(styear) %>%
  summarise(n_obs = sum(no2_adj_n),
            n_homes = n_distinct(hhid_x),
            geo_mean = exp(mean(log(no2_adj))),
            geo_sd = exp(sd(log(no2_adj))),
            minimum = min(no2_adj),
            maximum = max(no2_adj)) %>%
  mutate(Year = styear)

pcws_no2_sum_annualavgs_all<- inner_join(pcws_ap, pcws_addrs,by=c("hhid_x")) %>%
  filter(!is.na(no2_adj)) %>%
  summarise(n_obs = sum(no2_adj_n),
            n_homes = n_distinct(hhid_x),            
            geo_mean = exp(mean(log(no2_adj))),
            geo_sd = exp(sd(log(no2_adj))),
            minimum = min(no2_adj),
            maximum = max(no2_adj)) %>%
  mutate(Year = NA)

pcws_no2_sum_annualavgs <- bind_rows(pcws_no2_sum_annualavgs, pcws_no2_sum_annualavgs_all) %>%
  mutate(N_obs_homes = paste0(n_homes, " (", n_obs, ")"),
         geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(Year, N_obs_homes, Geo_mean_sd, Range)

formattable(pcws_no2_sum_annualavgs)

# NO2 Summary for > 1 obs. subset

# pcws_ap_no2_multi <- data %>%
#   filter(!is.na(no2_adj)) %>%
#   dplyr::group_by(hhid,styear) %>%
#   mutate(no2_byseason3 = n_distinct(stssn3)) %>%
#   filter(no2_byseason3>1) %>%
#   mutate(hhid_x=paste0(hhid,"_A")) %>%
#   dplyr::ungroup() %>%
#   dplyr::select(-hhid) %>%
#   group_by(hhid_x, styear) %>%
#   filter(no2_byseason3==max(no2_byseason3)) %>%
#   filter(styear==min(styear)) %>%
#   summarise(no2_adj=mean(no2_adj, na.rm=T),
#             pm25_adj=mean(pm25_adj, na.rm=T),
#             pm10_adj=mean(pm10_adj, na.rm=T),
#             no2_adj_n = mean(no2_byseason3)) %>%
#   dplyr::ungroup()

pcws_no2_sum_annualavgs_multi<- pcws_ap_no2_multi %>%
  filter(!is.na(no2_adj)) %>%
  dplyr::group_by(styear) %>%
  summarise(
            geo_mean = exp(mean(log(no2_adj))),
            geo_sd = exp(sd(log(no2_adj))),
            minimum = min(no2_adj),
            maximum = max(no2_adj)) %>%
  mutate(Year = styear)

pcws_no2_sum_annualavgs_all_multi<- pcws_ap_no2_multi %>%
  filter(!is.na(no2_adj)) %>%
  summarise(
            geo_mean = exp(mean(log(no2_adj))),
            geo_sd = exp(sd(log(no2_adj))),
            minimum = min(no2_adj),
            maximum = max(no2_adj)) %>%
  mutate(Year = NA)

pcws_no2_sum_annualavgs_multi <- bind_rows(pcws_no2_sum_annualavgs_multi, pcws_no2_sum_annualavgs_all_multi) %>%
  mutate(geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(Year, Geo_mean_sd, Range)

formattable(pcws_no2_sum_annualavgs_multi)


# PM2.5 Summary by Year

# NOTE All observations prior to having background monitor running are dropped, eg nothing before 1989 is included
pcws_pm25_sum_annualavgs<- inner_join(pcws_ap, pcws_addrs,by=c("hhid_x")) %>%
  filter(!is.na(pm25_adj)) %>%
  dplyr::group_by(styear) %>%
  summarise(n_obs = sum(pm25_adj_n),
            n_homes = n_distinct(hhid_x),
            geo_mean = exp(mean(log(pm25_adj))),
            geo_sd = exp(sd(log(pm25_adj))),
            minimum = min(pm25_adj),
            maximum = max(pm25_adj)) %>%
  mutate(Year = styear)

pcws_pm25_sum_annualavgs_all<- inner_join(pcws_ap, pcws_addrs,by=c("hhid_x")) %>%
  filter(!is.na(pm25_adj)) %>%
  summarise(n_obs = sum(pm25_adj_n),
            n_homes = n_distinct(hhid_x),
            geo_mean = exp(mean(log(pm25_adj))),
            geo_sd = exp(sd(log(pm25_adj))),
            minimum = min(pm25_adj),
            maximum = max(pm25_adj)) %>%
  mutate(Year = NA)

pcws_pm25_sum_annualavgs <- bind_rows(pcws_pm25_sum_annualavgs, pcws_pm25_sum_annualavgs_all) %>%
  mutate(N_obs_homes = paste0(n_homes, " (", n_obs, ")"),
         geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(Year, N_obs_homes, Geo_mean_sd, Range)

formattable(pcws_pm25_sum_annualavgs)

# PM10 Summary by Year
pcws_pm10_sum_annualavgs<- inner_join(pcws_ap, pcws_addrs,by=c("hhid_x")) %>%
  filter(!is.na(pm10_adj)) %>%
  dplyr::group_by(styear) %>%
  summarise(n_obs = sum(pm10_adj_n),
            n_homes = n_distinct(hhid_x),
            geo_mean = exp(mean(log(pm10_adj))),
            geo_sd = exp(sd(log(pm10_adj))),
            minimum = min(pm10_adj),
            maximum = max(pm10_adj)) %>%
  mutate(Year = styear)

pcws_pm10_sum_annualavgs_all<- inner_join(pcws_ap, pcws_addrs,by=c("hhid_x")) %>%
  filter(!is.na(pm10_adj)) %>%
  summarise(n_obs = sum(pm10_adj_n),
            n_homes = n_distinct(hhid_x),
            geo_mean = exp(mean(log(pm10_adj))),
            geo_sd = exp(sd(log(pm10_adj))),
            minimum = min(pm10_adj),
            maximum = max(pm10_adj)) %>%
  mutate(Year = NA)

pcws_pm10_sum_annualavgs <- bind_rows(pcws_pm10_sum_annualavgs, pcws_pm10_sum_annualavgs_all) %>%
  mutate(N_obs_homes = paste0(n_homes, " (", n_obs, ")"),
         geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(Year, N_obs_homes, Geo_mean_sd, Range)

formattable(pcws_pm10_sum_annualavgs)


no2 <- ggplot(data = pcws_ap, aes(y=no2_adj)) +
  geom_boxplot() +
  labs(y=expression(paste(NO[2]," Conc. (ppb)"))) +
  theme_bw() +
  theme(axis.text.y=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        axis.text.x = element_blank()) +
  scale_y_continuous(limits=c(0,60), breaks=seq(0,60,by=10))

jpeg("pcws_no2_avg.jpeg", width = 600, height = 1000, res = 300)
no2
print(no2)
dev.off()


pm25 <- ggplot(data = pcws_ap, aes(y=pm25_adj)) +
  geom_boxplot() +
  labs(y=expression(paste(PM[2.5]," Conc. (", mu, g/m^3,")"))) +
  theme_bw() +
  theme(axis.text.y=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        axis.text.x = element_blank()) +
  scale_y_continuous(limits=c(0,50), breaks=seq(0,50,by=10))

jpeg("pcws_pm25_avg.jpeg", width = 600, height = 1000, res = 300)
pm25
print(pm25)
dev.off()


pm10 <- ggplot(data = pcws_ap, aes(y=pm10_adj)) +
  geom_boxplot() +
  labs(y=expression(paste(PM[10]," Conc. (", mu, g/m^3,")"))) +
  theme_bw() +
  theme(axis.text.y=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        axis.text.x = element_blank()) +
  scale_y_continuous(limits=c(0,70), breaks=seq(0,70,by=10))

jpeg("pcws_pm10_avg.jpeg", width = 600, height = 1000, res = 300)
pm10
print(pm10)
dev.off()

```

```{r Summary Statistics  of Annual Air Pollution Outcomes for TAPS, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}


# NOTE - because it doesn't make sense to split taps annual avgs into different years, this column is excluded completely

# NO2 Summary
taps_no2_sum_annualavgs<- taps_ap %>%
  filter(!is.na(no2_adj)) %>%
  summarise(n_homes = n_distinct(hhid_x),
            geo_mean = exp(mean(log(no2_adj))),
            geo_sd = exp(sd(log(no2_adj))),
            minimum = min(no2_adj),
            maximum = max(no2_adj)) %>%
  mutate(geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(n_homes, Geo_mean_sd, Range)

formattable(taps_no2_sum_annualavgs)

# NOx Summary
taps_nox_sum_annualavgs<- taps_ap %>%
  filter(!is.na(nox_adj)) %>%
  summarise(n_homes = n_distinct(hhid_x),
            geo_mean = exp(mean(log(nox_adj))),
            geo_sd = exp(sd(log(nox_adj))),
            minimum = min(nox_adj),
            maximum = max(nox_adj)) %>%
  mutate(geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(n_homes, Geo_mean_sd, Range)

formattable(taps_nox_sum_annualavgs)


# PM2.5 Summary

# NOTE - the -1.00 value is replaced here by lowest detected level (ie a pseudo LOD)/sqrt(2)
taps_pm25_sum_annualavgs_1 <- taps_ap %>%
  filter(!is.na(pm25_adj)) %>%
  summarise(n_homes = n_distinct(hhid_x),
            minimum = min(pm25_adj),
            maximum = max(pm25_adj)) 

taps_pm25_sum_annualavgs_2 <- taps_ap %>%
  filter(!is.na(pm25_adj)) %>%
  mutate(pm25_adj = ifelse(pm25_adj<0, (1.68/sqrt(2)), pm25_adj)) %>%
  summarise(geo_mean = exp(mean(log(pm25_adj))),
            geo_sd = exp(sd(log(pm25_adj))))

taps_pm25_sum_annualavgs <- data.frame(taps_pm25_sum_annualavgs_1, taps_pm25_sum_annualavgs_2) %>%
  mutate(geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(n_homes, Geo_mean_sd, Range)

formattable(taps_pm25_sum_annualavgs)

# PM10 Summary
taps_pm10_sum_annualavgs<- taps_ap %>%
  filter(!is.na(pm10_adj)) %>%
  summarise(n_homes = n_distinct(hhid_x),
            geo_mean = exp(mean(log(pm10_adj))),
            geo_sd = exp(sd(log(pm10_adj))),
            minimum = min(pm10_adj),
            maximum = max(pm10_adj)) %>%
  mutate(geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(n_homes, Geo_mean_sd, Range)

formattable(taps_pm10_sum_annualavgs)

no2 <- ggplot(data = taps_ap, aes(y=no2_adj)) +
  geom_boxplot() +
  labs(y=expression(paste(NO[2]," Conc. (ppb)"))) +
  theme_bw() +
  theme(axis.text.y=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        axis.text.x = element_blank()) +
  scale_y_continuous(limits=c(0,60), breaks=seq(0,60,by=10))

jpeg("taps_no2_avg.jpeg", width = 600, height = 1000, res = 300)
no2
print(no2)
dev.off()

nox <- ggplot(data = taps_ap, aes(y=nox_adj)) +
  geom_boxplot() +
  labs(y=expression(paste(NO[x]," Conc. (ppb)"))) +
  theme_bw() +
  theme(axis.text.y=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        axis.text.x = element_blank()) +
  scale_y_continuous(limits=c(0,60), breaks=seq(0,60,by=10))

jpeg("taps_nox_avg.jpeg", width = 600, height = 1000, res = 300)
no2
print(nox)
dev.off()

pm25 <- ggplot(data = taps_ap, aes(y=pm25_adj)) +
  geom_boxplot() +
  labs(y=expression(paste(PM[2.5]," Conc. (", mu, g/m^3,")"))) +
  theme_bw() +
  theme(axis.text.y=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        axis.text.x = element_blank()) +
  scale_y_continuous(limits=c(0,50), breaks=seq(0,50,by=10))

jpeg("taps_pm25_avg.jpeg", width = 600, height = 1000, res = 300)
pm25
print(pm25)
dev.off()


pm10 <- ggplot(data = taps_ap, aes(y=pm10_adj)) +
  geom_boxplot() +
  labs(y=expression(paste(PM[10]," Conc. (", mu, g/m^3,")"))) +
  theme_bw() +
  theme(axis.text.y=element_text(size=12,face="bold"),
        axis.title.y=element_text(size=12,face="bold"),
        axis.text.x = element_blank()) +
  scale_y_continuous(limits=c(0,70), breaks=seq(0,70,by=10))

jpeg("taps_pm10_avg.jpeg", width = 600, height = 1000, res = 300)
pm10
print(pm10)
dev.off()


```



```{r Summary Statistics  of Annual Air Pollution Outcomes for TAPS by Site Type, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}


sitetype <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/AddressSiteType.csv") %>%
  mutate(hhid_x = paste(HHID,HHIDX,sep="_"))

taps_ap_sitetype <- left_join(taps_ap, sitetype, by=c("hhid_x"))

taps_ap_sitetype$Site <- factor(taps_ap_sitetype$Site,
                                levels = c(1,2,3),
                                labels = c("Regional","Urban","Traffic"))
# No2/x ratio
taps_all_data_no2xratios <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/Results/TAPSData_AllObs.csv", sep=",") %>%
  mutate(hhid_x=HHID_X,
         no2x_ratio = no2/nox) %>%
  inner_join(.,taps_ap_sitetype)

summary(lm(no2~nox, filter(taps_all_data_no2xratios,Site=="Regional")))
summary(lm(no2~nox, filter(taps_all_data_no2xratios,Site=="Urban")))
summary(lm(no2~nox, filter(taps_all_data_no2xratios,Site=="Regional")))
summary(lm(no2~nox, taps_all_data_no2xratios))


taps_nox_ratios_table_regional <- taps_all_data_no2xratios %>%
  filter(!is.na(no2x_ratio)) %>%
  filter(Site=="Regional") %>%
  summarise(geo_mean = exp(mean(log(no2x_ratio))),
            geo_sd = exp(sd(log(no2x_ratio))),
            minimum = min(no2x_ratio),
            maximum = max(no2x_ratio))

taps_nox_ratios_table_urban <- taps_all_data_no2xratios %>%
  filter(!is.na(no2x_ratio)) %>%
  filter(Site=="Urban") %>%
  summarise(geo_mean = exp(mean(log(no2x_ratio))),
            geo_sd = exp(sd(log(no2x_ratio))),
            minimum = min(no2x_ratio),
            maximum = max(no2x_ratio))

taps_nox_ratios_table_traffic <- taps_all_data_no2xratios %>%
  filter(!is.na(no2x_ratio)) %>%
  filter(Site=="Traffic") %>%
  summarise(geo_mean = exp(mean(log(no2x_ratio))),
            geo_sd = exp(sd(log(no2x_ratio))),
            minimum = min(no2x_ratio),
            maximum = max(no2x_ratio))

formattable(bind_rows(taps_nox_ratios_table_regional,
                      taps_nox_ratios_table_urban,
                      taps_nox_ratios_table_traffic) %>%
  mutate(geo_mean = formatC(geo_mean, format = "fg", digits = 2, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 2, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(Geo_mean_sd, Range))


# Ratio of site measures
taps_all_data_no2xsiteratios <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/Results/TAPSData_AllObs.csv", sep=",") %>%
  mutate(hhid_x=HHID_X) %>%
  inner_join(.,taps_ap_sitetype) %>%
  group_by(Site) %>%
  summarise(no2_sum = mean(no2, na.rm=T),
            nox_sum = mean(nox, na.rm=T),
            pm25_sum = mean(pm25, na.rm=T),
            pm10_sum = mean(pm10, na.rm=T))
  
  
taps_all_data_no2xsiteratios

# Done manually - no2
# reg/urb
3.91/5.05

# str/urb
6.66/5.05

 
# Done manually - nox
# reg/urb
6.87/9.54

# str/urb
11.9/5.05 

taps_all_data_no2xsiteratios

# Done manually - pm25
# reg/urb
3.52/4.32

# str/urb
4.75/4.32

# Done manually - pm25
# reg/urb
18.6/20.7

# str/urb
19.8/20.7


taps_all_data <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/Results/TAPSData_AllObs.csv", sep=",") %>%
  mutate(hhid_x=HHID_X) %>%
  group_by(hhid_x) %>%
  summarise(no2_adj_n = n_distinct(no2,na.rm=T),
         nox_adj_n = n_distinct(nox,na.rm=T),
         pm25_adj_n = n_distinct(pm25,na.rm=T),
         pm10_adj_n = n_distinct(pm10,na.rm=T))

taps_ap_sitetype <- left_join(taps_ap_sitetype, taps_all_data, by=c("hhid_x"))



# NO2 Summary
taps_no2_sum_annualavgs<- taps_ap_sitetype %>%
  filter(!is.na(no2_adj)) %>%
  group_by(Site) %>%
  summarise(n_homes = n_distinct(hhid_x),
            n_obs = sum(no2_adj_n),
            geo_mean = exp(mean(log(no2_adj))),
            geo_sd = exp(sd(log(no2_adj))),
            minimum = min(no2_adj),
            maximum = max(no2_adj)) %>%
  mutate(N_obs_homes = paste0(n_homes, " (", n_obs, ")"),
         geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(Site, N_obs_homes, Geo_mean_sd, Range)

taps_nox_sum_annualavgs<- taps_ap_sitetype %>%
  filter(!is.na(nox_adj)) %>%
  group_by(Site) %>%
  summarise(n_homes = n_distinct(hhid_x),
            n_obs = sum(nox_adj_n),
            geo_mean = exp(mean(log(nox_adj))),
            geo_sd = exp(sd(log(nox_adj))),
            minimum = min(nox_adj),
            maximum = max(nox_adj)) %>%
  mutate(N_obs_homes = paste0(n_homes, " (", n_obs, ")"),
         geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(Geo_mean_sd, Range)
# site type and number of homes in each is same so it's cut out for purpose of table

formattable(bind_cols(taps_no2_sum_annualavgs,taps_nox_sum_annualavgs))


# NOTE - the -1.00 value is replaced here by lowest detected level (ie a pseudo LOD)/sqrt(2)
taps_pm25_sum_annualavgs_1 <- taps_ap_sitetype %>%
  filter(!is.na(pm25_adj)) %>%
  group_by(Site) %>%
  summarise(n_homes = n_distinct(hhid_x),
            n_obs = sum(pm25_adj_n),
            minimum = min(pm25_adj),
            maximum = max(pm25_adj)) 

taps_pm25_sum_annualavgs_2 <- taps_ap_sitetype %>%
  filter(!is.na(pm25_adj)) %>%
  mutate(pm25_adj = ifelse(pm25_adj<0, (1.68/sqrt(2)), pm25_adj)) %>%
  group_by(Site) %>%
  summarise(geo_mean = exp(mean(log(pm25_adj))),
            geo_sd = exp(sd(log(pm25_adj))))

taps_pm25_sum_annualavgs <- data.frame(taps_pm25_sum_annualavgs_1, taps_pm25_sum_annualavgs_2) %>%
  mutate(N_obs_homes = paste0(n_homes, " (", n_obs, ")"),
         geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(Site, N_obs_homes, Geo_mean_sd, Range)

taps_pm10_sum_annualavgs<- taps_ap_sitetype %>%
  filter(!is.na(pm10_adj)) %>%
  group_by(Site) %>%
  summarise(n_homes = n_distinct(hhid_x), 
            n_obs = sum(pm10_adj_n),
            geo_mean = exp(mean(log(pm10_adj))),
            geo_sd = exp(sd(log(pm10_adj))),
            minimum = min(pm10_adj),
            maximum = max(pm10_adj))  %>%
  mutate(N_obs_homes = paste0(n_homes, " (", n_obs, ")"),
         geo_mean = formatC(geo_mean, format = "fg", digits = 3, flag = "#"),
         geo_sd = formatC(geo_sd, format = "fg", digits = 3, flag = "#"),
         Geo_mean_sd = paste0(geo_mean, " (", geo_sd, ")"),
         minimum = formatC(minimum, format = "fg", digits = 3, flag = "#"),
         maximum = formatC(maximum, format = "fg", digits = 3, flag = "#"),
         Range = paste0(minimum," - ", maximum)) %>%
  dplyr::select(Site, N_obs_homes, Geo_mean_sd, Range)

formattable(bind_cols(taps_pm25_sum_annualavgs,taps_pm10_sum_annualavgs))



```


```{r Statistical Comparison of Annual Air Pollution Outcomes b/w PCWS and TAPS, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

# Testing b/w 2 studies with same pollutant
t.test(pcws_ap$no2_adj, taps_ap$no2_adj)
t.test(pcws_ap$pm25_adj, taps_ap$pm25_adj)
t.test(pcws_ap$pm10_adj, taps_ap$pm10_adj)

# Test b/w all homes with NO2 vs. those in the sensitivity analysis group (>1 obs.)
t.test(pcws_ap$no2_adj, pcws_ap_no2_multi$no2_adj)
t.test(taps_ap$no2_adj, pcws_ap_no2_multi$no2_adj)

```

```{r  Comparison of Meteorological Variables b/w PCWS and TAPS, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

# Precip changes 
ggplot(dplyr::filter(atmos_data, (Year==2015 | Year<1992 & Year>1985)), aes(as.factor(Month),PrecipInch)) + 
  geom_boxplot() + 
  facet_wrap(~Year)

#(12.5+11.63+6.49+14.95+10.78)/5 or 11.27 vs. 12.23



# Windspeed changes
ggplot(dplyr::filter(atmos_data, (Year==2015 | Year<1992 & Year>1986)), aes(as.factor(Month),WindSpeedmph)) + 
  geom_boxplot() + 
  facet_wrap(~Year)

# Avg differences in windspeed for PCWS v TAPS
# 7.95+8.67+9.59+8.09+3.15/5 vs 2.59

# Humidity changes
ggplot(dplyr::filter(atmos_data, (Year==2015 | Year<1992 & Year>1986)), aes(as.factor(Month),RHpct_Hour11)) + 
  geom_boxplot() + 
  facet_wrap(~Year)

# Avg differences in windspeed for PCWS v TAPS
# (30.6+29.0+24.3+31.2+31.7)/5 vs 23.5

# Avg high temp changes
# (1.04+.96+.54+1.24+.89)/5 or 0.93 vs. 1.01




```


```{r Comparison of LUR Predictor Values b/w PCWS and TAPS, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}


# Pull all predictors that are in models so only do summary stats of model predictors
pcws_mod_preds <- unique(c(names(coef(pcws_no2_mod))[-1],
                    names(coef(pcws_no2_mod_multiobs))[-1],
                    names(coef(pcws_pm25_mod))[-1],
                    names(coef(pcws_pm10_mod))[-1]))

taps_mod_preds <- unique(c(names(coef(taps_no2_mod))[-1],
                    names(coef(taps_nox_mod))[-1],
                    names(coef(taps_pm25_mod))[-1],
                    names(coef(taps_pm10_mod))[-1]))

# Pull out stats only for predictors
pcws_preds_stats <- data.frame("Variable" = names(apply(pcws_lur_data[,6:length(pcws_lur_data)], 2, mean)),
           "Mean"= apply(pcws_lur_data[,6:length(pcws_lur_data)], 2, mean),
           "SD"= apply(pcws_lur_data[,6:length(pcws_lur_data)], 2, sd),
           "Minimum" = apply(pcws_lur_data[,6:length(pcws_lur_data)], 2, min),
           "Maximum" = apply(pcws_lur_data[,6:length(pcws_lur_data)], 2, max)) 

taps_preds_stats <- data.frame("Variable" = names(apply(taps_lur_data[,6:length(taps_lur_data)], 2, mean)),
           "Mean"= apply(taps_lur_data[,6:length(taps_lur_data)], 2, mean),
           "SD"= apply(taps_lur_data[,6:length(taps_lur_data)], 2, sd),
           "Minimum" = apply(taps_lur_data[,6:length(taps_lur_data)], 2, min),
           "Maximum" = apply(taps_lur_data[,6:length(taps_lur_data)], 2, max)) 

pcws_preds_stats_all <- pcws_preds_stats %>%
  mutate(Mean = formatC(Mean, format = "fg", digits = 2),
         SD = formatC(SD, format = "fg", digits = 2),
         Mean_sd = paste0(Mean, " (", SD, ")"),
         Minimum = formatC(Minimum, format = "fg", digits = 2),
         Maximum = formatC(Maximum, format = "fg", digits = 2),
         Range = paste0(Minimum," - ", Maximum)) %>%
  dplyr::select(Variable, Mean_sd, Range) %>%
  arrange(Variable)

taps_preds_stats_all <- taps_preds_stats  %>%
  mutate(Mean = formatC(Mean, format = "fg", digits = 2),
         SD = formatC(SD, format = "fg", digits = 2),
         Mean_sd = paste0(Mean, " (", SD, ")"),
         Minimum = formatC(Minimum, format = "fg", digits = 2),
         Maximum = formatC(Maximum, format = "fg", digits = 2),
         Range = paste0(Minimum," - ", Maximum)) %>%
  dplyr::select(Variable, Mean_sd, Range) %>%
  arrange(Variable)

preds_stats_all <- inner_join(pcws_preds_stats_all, taps_preds_stats_all, by=c("Variable"))

# DISSERTATION REVISION - It was questioned whether a t-test made sense here to test how similar  distributions were of LUR predictors b/w TAPS and PCWS. Most predictors are NOT normally distributed (an assumption to meet for use of t-test) in both datasets, so it doesn't make sense to use this. Instead we will use the Wilcox rank sum test (aka Mann Whitney test) or the non-parametric version of the 2 sample t-test. I also redid the testing script as this wasn't functioning correctly before.

# # Test for significant differences between the two datasets with TTest
# pvalues <- unlist(lapply(pcws_lur_data[,c(preds_stats_all$Variable)], function(x) wilcox.test(x, taps_lur_data$x)$p.value), use.names = F)
# 
# pvalues <- lapply(pcws_lur_data[,c(preds_stats_all$Variable)], function(x) wilcox.test(x, taps_lur_data$x)$p.value)

pvalues <- c()

for (i in 1:length(preds_stats_all$Variable)) {
  
  pcws_name <- paste("pcws_lur_data$", eval(preds_stats_all$Variable[i]), sep = "")
  taps_name <- paste("taps_lur_data$", eval(preds_stats_all$Variable[i]), sep = "")

  pvalue <- wilcox.test(x=eval(parse(text=pcws_name)),y=eval(parse(text=taps_name)))$p.value
  
  pvalues <- c(pvalues,pvalue)
}

# Combine datasets and add in pvalues for t tests
preds_stats_all <- preds_stats_all %>%
  mutate(p_value = formatC(pvalues, format = "fg", digits = 2))


pcws_preds_stats_modsonly <- preds_stats_all[preds_stats_all$Variable %in% pcws_mod_preds,]

rownames(pcws_preds_stats_modsonly) <- NULL

taps_preds_stats_modsonly <- preds_stats_all[preds_stats_all$Variable %in% taps_mod_preds,]

rownames(taps_preds_stats_modsonly) <- NULL

preds_stats_modsonly <- bind_rows(pcws_preds_stats_modsonly,taps_preds_stats_modsonly) %>%
  distinct(.,Variable, Mean_sd.x, Range.x, Mean_sd.y, Range.y, p_value) %>%
  arrange(Variable)

formattable(preds_stats_all)
formattable(pcws_preds_stats_modsonly)
formattable(taps_preds_stats_modsonly)
formattable(preds_stats_modsonly)


pcws_lur_data %>%
  tally()

taps_lur_data%>%
  tally()

# # Dummy checks on the comparisons b/w distributions of predictors from either model
# 
# hist(log(pcws_lur_data$caline_pm25))
# hist(log(taps_lur_data$caline_pm25))
# 
# hist(pcws_lur_data$hh_100)
# hist(taps_lur_data$hh_100)
# 
# hist(pcws_lur_data$hh_300)
# hist(taps_lur_data$hh_300)
# 
# hist(pcws_lur_data$hh_5000)
# hist(taps_lur_data$hh_5000)
# 
# hist(pcws_lur_data$RHpct_Hour11)
# hist(taps_lur_data$RHpct_Hour11)



```
## Investigation of POP and HHOLD - SENSITIVITY MODEL TEST ONLY!
POP and HHOLD make model better but make no sense as variables (they're just population, not normalized by anything)

We need to figure out:
1. are they a proxy for something or different things?
2. is there an interaction going on here?

```{r Reload Full PCWS Dataset, echo=FALSE, message=FALSE, warning=FALSE}

# Combine air pollution values and predictors
pcws_lur_data <- inner_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))

# Check for homes iwth no addresses and make sure they really don't have anything
pcws_lur_data_nonjoin <- anti_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))
# 21 homes with no address data, many unique to 1992, which we know have no address data

# Join atmospheric data
pcws_lur_data <- inner_join(pcws_lur_data, atmos_data_yearavgs, by=c("styear"="Year"))

# Correct any predictor values (starting with the 6th column) that are NA to 0
pcws_lur_data[, 6:ncol(pcws_lur_data)][is.na(pcws_lur_data[, 6:ncol(pcws_lur_data)])] <- 0
# 
# # Remove predictor columns with all 0s, as this will cause the regression to fail
# pcws_lur_data <- pcws_lur_data[, apply(pcws_lur_data, 2, function(x) !all(x==0))] 

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
# Also drop count variables
pcws_lur_data <- pcws_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2)) %>%
  dplyr::select(-no2_adj_n ,-pm25_adj_n ,-pm10_adj_n) 

# NOTE - we are dropping observations that start in 1992 - there are results data but no address info
pcws_lur_data <- filter(pcws_lur_data, styear!=1992)
```


```{r Investigate POP and HHOLD, eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}

# Testing PM2.5
pcws_pm25_mod

plot(pcws_lur_data$hh_100, pcws_lur_data$distintvbusdepots1)
plot(pcws_lur_data$hh_100, pcws_lur_data$roads_rl_300)
plot(pcws_lur_data$hh_100, pcws_lur_data$stspeed_sl_100)

summary(lm(pm25_adj ~ hh_100 + distintvbusdepots1 + roads_rl_300 + stspeed_sl_100 + hh_100*distintvbusdepots1, data=pcws_lur_data))

summary(lm(pm25_adj ~ hh_100 + distintvbusdepots1 + roads_rl_300 + stspeed_sl_100 + hh_100*roads_rl_300, data=pcws_lur_data))

summary(lm(pm25_adj ~ hh_100 + distintvbusdepots1 + roads_rl_300 + stspeed_sl_100 + hh_100*stspeed_sl_100, data=pcws_lur_data))

# No significant interactions between variables!!!

pcws_lur_data %>%
  dplyr::select(-hhid_x) %>%
  correlate() %>%
  focus(hh_100) %>%
  arrange(hh_100)

pcws_lur_data %>%
  dplyr::select(-hhid_x) %>%
  correlate() %>%
  focus(hh_100) %>%
  arrange(desc(hh_100))

pcws_lur_data %>% 
  dplyr::select(-hhid_x) %>%
  correlate() %>% focus(hh_100) %>% filter(hh_100>0.3) %>%
  mutate(rowname = factor(rowname, levels = rowname[order(hh_100)])) %>%
  ggplot(aes(x = rowname, y = hh_100)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with hh_100") +
    xlab("Variable")

pcws_lur_data %>% 
  dplyr::select(-hhid_x) %>%
  correlate() %>% focus(hh_100) %>% filter(hh_100 < -0.35) %>%
  mutate(rowname = factor(rowname, levels = rowname[order(hh_100)])) %>%
  ggplot(aes(x = rowname, y = hh_100)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with hh_100") +
    xlab("Variable")

plot(pcws_lur_data$hh_100, pcws_lur_data$stspeed_sl_1000)

pcws_addrs_ap <- left_join(pcws_addrs, pcws_lur_data, by=c("hhid_x"))

tmap_mode("view")
tm_shape(filter(pcws_addrs_ap, !is.na(pm25_adj)))  +
  tm_dots(col="hh_100", n=4, style='quantile', palette = 'seq', size=0.1) +
  tm_basemap(server = "OpenTopoMap")




dev_hist <- read_sf("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Shapefiles/dev_plan.shp") %>%
  mutate(apprv_year = year(APPR_DATE))

dev_plan <- read_sf("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Shapefiles/dev_plan.shp") %>%
  mutate(apprv_year = year(APPR_DATE))

tm_shape(filter(dev_hist,!is.na(dev_hist$apprv_year) & dev_hist$apprv_year<1992 & dev_hist$apprv_year>1980)) +
  # tm_fill(col='blue2') +
  tm_squares(size="UNITS",col='blue2') +
  tm_shape(filter(pcws_addrs_ap, !is.na(pm25_adj)))  +
  tm_dots(col="hh_100", n=4, style='quantile', palette = 'seq', size=0.1)






# Testing PM10
pcws_pm10_mod

summary(lm(pm10_adj ~ hh_5000 + busstops_5000 + distintvrail1 + hh_5000*busstops_5000, data=pcws_lur_data))

summary(lm(pm10_adj ~ hh_5000 + busstops_5000 + distintvrail1 + hh_5000*distintvrail1, data=pcws_lur_data))

# No significant interactions between variables!!!


pcws_lur_data %>%
  dplyr::select(-hhid_x) %>%
  correlate() %>%
  focus(hh_5000) %>%
  arrange(desc(hh_5000))

# Serious correlation with Ycoord

pcws_lur_data %>%
  dplyr::select(-hhid_x) %>%
  correlate() %>%
  focus(hh_5000) %>%
  arrange(hh_5000)

# Serious neg correlation with XminusY

# what if we refit with those 2 vas?...
summary(lm(pm10_adj ~ Ycoord + XminusY + busstops_5000 + distintvrail1, data=pcws_lur_data))

# Either way, they don't help predict pm10...


pcws_addrs_ap <- left_join(pcws_addrs, pcws_lur_data, by=c("hhid_x"))

tmap_mode("view")
tm_shape(filter(pcws_addrs_ap, !is.na(pm10_adj)))  +
  tm_dots(col="hh_5000", n=4, style='quantile', palette = 'seq', size=0.1) +
  tm_basemap(server = "OpenTopoMap")

# this just looks like a proxy for how far outsdie of urban area in peripheral suburbs (foothills, marana, oro valley, etc), so higher the hh_5000, further outside core you are

# Could be a proxy for housing development during this time 1987-1991...?

dev_hist <- read_sf("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Shapefiles/dev_hist.shp") %>%
  mutate(apprv_year = year(APPR_DATE))

tm_shape(filter(dev_plan,!is.na(dev_plan$apprv_year) & dev_plan$apprv_year<1992 & dev_hist$apprv_year>1980)) +
  # tm_fill(col='blue2') +
  tm_squares(size="UNITS",col='blue2') +
  tm_shape(filter(pcws_addrs_ap, !is.na(pm10_adj)))  +
  tm_dots(col="hh_5000", n=4, style='quantile', palette = 'seq', size=0.1)

tm_shape(filter(dev_plan,!is.na(dev_plan$apprv_year) & dev_plan$apprv_year<1991 & dev_hist$apprv_year>1984)) +
  tm_squares(size="UNITS",col='blue2') +
  tm_shape(filter(pcws_addrs_ap, !is.na(pm10_adj)))  +
  tm_dots(col="hh_5000", n=4, style='quantile', palette = 'seq', size=0.1) +
  tm_scale_bar()



# Testing PM10
taps_pm10_mod

summary(lm(pm10_adj ~ hh_5000 + XplusY + elev + lu_nt_300 + hh_5000*XplusY, data=taps_lur_data))

summary(lm(pm10_adj ~ hh_5000 + XplusY + elev + lu_nt_300 + hh_5000*elev, data=taps_lur_data)) # significant interaction!

summary(lm(pm10_adj ~ hh_5000 + XplusY + elev + lu_nt_300 + hh_5000*lu_nt_300, data=taps_lur_data))



# Check to see how census numbers have changed between 1980 and 1990 in rural areas

census1980 <- read_sf("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Shapefiles/Tracts1980.shp") %>%
  mutate(tract_pop_dens_sqmi = POP/(Shape_Area * (1/2.788e+7)),
         tract_hhold_dens_sqmi = HHOLD/(Shape_Area * (1/2.788e+7)))

census1990 <- read_sf("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Shapefiles/Tracts1990.shp") %>%
  mutate(tract_pop_dens_sqmi = POP/(Shape_Area * (1/2.788e+7)),
         tract_hhold_dens_sqmi = HHOLD/(Shape_Area * (1/2.788e+7)))

tm_shape(census1990) +
  tm_fill(col="POP",palette = 'seq', n=4) +
  tm_shape(census1980) +
  tm_fill(col="POP",palette = 'seq', n=4)

tm_shape(census1990) +
  tm_fill(col="HHOLD",palette = 'seq', n=4) +
  tm_shape(census1980) +
  tm_fill(col="HHOLD",palette = 'seq', n=4)

tm_shape(census1990) +
  tm_fill(col="tract_hhold_dens_sqmi",palette = 'seq', breaks=c(0,1000,2000,3000)) +
  tm_shape(census1980) +
  tm_fill(col="tract_hhold_dens_sqmi",palette = 'seq', breaks=c(0,1000,2000,3000)) +
  tm_shape(filter(dev_plan,!is.na(dev_plan$apprv_year) & dev_plan$apprv_year<1991 & dev_plan$apprv_year>1984)) +
  tm_squares(size="UNITS",col='blue2')



```
It looks like development could be a proxy for total homes in larger rural census tracts outside of Tucson core!!!

This could also be related to road construction emissions from increasing capacity on roads! 


#### PCWS LUR Creation - Testing with Both Pop and Hhold and Densities
PCWS data is collected 1987-1992, however there are some homes that have no address data, notably ALL homes done in 1992 have NO address, thus cannot be used.

Due to there not being a background monitor available for PM2.5 until June 1988, we will exclude all values from 1988 and earlier (eg only PM2.5 values from 1989-1991)

For homes measured in more than 1 year, only keep year with the most measurements that year. For homes measured the same # of times in different years, keep the older as it will be closer to more life years of CRS early life stage (Birth + ID1, 1980-1992).


```{r PCWS LUR Predictor Assembly, echo=FALSE, message=FALSE, warning=FALSE}

pcws_lur_preds_pull <- function(folder_year){
  # Pull out predictors from each year folder, assign a 'styear' to them for merging to pollutant measurement dataset
  filenames=list.files(path=paste0("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/PCWS/Data/LUR/Predictors/",folder_year), full.names=TRUE)

  datalist = lapply(filenames, function(x){read.csv(file=x,header=T)})

  # Combine all predictor files and creates a styear field and deletes all "....x....y" styear artifacts from merging
  pcws_lur_preds <- datalist %>%
    Reduce(function(x,y) left_join(x,y,by="hhid_x"), .) %>% # merge all predictor csvs together
    mutate(year = styear.x) %>%
    dplyr::select(-starts_with("styear")) %>%
    mutate(styear = year) %>%
    dplyr::select(-year)
  
  pcws_lur_preds

}

# Save the LUR predictor file for each folder year
pcws_lur_preds_1987 <- pcws_lur_preds_pull('1987')
pcws_lur_preds_1988 <- pcws_lur_preds_pull('1988')
pcws_lur_preds_1989 <- pcws_lur_preds_pull('1989')
pcws_lur_preds_1990 <- pcws_lur_preds_pull('1990')
pcws_lur_preds_1991 <- pcws_lur_preds_pull('1991')



# Combine all the predictor sets
pcws_lur_preds <- bind_rows(pcws_lur_preds_1987,
                             pcws_lur_preds_1988,
                             pcws_lur_preds_1989,
                             pcws_lur_preds_1990,
                             pcws_lur_preds_1991)

# Combine air pollution values and predictors
pcws_lur_data <- inner_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))

# Check for homes iwth no addresses and make sure they really don't have anything
pcws_lur_data_nonjoin <- anti_join(pcws_ap, pcws_lur_preds, by=c("hhid_x","styear"))
# 21 homes with no address data, many unique to 1992, which we know have no address data

# Correct any predictor values (starting with the 6th column) that are NA to 0
pcws_lur_data[, 6:ncol(pcws_lur_data)][is.na(pcws_lur_data[, 6:ncol(pcws_lur_data)])] <- 0
# 
# # Remove predictor columns with all 0s, as this will cause the regression to fail
# pcws_lur_data <- pcws_lur_data[, apply(pcws_lur_data, 2, function(x) !all(x==0))] 

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
# Also drop count variables
pcws_lur_data <- pcws_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2)) %>%
  dplyr::select(-no2_adj_n ,-pm25_adj_n ,-pm10_adj_n) 

# NOTE - we are dropping observations that start in 1992 - there are results data but no address info
pcws_lur_data <- filter(pcws_lur_data, styear!=1992)

# Read in monthly meteorological variables
atmos_data <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/Temp_Wind Data_NOAA/NOAAValues_TempWindAvgs_LURModelingUpdated2019.csv")

atmos_data_yearavgs <- atmos_data %>%
  group_by(Year) %>%
  summarise(WindSpeedmph = mean(WindSpeedmph),
            MaxTempF = mean(MaxTempF),
            MinTempF = mean(MinTempF),
            PrecipInch = sum(PrecipInch),
            RHpct_Hour11 = mean(RHpct_Hour11))

pcws_lur_data <- inner_join(pcws_lur_data, atmos_data_yearavgs, by=c("styear"="Year"))




```

### PCWS LUR Creation - NO2

```{r PCWS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - NO2, echo=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
pcws_addrs <- pcws_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_pcws(pollutant = 'no2')
pcws_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'no2')
summary(mod)

```
Model is same as without density measures!


### PCWS LUR Creation - PM2.5
```{r PCWS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - PM2.5, echo=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
pcws_addrs <- pcws_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_pcws(pollutant = 'pm25')
pcws_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm25')
summary(mod)
AICc(mod)

# General diagnostics
ols_plot_diagnostics(mod) 

# Spatial autocorrelation diagnostics
spatial_auto_residmap(pollutant = 'pm25', pollutant_unit = 'ug/m3')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_taps_pm25()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)



```

Model changes to just bus depots and street speed in 50 m!!!

### PCWS LUR Creation - PM10
```{r PCWS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - PM10, echo=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
pcws_addrs <- pcws_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_pcws(pollutant = 'pm10')
pcws_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm10')
summary(mod)

```
Model does NOT change!


## Senstivity Analysis - PCWS LUR Creation - NO2 - Only Including Homes w/ Multi Measures in Different Seasons (ESCAPE/TAPS 3 seasons)

For homes measured more than once in more than 1 year, only keep year with the most measurements that year. For homes measured the same # of times in different years, keep the older as it will be closer to more life years of CRS early life stage (Birth + ID1, 1980-1992).

NO2 will be the test case as this dataset has the most homes measured more than once/year.



## TAPS

#### TAPS Data Load

NOTE - data cleaning and averaging code is in TAPS_Data_Cleanup.R

```{r TAPS Data Load, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

data <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/Results/TAPSData_AllObs.csv", sep=",")

taps_ap <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/Results/TAPSData.csv", sep=",") %>%
  mutate(hhid_x = paste(HHID, HHIDX, sep="_")) %>%
  filter(hhid_x != "QF44_A") # Dropped due to no GIS point taken and poor site location next to fire station
```


```{r TAPS LUR Predictor Assembly, echo=FALSE, message=FALSE, warning=FALSE}

taps_lur_preds_pull <- function(folder_year){
  # Pull out predictors from each year folder, assign a 'styear' to them for merging to pollutant measurement dataset
  filenames=list.files(path=paste0("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Predictors/",folder_year), full.names=TRUE)

  datalist = lapply(filenames, function(x){read.csv(file=x,header=T)})

  # Combine all predictor files and creates a styear field and deletes all "....x....y" styear artifacts from merging
  taps_lur_preds <- datalist %>%
    Reduce(function(x,y) left_join(x,y,by="hhid_x"), .) %>% # merge all predictor csvs together
    mutate(year = styear.x) %>%
    dplyr::select(-starts_with("styear")) %>%
    mutate(styear = year) %>%
    dplyr::select(-year)
  
  # Read in monthly meteorological variables
  atmos_data <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/Temp_Wind Data_NOAA/NOAAValues_TempWindAvgs_LURModelingUpdated2019.csv")
  
  atmos_data_yearavgs <- atmos_data %>%
    group_by(Year) %>%
    summarise(WindSpeedmph = mean(WindSpeedmph),
              MaxTempF = mean(MaxTempF),
              MinTempF = mean(MinTempF),
              PrecipInch = sum(PrecipInch),
              RHpct_Hour11 = mean(RHpct_Hour11))

  taps_lur_preds <- inner_join(taps_lur_preds, atmos_data_yearavgs, by=c("styear"="Year"))

  taps_lur_preds

}

taps_lur_preds <- taps_lur_preds_pull('2015')



# Check for homes iwth no addresses and make sure they really don't have anything
taps_lur_data_nonjoin <- anti_join(taps_ap, taps_lur_preds, by=c("hhid_x"))
# 1 home doesn't join, QF44, as it has locational address data - this was actually by a fire house (constant intermittent diesel source) so it was excluded due to poor location choice intially

# Combine air pollution values and predictors
taps_lur_data <- inner_join(taps_ap, taps_lur_preds, by=c("hhid_x"))

# Drop HHID, HHIDX, and uncorrected columns
taps_lur_data <- taps_lur_data %>%
  dplyr::select(hhid_x, everything(), -HHID, -HHIDX) %>%
  dplyr::select(-ends_with('unadj'))

# Correct any predictor values (starting with the 6th column) that are NA to 0
taps_lur_data[, 6:ncol(taps_lur_data)][is.na(taps_lur_data[, 6:ncol(taps_lur_data)])] <- 0

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
taps_lur_data <- taps_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))


```



### TAPS LUR Creation - NO2

```{r TAPS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - NO2,  message=FALSE, warning=FALSE}

# Create address set to work on 
taps_addrs <- taps_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_taps(pollutant = 'no2')
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'no2')
summary(mod)
AICc(mod)

```

NO CHANGES!


### TAPS LUR Creation - NOx

```{r TAPS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - NOx, eval=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
taps_addrs <- taps_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_taps(pollutant = 'nox')
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'nox')
summary(mod)

# General diagnostics
ols_plot_diagnostics(mod) 

# Spatial autocorrelation diagnostics
spatial_auto_residmap(pollutant = 'pm25', pollutant_unit = 'ug/m3')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_taps_pm25()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

```

Model changes slightly, however no hh or pop predictors are included or excluded!

### TAPS LUR Creation - PM2.5

```{r TAPS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - PM2.5, eval=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
taps_addrs <- taps_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_taps(pollutant = 'pm25')

# Based on output, test model variables and home with Cook's D > 1: LE46_A
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm25')

summary(mod)

hhids_to_drop = c("LE46_A")

summary(lm(mod,data = filter(taps_lur_data_TEMP, !(hhid_x %in% hhids_to_drop))))
# Substantial change in Ycoord, redevelop model without this predictor
mod <- escape_lur_mod_taps(pollutant = 'pm25', pred_to_drop = c('Ycoord'))

# Continue with normal model diagnostics

summary(mod)

```

No changes!

### TAPS LUR Creation - PM10

```{r TAPS LUR Model Building, Diagnostics, and Internal Validation (LOOCV) - PM10, eval=TRUE, message=FALSE, warning=FALSE}

# Create address set to work on 
taps_addrs <- taps_addrs_load()

# Model building and summary stats
mod <- escape_lur_mod_taps(pollutant = 'pm10')

# Based on output, test model variables and home with Cook's D > 1: CH_25A
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm10')

summary(mod)

hhids_to_drop = c("CH25_A")

summary(lm(mod,data = filter(taps_lur_data_TEMP, !(hhid_x %in% hhids_to_drop))))

# So no substantial changes to any prediction coefs, we will drop this home entirely
taps_lur_data <- filter(taps_lur_data, !(hhid_x %in% hhids_to_drop))

mod <- escape_lur_mod_taps(pollutant = 'pm10')

summary(mod)

# Continue with normal model diagnostics
taps_lur_data_TEMP <- escape_lur_diag_data(pollutant = 'pm10')

summary(mod)

# General diagnostics
# ols_plot_diagnostics(mod)  #doesn't work

# Spatial autocorrelation diagnostics
spatial_auto_residmap(pollutant = 'pm25', pollutant_unit = 'ug/m3')

# Internal Validation
internal_valid_loocv()

# Mixed effects model adaptation of best fitting non-mixed model
mix_mod <- escape_lur_mix_mod_taps_pm25()
summary(mix_mod)

# AICc comparison of non-mixed vs. mixed (lower the better)
AICc(mod)
AICc(mix_mod)

```

Changes in variables, such that, now all predictors but one are related to buses!...and pop_5000, model R2 is now 86%

NO models incorporated pop or housing density predictors!!!

## Predict CRS Participant Birth and Age 6 Exposures to NO2, NOx, PM2.5, and PM10
For each pollutant, the model with: 1) the lowest Kolmogorov-Smirnov D; 2) the highest externally adjusted R2; and 3) lowest root-mean-squared error values will have residuals tested for spatial autocorrelation (Moran’s I). If there is statistically significant spatial autocorrelation (p-value <0.05) detected in the residuals of that model, the next model will be evaluated for spatial autocorrelation, and so on. I will use this model to predict air pollutant exposures for TCRS participants at birth and age 6 home addresses. 

All GIS predictors will already need to be pulled for this dataset for all relevant years (1980-1992) using the "GIS_Predictor_Creation.R" script.

Additionally, if a model to be used contains a predictor that is NOT in the CRS predictor set, it must be imputed as '0' (as done previously for lu_hr_1000 in PCWS data for the TAPS NO2 model).



```{r Compile TCRS GIS Predictors, eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}

# Pull in all predictor values for TCRS participants
tcrs_lur_preds_pull <- function(folder_year){
  # Pull out predictors from each year folder, assign a 'styear' to them for merging to pollutant measurement dataset
  filenames=list.files(path=paste0("/Volumes/Lexar/TCRS/Data/LUR/Predictors/",folder_year), full.names=TRUE)

  datalist = lapply(filenames, function(x){read.csv(file=x,header=T)})

  # Combine all predictor files and creates a styear field and deletes all "....x....y" styear artifacts from merging
  tcrs_lur_preds <- datalist %>%
    Reduce(function(x,y) left_join(x,y,by="hhid_x"), .) %>% # merge all predictor csvs together
    mutate(year = styear.x) %>%
    dplyr::select(-starts_with("styear")) %>%
    mutate(styear = year) %>%
    dplyr::select(-year)
  
    # Read in monthly meteorological variables
  atmos_data <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/Temp_Wind Data_NOAA/NOAAValues_TempWindAvgs_LURModelingUpdated2019.csv")
  
  atmos_data_yearavgs <- atmos_data %>%
    group_by(Year) %>%
    summarise(WindSpeedmph = mean(WindSpeedmph),
              MaxTempF = mean(MaxTempF),
              MinTempF = mean(MinTempF),
              PrecipInch = sum(PrecipInch),
              RHpct_Hour11 = mean(RHpct_Hour11))

  tcrs_lur_preds <- inner_join(tcrs_lur_preds, atmos_data_yearavgs, by=c("styear"="Year"))

  tcrs_lur_preds
  
}

# Save the LUR predictor file for each folder year
tcrs_lur_preds_1980 <- tcrs_lur_preds_pull('1980')
tcrs_lur_preds_1981 <- tcrs_lur_preds_pull('1981')
tcrs_lur_preds_1982 <- tcrs_lur_preds_pull('1982')
tcrs_lur_preds_1983 <- tcrs_lur_preds_pull('1983')
tcrs_lur_preds_1984 <- tcrs_lur_preds_pull('1984')
tcrs_lur_preds_1985 <- tcrs_lur_preds_pull('1985')
tcrs_lur_preds_1986 <- tcrs_lur_preds_pull('1986')
tcrs_lur_preds_1987 <- tcrs_lur_preds_pull('1987')
tcrs_lur_preds_1988 <- tcrs_lur_preds_pull('1988')
tcrs_lur_preds_1989 <- tcrs_lur_preds_pull('1989')
tcrs_lur_preds_1990 <- tcrs_lur_preds_pull('1990')
tcrs_lur_preds_1991 <- tcrs_lur_preds_pull('1991')
tcrs_lur_preds_1992 <- tcrs_lur_preds_pull('1992')




# Combine all the predictor sets
tcrs_lur_data <- bind_rows(tcrs_lur_preds_1980,
                           tcrs_lur_preds_1981,
                           tcrs_lur_preds_1982,
                           tcrs_lur_preds_1983,
                           tcrs_lur_preds_1984,
                           tcrs_lur_preds_1985,
                           tcrs_lur_preds_1986,
                           tcrs_lur_preds_1987,
                           tcrs_lur_preds_1988,
                           tcrs_lur_preds_1989,
                           tcrs_lur_preds_1990,
                           tcrs_lur_preds_1991,
                           tcrs_lur_preds_1992)

# Correct any predictor values (starting with the 1st column) that are NA to 0
tcrs_lur_data[, 1:ncol(tcrs_lur_data)][is.na(tcrs_lur_data[, 1:ncol(tcrs_lur_data)])] <- 0

# Remove predictor columns with all 0s, as this will cause the regression to fail
tcrs_lur_data <- tcrs_lur_data[, apply(tcrs_lur_data, 2, function(x) !all(x==0))]

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
# Also drop count variables
tcrs_lur_data <- tcrs_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

```


**From ESCAPE Procedure for Exposure Assessment:**
"The proposed procedure is then that we truncate the values for predictor values at the cohort addresses and these will be given the highest value which occurs at one of the monitoring sites for that specific predictor variable (NB. for PM/NOx areas this highest value depends on the pollutant that will be estimated, as well on whether it is all sites or background model). This will be done for all values above the highest (or lowest) value at a monitoring site, regardless of how much higher this value is.
   
For documentation purposes, please estimate then concentrations for the addresses using both the original predictor values (i.e. the untruncated values) and using the truncated values.
The distribution of the estimated concentrations at the addresses will be explored and compared with the distribution of the measured concentrations at the monitoring sites in order to assess the predicted concentrations."



```{r Predict TCRS Participant Exposures - Actual TCRS GIS Predictor Values, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

# Predict using the ACTUAL GIS predictor values

# Method 3 Prediction Function
method3_prediction_nontrunc <- function(year_mod_devel=1989, model_dataset='pcws', pred_dataset='tcrs_lur_data', pollutant){

  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a temporary dataset
  lur_data_TEMP <- eval(parse(text=pred_dataset))
  
  # Create the model to predict the pollutant outcome
  mod_TEMP <- eval(parse(text=(paste(model_dataset, pollutant, "mod", sep="_"))))
  
  # Predict pollutant outcome at sites
  lur_data_TEMP$predicted <- predict(mod_TEMP, newdata=lur_data_TEMP)

    # Update the ref_year_data sets so that PM2.5 and PM10 are updated with the mean value of all available TCRS years
  # This is done as there is little to no discernable pattern in these pollutants to adjust for, nor do they track with NO2 measures during the same
  
  ref_data_year$pm25_year_avg <- ifelse(ref_data_year$year<1989,
                                        mean(unlist(ref_data_year[which(ref_data_year$year>1988 & ref_data_year$year<1993),3])),
                                        ref_data_year$pm25_year_avg)
  
    ref_data_year$pm10_year_avg <- ifelse(ref_data_year$year<1985,
                                        mean(unlist(ref_data_year[which(ref_data_year$year>1984 & ref_data_year$year<1993),4])),
                                        ref_data_year$pm10_year_avg)
    
    
  
  # Merge the reference data to the temporary dataset on year=styear, note that taps_ap doesn't have a 'styear' field since they're all the same, so we're band-aiding it here by an if statement for 2 styaer fields when merging pcws
  ifelse("styear.x" %in% names(lur_data_TEMP),
         lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear.x'='year')),
                                     lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear'='year')))
  

  for (i in 1: length(lur_data_TEMP$predicted)) {
  if(pollutant == 'no2'){
    lur_data_TEMP$predicted[i] <- lur_data_TEMP$predicted[i] + (lur_data_TEMP$no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2]))
  }
    if(pollutant == 'pm25'){
    lur_data_TEMP$predicted[i] <- lur_data_TEMP$predicted[i] + (lur_data_TEMP$pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3]))
    }
  if(pollutant == 'pm10'){
    lur_data_TEMP$predicted[i] <- lur_data_TEMP$predicted[i] + (lur_data_TEMP$pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4]))
  }
}
  # lur_data_TEMP$predicted <- ifelse(pollutant == 'no2',
  #                                   lur_data_TEMP$predicted + (lur_data_TEMP$no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2])), lur_data_TEMP$predicted)
  # 
  # lur_data_TEMP$predicted <- ifelse(pollutant == 'pm25',
  #                                   lur_data_TEMP$predicted + lur_data_TEMP$pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3]), lur_data_TEMP$predicted)
  # 
  # lur_data_TEMP$predicted <- ifelse(pollutant == 'pm10',
  #                                   lur_data_TEMP$predicted + (lur_data_TEMP$pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4])), lur_data_TEMP$predicted)
  #                
                                                 
  lur_data_TEMP <- lur_data_TEMP %>%
    dplyr::select(hhid_x, predicted) %>%
    mutate(tcrs_indepth = ifelse(str_detect(hhid_x, "enr"),
                                 "enr",
                                 ifelse(str_detect(hhid_x, "id1"),
                                        "id1",
                                        "Check TCRS ids for proper suffix!")),
           hhid_x = substr(hhid_x,1,5)) %>%
    spread(key=tcrs_indepth, value=predicted)
  
  colnames(lur_data_TEMP)[colnames(lur_data_TEMP)=="enr"] <- paste0(pollutant,"_enr_nontrunc") 
  colnames(lur_data_TEMP)[colnames(lur_data_TEMP)=="id1"] <- paste0(pollutant,"_id1_nontrunc")
  
  lur_data_TEMP
  
}

tcrs_no2_nontrunc <- method3_prediction_nontrunc(year_mod_devel=1989, model_dataset='pcws', pred_dataset='tcrs_lur_data', pollutant='no2')

tcrs_nox_nontrunc <- method3_prediction_nontrunc(year_mod_devel=2015, model_dataset='taps', pred_dataset='tcrs_lur_data', pollutant='nox')

tcrs_pm25_nontrunc <- method3_prediction_nontrunc(year_mod_devel=1989, model_dataset='pcws', pred_dataset='tcrs_lur_data', pollutant='pm25')

tcrs_pm10_nontrunc <- method3_prediction_nontrunc(year_mod_devel=1989, model_dataset='pcws', pred_dataset='tcrs_lur_data', pollutant='pm10')


tcrs_nontrunc_list <- list(tcrs_no2_nontrunc,
                           tcrs_nox_nontrunc,
                           tcrs_pm25_nontrunc,
                           tcrs_pm10_nontrunc)

tcrs_nontrunc_predictions <- tcrs_nontrunc_list %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)

```

```{r Predict TCRS Participant Exposures - TRUNCATED TCRS GIS Predictor Values, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

# Predict using the truncated GIS predictor values (no TCRS values higher than max PCWS/TAPS value; ditto for low values and min PCWS/TAPS value for predictor)

pcws_lur_data_mins <- data.frame(t(apply(pcws_lur_data, 2, min)))
pcws_lur_data_maxs <- data.frame(t(apply(pcws_lur_data, 2, max)))

taps_lur_data_mins <- data.frame(t(apply(taps_lur_data, 2, min)))
taps_lur_data_maxs <- data.frame(t(apply(taps_lur_data, 2, max)))

lur_data_minmax <- bind_rows(pcws_lur_data_mins,
                                  pcws_lur_data_maxs,
                                  taps_lur_data_mins,
                                  taps_lur_data_maxs) %>%
  dplyr::select(-hhid_x, -styear, -no2_adj, -nox_adj, -pm25_adj, -pm10_adj) %>%
  mutate_all(as.numeric)

lur_data_minmax <-lur_data_minmax %>%
  mutate(hhid_x = c("pcws_min", "pcws_max", "taps_min", "taps_max"))

# Method 3 Prediction Function - truncated predictors
method3_prediction_trunc <- function(year_mod_devel=1989, model_dataset='pcws', pred_dataset='tcrs_lur_data', pollutant){

  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a temporary dataset
  lur_data_TEMP <- eval(parse(text=pred_dataset))
  
  # Truncate so all predictor values are b/w min and max in model_dataset
  # Reorder so hhidx and styear are first 2 cols and can be easily skipped in truncation
  lur_data_TEMP <- bind_rows(lur_data_TEMP,lur_data_minmax) %>%
    dplyr::select(hhid_x, styear, everything())
  
  # Change any NAs to 0s just as in previous predictor datasets
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

# Replace any values below the min in original model dataset to the min, ditto for max
# done first for PCWS models, then if not that, then assumed to be TAPS
if(model_dataset=='pcws'){
  #loop through each column in dataset, except first which has just IDs
  for(j in 3:ncol(lur_data_TEMP)){
      min <- lur_data_TEMP[which(lur_data_TEMP$hhid_x == 'pcws_min'),j]
      max <- lur_data_TEMP[which(lur_data_TEMP$hhid_x == 'pcws_max'),j]
  for(i in 1:(nrow(lur_data_TEMP)-4)){
  #loop for all rows, except for the 4 which hold min/max values for PCWS and TAPS model datasets
      if(min>lur_data_TEMP[i,j]){
        lur_data_TEMP[i,j] <- min
      } else {
        lur_data_TEMP[i,j]
      }
      if(max<lur_data_TEMP[i,j]){
        lur_data_TEMP[i,j] <- max
      } else {
        lur_data_TEMP[i,j]
      }}}} else {
          #loop through each column in dataset, except first which has just IDs
          for(j in 3:ncol(lur_data_TEMP)){
      min <- lur_data_TEMP[which(lur_data_TEMP$hhid_x == 'taps_min'),j]
      max <- lur_data_TEMP[which(lur_data_TEMP$hhid_x == 'taps_max'),j]
  for(i in 1:(nrow(lur_data_TEMP)-4)){
  #loop for all rows, except for the 4 which hold min/max values for PCWS and TAPS model datasets
      if(min>lur_data_TEMP[i,j]){
        lur_data_TEMP[i,j] <- min
      } else {
        lur_data_TEMP[i,j]
      }
      if(max<lur_data_TEMP[i,j]){
        lur_data_TEMP[i,j] <- max
      } else {
        lur_data_TEMP[i,j]
      }}}}
  
  # Drop the min/max truncation rows
  lur_data_TEMP <- lur_data_TEMP[1:(nrow(lur_data_TEMP)-4),]

  # Create the model to predict the pollutant outcome
  mod_TEMP <- eval(parse(text=(paste(model_dataset, pollutant, "mod", sep="_"))))
  
  # Predict pollutant outcome at sites
  lur_data_TEMP$predicted <- predict(mod_TEMP, newdata=lur_data_TEMP)

    # Update the ref_year_data sets so that PM2.5 and PM10 are updated with the mean value of all available TCRS years
  # This is done as there is little to no discernable pattern in these pollutants to adjust for, nor do they track with NO2 measures during the same
  
  ref_data_year$pm25_year_avg <- ifelse(ref_data_year$year<1989,
                                        mean(unlist(ref_data_year[which(ref_data_year$year>1988 & ref_data_year$year<1993),3])),
                                        ref_data_year$pm25_year_avg)
  
    ref_data_year$pm10_year_avg <- ifelse(ref_data_year$year<1985,
                                        mean(unlist(ref_data_year[which(ref_data_year$year>1984 & ref_data_year$year<1993),4])),
                                        ref_data_year$pm10_year_avg)
    
    
  
  # Merge the reference data to the temporary dataset on year=styear, note that taps_ap doesn't have a 'styear' field since they're all the same, so we're band-aiding it here by an if statement for 2 styaer fields when merging pcws
  ifelse("styear.x" %in% names(lur_data_TEMP),
         lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear.x'='year')),
                                     lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear'='year')))
  

    for (i in 1: length(lur_data_TEMP$predicted)) {
  if(pollutant == 'no2'){
    lur_data_TEMP$predicted[i] <- lur_data_TEMP$predicted[i] + (lur_data_TEMP$no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2]))
  }
    if(pollutant == 'pm25'){
    lur_data_TEMP$predicted[i] <- lur_data_TEMP$predicted[i] + (lur_data_TEMP$pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3]))
    }
  if(pollutant == 'pm10'){
    lur_data_TEMP$predicted[i] <- lur_data_TEMP$predicted[i] + (lur_data_TEMP$pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4]))
  }
}  
  
  # lur_data_TEMP$predicted <- ifelse(pollutant == 'no2',
  #                                   lur_data_TEMP$predicted + (lur_data_TEMP$no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2])), lur_data_TEMP$predicted)
  # 
  # lur_data_TEMP$predicted <- ifelse(pollutant == 'pm25',
  #                                   lur_data_TEMP$predicted + lur_data_TEMP$pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3]), lur_data_TEMP$predicted)
  # 
  # lur_data_TEMP$predicted <- ifelse(pollutant == 'pm10',
  #                                   lur_data_TEMP$predicted + (lur_data_TEMP$pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4])), lur_data_TEMP$predicted)
  #                
                                                 
  lur_data_TEMP <- lur_data_TEMP %>%
    dplyr::select(hhid_x, predicted) %>%
    mutate(tcrs_indepth = ifelse(str_detect(hhid_x, "enr"),
                                 "enr",
                                 ifelse(str_detect(hhid_x, "id1"),
                                        "id1",
                                        "Check TCRS ids for proper suffix!")),
           hhid_x = substr(hhid_x,1,5)) %>%
    spread(key=tcrs_indepth, value=predicted)
  
  colnames(lur_data_TEMP)[colnames(lur_data_TEMP)=="enr"] <- paste0(pollutant,"_enr_trunc") 
  colnames(lur_data_TEMP)[colnames(lur_data_TEMP)=="id1"] <- paste0(pollutant,"_id1_trunc")
  
  lur_data_TEMP
  
}


tcrs_no2_trunc <- method3_prediction_trunc(year_mod_devel=1989, model_dataset='pcws', pred_dataset='tcrs_lur_data', pollutant='no2')

tcrs_nox_trunc <- method3_prediction_trunc(year_mod_devel=2015, model_dataset='taps', pred_dataset='tcrs_lur_data', pollutant='nox')

tcrs_pm25_trunc <- method3_prediction_trunc(year_mod_devel=1989, model_dataset='pcws', pred_dataset='tcrs_lur_data', pollutant='pm25')

tcrs_pm10_trunc <- method3_prediction_trunc(year_mod_devel=1989, model_dataset='pcws', pred_dataset='tcrs_lur_data', pollutant='pm10')

tcrs_trunc_list <- list(tcrs_no2_trunc,
                           tcrs_nox_trunc,
                           tcrs_pm25_trunc,
                           tcrs_pm10_trunc)

tcrs_trunc_predictions <- tcrs_trunc_list %>%
  Reduce(function(x,y) left_join(x,y,by="hhid_x"), .)

```

```{r Predict TCRS Participant Exposures - Combine and Compare Non/Truncated Predictions from TCRS GIS Predictor Values, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

tcrs_predictions <- inner_join(tcrs_trunc_predictions, tcrs_nontrunc_predictions, by=c("hhid_x"))


tcrs_predictions_correlation <- tcrs_predictions[,-1]
chart.Correlation(tcrs_predictions_correlation, histogram=TRUE, method= c("spearman"), pch=19)


```

* As per instructions, we will analyze complete cases (all LUR variables and predictions AND TCRS predictors from study).

```{r Combine TCRS Predictors and Predictions for Each In-Depth Period, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

# Spread out TCRS LUR predictor data to join with truncated prediction values by hhid_x
tcrs_lur_data_enr <- tcrs_lur_data %>%
  mutate(tcrs_indepth = ifelse(str_detect(hhid_x, "enr"),
                               "enr",
                               ifelse(str_detect(hhid_x, "id1"),
                                      "id1",
                                      "Check TCRS ids for proper suffix!")),
         hhid_x = substr(hhid_x,1,5)) %>%
  filter(tcrs_indepth=="enr")

tcrs_lur_data_id1 <- tcrs_lur_data %>%
  mutate(tcrs_indepth = ifelse(str_detect(hhid_x, "enr"),
                               "enr",
                               ifelse(str_detect(hhid_x, "id1"),
                                      "id1",
                                      "Check TCRS ids for proper suffix!")),
         hhid_x = substr(hhid_x,1,5)) %>%
  filter(tcrs_indepth=="id1")

# Drop all homes that don't have both enrollment and ID1 measures
tcrs_complt_preds_enr <- inner_join(tcrs_trunc_predictions, tcrs_lur_data_enr, by=c("hhid_x")) %>%
  dplyr::select(-no2_id1_trunc,-nox_id1_trunc,-pm25_id1_trunc,-pm10_id1_trunc)

colnames(tcrs_complt_preds_enr)<-sub("_enr_trunc","",colnames(tcrs_complt_preds_enr))


tcrs_complt_preds_id1 <- inner_join(tcrs_trunc_predictions, tcrs_lur_data_id1, by=c("hhid_x")) %>%
  dplyr::select(-no2_enr_trunc,-nox_enr_trunc,-pm25_enr_trunc,-pm10_enr_trunc)

colnames(tcrs_complt_preds_id1)<-sub("_id1_trunc","",colnames(tcrs_complt_preds_id1))


# Output file of GIS-based predictors
write.csv(tcrs_complt_preds_enr, "/Volumes/Lexar/AA_CRS_NATE_DISSERTATION_TEMP/Data/tcrs_complt_preds_enr.csv", row.names = F, )
write.csv(tcrs_complt_preds_id1, "/Volumes/Lexar/AA_CRS_NATE_DISSERTATION_TEMP/Data/tcrs_complt_preds_id1.csv", row.names = F)

```



## Predict 1 Mile Grid Exposures to NO2, NOx, PM2.5, and PM10
Same rules apply as in intro to previous section "Predict CRS Participant..."


```{r Compile TCRS GIS Predictors, eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}

# Pull in all predictor values for TCRS participants
grid_lur_preds_pull <- function(folder_year){
  # Pull out predictors from each year folder, assign a 'styear' to them for merging to pollutant measurement dataset
  filenames=list.files(path=paste0("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/GRID/Data/LUR/Predictors/",folder_year), full.names=TRUE)

  datalist = lapply(filenames, function(x){read.csv(file=x,header=T)})

  # Combine all predictor files and creates a styear field and deletes all "....x....y" styear artifacts from merging
  grid_lur_preds <- datalist %>%
    Reduce(function(x,y) left_join(x,y,by="hhid_x"), .) %>% # merge all predictor csvs together
    mutate(year = styear.x) %>%
    dplyr::select(-starts_with("styear")) %>%
    mutate(styear = year) %>%
    dplyr::select(-year)
  
    # Read in monthly meteorological variables
  atmos_data <- read.csv("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/Temp_Wind Data_NOAA/NOAAValues_TempWindAvgs_LURModelingUpdated2019.csv")
  
  atmos_data_yearavgs <- atmos_data %>%
    group_by(Year) %>%
    summarise(WindSpeedmph = mean(WindSpeedmph),
              MaxTempF = mean(MaxTempF),
              MinTempF = mean(MinTempF),
              PrecipInch = sum(PrecipInch),
              RHpct_Hour11 = mean(RHpct_Hour11))

  grid_lur_preds <- inner_join(grid_lur_preds, atmos_data_yearavgs, by=c("styear"="Year"))

  grid_lur_preds
  
}

# Save the LUR predictor file for each folder year
grid_lur_preds_1980 <- grid_lur_preds_pull('1980')
grid_lur_preds_1992 <- grid_lur_preds_pull('1992')
grid_lur_preds_2015 <- grid_lur_preds_pull('2015')





# Combine all the predictor sets
grid_lur_data <- bind_rows(grid_lur_preds_1980,
                           grid_lur_preds_1992,
                           grid_lur_preds_2015)

# Correct any predictor values (starting with the 1st column) that are NA to 0
grid_lur_data[, 1:ncol(grid_lur_data)][is.na(grid_lur_data[, 1:ncol(grid_lur_data)])] <- 0

# Remove predictor columns with all 0s, as this will cause the regression to fail
grid_lur_data <- grid_lur_data[, apply(grid_lur_data, 2, function(x) !all(x==0))]

# NOTE - stspeed_nr variables will be deleted until this analysis can be worked out!!! It doesn't work for whatever reason (even though the stspeed loading analyses do - see section in GIS_Prediction_Creation.R script)
# Also drop count variables
grid_lur_data <- grid_lur_data %>%
  dplyr::select(-c(stspeednear, distinvstspeed1, distinvstspeed2))

```


**From ESCAPE Procedure for Exposure Assessment:**
"The proposed procedure is then that we truncate the values for predictor values at the cohort addresses and these will be given the highest value which occurs at one of the monitoring sites for that specific predictor variable (NB. for PM/NOx areas this highest value depends on the pollutant that will be estimated, as well on whether it is all sites or background model). This will be done for all values above the highest (or lowest) value at a monitoring site, regardless of how much higher this value is.
   
For documentation purposes, please estimate then concentrations for the addresses using both the original predictor values (i.e. the untruncated values) and using the truncated values.
The distribution of the estimated concentrations at the addresses will be explored and compared with the distribution of the measured concentrations at the monitoring sites in order to assess the predicted concentrations."

IN THE CASE OF GRIDS, we will only use truncated versions as points will be unrealistically close to certain sources!!!


```{r Predict Grid Exposures - TRUNCATED GRID GIS Predictor Values, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

# Predict using the truncated GIS predictor values (no GRID values higher than max PCWS/TAPS value; ditto for low values and min PCWS/TAPS value for predictor)

pcws_lur_data_mins <- data.frame(t(apply(pcws_lur_data, 2, min)))
pcws_lur_data_maxs <- data.frame(t(apply(pcws_lur_data, 2, max)))

taps_lur_data_mins <- data.frame(t(apply(taps_lur_data, 2, min)))
taps_lur_data_maxs <- data.frame(t(apply(taps_lur_data, 2, max)))

lur_data_minmax <- bind_rows(pcws_lur_data_mins,
                                  pcws_lur_data_maxs,
                                  taps_lur_data_mins,
                                  taps_lur_data_maxs) %>%
  dplyr::select(-hhid_x, -styear, -no2_adj, -nox_adj, -pm25_adj, -pm10_adj) %>%
  mutate_all(as.numeric)

lur_data_minmax <-lur_data_minmax %>%
  mutate(hhid_x = c("pcws_min", "pcws_max", "taps_min", "taps_max"))

# Method 3 Prediction Function - truncated predictors
method3_prediction_trunc_GRID <- function(year_mod_devel=1989, model_dataset='pcws', pred_dataset='grid_lur_data', pollutant){

  # Create a pollutant outcome field variable (eg "no2_adj")
  pollutant_outcome <- paste0(pollutant,"_adj")
  
  # Create a temporary dataset
  lur_data_TEMP <- eval(parse(text=pred_dataset))
  
  # Truncate so all predictor values are b/w min and max in model_dataset
  # Reorder so hhidx and styear are first 2 cols and can be easily skipped in truncation
  lur_data_TEMP <- bind_rows(lur_data_TEMP,lur_data_minmax) %>%
    dplyr::select(hhid_x, styear, everything())
  
  # Change any NAs to 0s just as in previous predictor datasets
  lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)][is.na(lur_data_TEMP[, grep("distintvair1", colnames(lur_data_TEMP)):ncol(lur_data_TEMP)])] <- 0

# Replace any values below the min in original model dataset to the min, ditto for max
# done first for PCWS models, then if not that, then assumed to be TAPS
if(model_dataset=='pcws'){
  #loop through each column in dataset, except first which has just IDs
  for(j in 3:ncol(lur_data_TEMP)){
      min <- lur_data_TEMP[which(lur_data_TEMP$hhid_x == 'pcws_min'),j]
      max <- lur_data_TEMP[which(lur_data_TEMP$hhid_x == 'pcws_max'),j]
  for(i in 1:(nrow(lur_data_TEMP)-4)){
  #loop for all rows, except for the 4 which hold min/max values for PCWS and TAPS model datasets
      if(min>lur_data_TEMP[i,j]){
        lur_data_TEMP[i,j] <- min
      } else {
        lur_data_TEMP[i,j]
      }
      if(max<lur_data_TEMP[i,j]){
        lur_data_TEMP[i,j] <- max
      } else {
        lur_data_TEMP[i,j]
      }}}} else {
          #loop through each column in dataset, except first which has just IDs
          for(j in 3:ncol(lur_data_TEMP)){
      min <- lur_data_TEMP[which(lur_data_TEMP$hhid_x == 'taps_min'),j]
      max <- lur_data_TEMP[which(lur_data_TEMP$hhid_x == 'taps_max'),j]
  for(i in 1:(nrow(lur_data_TEMP)-4)){
  #loop for all rows, except for the 4 which hold min/max values for PCWS and TAPS model datasets
      if(min>lur_data_TEMP[i,j]){
        lur_data_TEMP[i,j] <- min
      } else {
        lur_data_TEMP[i,j]
      }
      if(max<lur_data_TEMP[i,j]){
        lur_data_TEMP[i,j] <- max
      } else {
        lur_data_TEMP[i,j]
      }}}}
  
  # Drop the min/max truncation rows
  lur_data_TEMP <- lur_data_TEMP[1:(nrow(lur_data_TEMP)-4),]

  # Create the model to predict the pollutant outcome
  mod_TEMP <- eval(parse(text=(paste(model_dataset, pollutant, "mod", sep="_"))))
  
  # Predict pollutant outcome at sites
  lur_data_TEMP$predicted <- predict(mod_TEMP, newdata=lur_data_TEMP)

    # Update the ref_year_data sets so that PM2.5 and PM10 are updated with the mean value of all available TCRS years
  # This is done as there is little to no discernable pattern in these pollutants to adjust for, nor do they track with NO2 measures during the same
  
  ref_data_year$pm25_year_avg <- ifelse(ref_data_year$year<1989,
                                        mean(unlist(ref_data_year[which(ref_data_year$year>1988 & ref_data_year$year<1993),3])),
                                        ref_data_year$pm25_year_avg)
  
    ref_data_year$pm10_year_avg <- ifelse(ref_data_year$year<1985,
                                        mean(unlist(ref_data_year[which(ref_data_year$year>1984 & ref_data_year$year<1993),4])),
                                        ref_data_year$pm10_year_avg)
    
    
  
  # Merge the reference data to the temporary dataset on year=styear, note that taps_ap doesn't have a 'styear' field since they're all the same, so we're band-aiding it here by an if statement for 2 styaer fields when merging pcws
  ifelse("styear.x" %in% names(lur_data_TEMP),
         lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear.x'='year')),
                                     lur_data_TEMP <- inner_join(lur_data_TEMP, ref_data_year, by=c('styear'='year')))
  
for (i in 1: length(lur_data_TEMP$predicted)) {
  if(pollutant == 'no2'){
    lur_data_TEMP$predicted[i] <- lur_data_TEMP$predicted[i] + (lur_data_TEMP$no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2]))
  }
    if(pollutant == 'pm25'){
    lur_data_TEMP$predicted[i] <- lur_data_TEMP$predicted[i] + (lur_data_TEMP$pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3]))
    }
  if(pollutant == 'pm10'){
    lur_data_TEMP$predicted[i] <- lur_data_TEMP$predicted[i] + (lur_data_TEMP$pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4]))
  }
}
  

    
  # lur_data_TEMP$predicted <- ifelse(pollutant == 'no2',
  #                                   lur_data_TEMP$predicted + (lur_data_TEMP$no2_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),2])), lur_data_TEMP$predicted)
  # 
  # lur_data_TEMP$predicted <- ifelse(pollutant == 'pm25',
  #                                   lur_data_TEMP$predicted + lur_data_TEMP$pm25_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),3]), lur_data_TEMP$predicted)
  # 
  # lur_data_TEMP$predicted <- ifelse(pollutant == 'pm10',
  #                                   lur_data_TEMP$predicted + (lur_data_TEMP$pm10_year_avg - as.numeric(ref_data_year[which(ref_data_year$year==year_mod_devel),4])), lur_data_TEMP$predicted)


  lur_data_TEMP <- lur_data_TEMP %>%
    dplyr::select(hhid_x, predicted, styear)
  
  colnames(lur_data_TEMP)[colnames(lur_data_TEMP)=="predicted"] <- pollutant

  lur_data_TEMP
  
}


grid_no2_trunc <- method3_prediction_trunc_GRID(year_mod_devel=1989, model_dataset='pcws', pred_dataset='grid_lur_data', pollutant='no2')

grid_nox_trunc <- method3_prediction_trunc_GRID(year_mod_devel=2015, model_dataset='taps', pred_dataset='grid_lur_data', pollutant='nox')

grid_pm25_trunc <- method3_prediction_trunc_GRID(year_mod_devel=1989, model_dataset='pcws', pred_dataset='grid_lur_data', pollutant='pm25')

grid_pm10_trunc <- method3_prediction_trunc_GRID(year_mod_devel=1989, model_dataset='pcws', pred_dataset='grid_lur_data', pollutant='pm10')

grid_trunc_list <- list(grid_no2_trunc,
                           grid_nox_trunc,
                           grid_pm25_trunc,
                           grid_pm10_trunc)

grid_trunc_predictions <- grid_trunc_list %>%
  Reduce(function(x,y) left_join(x,y,by=c("hhid_x","styear")), .) %>%
  dplyr::select(hhid_x, styear, everything())


```

```{r Amend Grid Exposures, echo=TRUE, message=FALSE, warning=FALSE}

# Note that for some pollutants, the model built predicts negative values. This is not acceptable, so we will scale ALL of those pollutants predicted values up as needed on a pollutant by pollutant basis by adding the absolute value of the minimum to all values. Any 0 values are replaced with the original PCWS or TAPS annual LOD/sqrt(2). Ultimately, this will not change associations in any way in statistical analyses.

pcws_no2_lod <- 4.072744

taps_nox_lod <- 4.262549

pcws_pm25_lod <- 0.9072633

pcws_pm10_lod <- 0.6721785

# Make vector of all years we have grid predictions for to loop through each to scale pollutant values to above 0 on yearly basis for each pollutant
years <- grid_trunc_predictions %>%
  distinct(.,styear)
  
years <- unlist(years[,1])

# Create year-specific minimum and merge by year to grid predictions
# year_mins <- grid_trunc_predictions %>%
#   group_by(styear) %>%
#   summarise(min_no2 = min(no2, na.rm=T),
#             min_nox = min(nox, na.rm=T),
#             min_pm25 = min(pm25, na.rm=T),
#             min_pm10 = min(pm10, na.rm=T))

grid_trunc_predictions <- left_join(grid_trunc_predictions, year_mins, by=c("styear"))

# NO2
summary(grid_trunc_predictions$no2)

for (y in 1:length(years)){
  
  for (i in 1:length(grid_trunc_predictions$no2)){
  
  no2_min <- min(grid_trunc_predictions$no2, na.rm = T)

  if (no2_min<0 & grid_trunc_predictions$styear==years[y]) {
    grid_trunc_predictions$no2[i] <- grid_trunc_predictions$no2[i] + abs(no2_min)

  }

  }
  for (i in 1:length(grid_trunc_predictions$no2)){
  
  if (grid_trunc_predictions$no2[i]==0 & !is.na(grid_trunc_predictions$no2[i]))  {
        grid_trunc_predictions$no2[i] <- pcws_no2_lod/sqrt(2)
  }
}
}
summary(grid_trunc_predictions$no2)

# NOx
summary(grid_trunc_predictions$nox)
for (y in 1:length(years)){
  
  for (i in 1:length(grid_trunc_predictions$nox)){
  
  nox_min <- min(grid_trunc_predictions$nox, na.rm = T)

  if (nox_min<0 & grid_trunc_predictions$styear==years[y]) {
    grid_trunc_predictions$nox[i] <- grid_trunc_predictions$nox[i] + abs(nox_min)

  }

  }
  for (i in 1:length(grid_trunc_predictions$nox)){
  
  if (grid_trunc_predictions$nox[i]==0 & !is.na(grid_trunc_predictions$nox[i]))  {
        grid_trunc_predictions$nox[i] <- taps_nox_lod/sqrt(2)
  }
}
}
summary(grid_trunc_predictions$nox)

# PM2.5
summary(grid_trunc_predictions$pm25)
for (y in 1:length(years)){
  
  for (i in 1:length(grid_trunc_predictions$pm25)){
  
  pm25_min <- min(grid_trunc_predictions$pm25, na.rm = T)

  if (pm25_min<0 & grid_trunc_predictions$styear==years[y]) {
    grid_trunc_predictions$pm25[i] <- grid_trunc_predictions$pm25[i] + abs(pm25_min)

  }

  }
  for (i in 1:length(grid_trunc_predictions$pm25)){
  
  if (grid_trunc_predictions$pm25==0 & !is.na(grid_trunc_predictions$pm25[i]))  {
        grid_trunc_predictions$pm25[i] <- pcws_pm25_lod/sqrt(2)
  }
}
}
summary(grid_trunc_predictions$pm25)

# PM10
summary(grid_trunc_predictions$pm10)
for (y in 1:length(years)){
  
  for (i in 1:length(grid_trunc_predictions$pm10)){
  
  pm10_min <- min(grid_trunc_predictions$pm10, na.rm = T)

  if (pm10_min<0 & grid_trunc_predictions$styear==years[y]) {
    grid_trunc_predictions$pm10[i] <- grid_trunc_predictions$pm10[i] + abs(pm10_min)

  }

  }
  for (i in 1:length(grid_trunc_predictions$pm10)){
  
  if (grid_trunc_predictions$pm10==0 & !is.na(grid_trunc_predictions$pm10[i]))  {
        grid_trunc_predictions$pm10[i] <- pcws_pm10_lod/sqrt(2)
  }
}
}
summary(grid_trunc_predictions$pm10)

```

```{r Map Grid Exposures, echo=TRUE, message=FALSE, warning=FALSE}

GRID_read_addrs <- function(addrs){
  setwd("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Shapefiles/")
  addrs <- st_read("grid_5284ft_tucson.shp", stringsAsFactors = F)
  addrs$hhid_x <- paste(addrs$OBJECTID, "A", sep="_") #make unique address ID

  addrs <- addrs  %>% 
    st_set_crs(NA) %>% 
    st_set_crs(2868) %>%
    st_transform(addrs, crs = 2868)
}

GRID_read_poly <- function(addrs){
  setwd("/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/TAPS/Data/LUR/Shapefiles/")
  addrs <- st_read("grid_5284ft.shp", stringsAsFactors = F)

  addrs <- addrs  %>% 
    st_set_crs(NA) %>% 
    st_set_crs(2868) %>%
    st_transform(addrs, crs = 2868)
}

# Join predictions to points
addrs <- full_join(GRID_read_addrs(), grid_trunc_predictions, by=c("hhid_x"))

# Read in polygons for better display
poly <- GRID_read_poly()

# Join points with exposures to polygons
addrs_poly <- st_join(poly, addrs) %>%
  filter(!is.na(hhid_x))

write_sf(addrs_poly,"/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/GRID/Data/LUR/grid_predictions.shp", delete_layer = T)

plot(addrs_poly)

addrs_poly.osm <- getTiles(
  x = addrs_poly, 
  type = "osm", 
  zoom = 11, 
  crop = F
)

addrs_poly.stamenbw <- getTiles(
  x = addrs_poly, 
  type = "stamenbw", 
  zoom = 10, 
  crop = F
)


plot(st_geometry(addrs_poly),col=NA)
box()

# tilesLayer(x = addrs_poly.osm)
tilesLayer(x = addrs_poly.stamenbw) +
  choroLayer(x = filter(addrs_poly,styear==1980), var = "pm25",
     method = "quantile", nclass = 4,
     breaks = c(0,10,20,30,40),
     col = alpha(carto.pal("green.pal", 5),0.65),
     border = "grey", 
     lwd=.01) +
legendChoro(pos = "topright",
 title.txt = expression("PM2.5 (ug/m^3)"),
 breaks = c(0,10,20,30,40),
 col = carto.pal("green.pal", 5),
 border = "grey", 
 nodata = TRUE, nodata.txt = "No Data")



tmap_mode("view")

tm_shape(addrs_poly)  +
  tm_fill(col="no2", alpha=0.75, popup.vars=c("NO2"="no2"), popup.format = NA) +
  tm_basemap(server = "OpenTopoMap") +
  tm_facets(by="styear")

tm_shape(addrs_poly)  + 
  tm_fill(col="nox", alpha=0.75, popup.vars=c("NOx"="nox"), popup.format = NA) + 
  tm_basemap(server = "OpenTopoMap") +
  tm_facets(by="styear")

tm_shape(addrs_poly)  + 
  tm_fill(col="pm25", alpha=0.75, popup.vars=c("PM2.5"="pm25"), popup.format = NA) + 
  tm_basemap(server = "OpenTopoMap") +
  tm_facets(by="styear")

tm_shape(addrs_poly)  + 
  tm_fill(col="pm10", alpha=0.75, popup.vars=c("PM10"="pm10"), popup.format = NA) + 
  tm_basemap(server = "OpenTopoMap") +
  tm_facets(by="styear")
  


tmap_save(nox_grid, "/Users/nathanlothrop/Dropbox/P5_TAPS_TEMP/GRID/Data/LUR/Maps/nox_grid.html")



```

